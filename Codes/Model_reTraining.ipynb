{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad5815a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = \"mt5-base-cnn-summarizer-en-hi_v5/final_model\"\n",
    "NEW_MODEL_OUTPUT_DIR = \"mt5-base-cnn-summarizer-en-hi_v6\"\n",
    "NEW_DATA_PATH = \"../Dataset/filtered_articles_CNN.csv\"\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "WEIGHT_DECAY = 0.01\n",
    "NUM_BEAMS_EVAL = 6\n",
    "MAX_SUMMARY_LENGTH_EVAL = 256\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"finetuning_log_v6_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "    handlers=[logging.FileHandler(log_filename), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "def pre_run_checks(base_model_path, data_path, output_dir):\n",
    "    \"\"\"Performs checks for paths and permissions before starting.\"\"\"\n",
    "    logging.info(\"--- Performing Pre-Run Checks ---\")\n",
    "    all_checks_passed = True\n",
    "\n",
    "    if not os.path.isdir(base_model_path):\n",
    "        logging.error(f\"Base model path not found: {base_model_path}\")\n",
    "        all_checks_passed = False\n",
    "\n",
    "    if not os.path.isfile(data_path):\n",
    "        logging.error(f\"Data file not found: {data_path}\")\n",
    "        all_checks_passed = False\n",
    "\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        test_file_path = os.path.join(output_dir, \".permission_test\")\n",
    "        with open(test_file_path, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Output directory '{output_dir}' is not writable. Error: {e}\")\n",
    "        all_checks_passed = False\n",
    "\n",
    "    if all_checks_passed:\n",
    "        logging.info(\"--- All pre-run checks passed. ---\")\n",
    "    else:\n",
    "        logging.error(\"--- Pre-run checks failed. Halting execution. ---\")\n",
    "\n",
    "    return all_checks_passed\n",
    "\n",
    "def find_and_save_best_model(output_dir):\n",
    "    \"\"\"Finds the best checkpoint and saves it to a 'final_model' directory.\"\"\"\n",
    "    try:\n",
    "        state_path = os.path.join(output_dir, \"trainer_state.json\")\n",
    "        with open(state_path, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "        \n",
    "        best_checkpoint_path = state.get(\"best_model_checkpoint\")\n",
    "        if not best_checkpoint_path:\n",
    "            logging.error(\"Could not find 'best_model_checkpoint' in trainer_state.json.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"Best checkpoint found: {best_checkpoint_path}\")\n",
    "        \n",
    "        final_model_path = os.path.join(output_dir, \"final_model\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            shutil.rmtree(final_model_path)\n",
    "            \n",
    "        shutil.copytree(best_checkpoint_path, final_model_path)\n",
    "        logging.info(f\"Best model copied to {final_model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not save the best model due to: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not pre_run_checks(BASE_MODEL_PATH, NEW_DATA_PATH, NEW_MODEL_OUTPUT_DIR):\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_PATH)\n",
    "\n",
    "        df_new = pd.read_csv(NEW_DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        df_new.dropna(subset=['raw_news_article', 'english_summary', 'hindi_summary'], inplace=True)\n",
    "        df_new.reset_index(drop=True, inplace=True)\n",
    "        raw_dataset = Dataset.from_pandas(df_new)\n",
    "\n",
    "        PREFIX_ENG = \"summarize English: \"\n",
    "        PREFIX_HIN = \"summarize Hindi: \"\n",
    "\n",
    "        def format_dataset(batch):\n",
    "            inputs, targets = [], []\n",
    "            for article, eng_summary, hin_summary in zip(\n",
    "                batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']\n",
    "            ):\n",
    "                if isinstance(article, str):\n",
    "                    inputs.append(PREFIX_ENG + article)\n",
    "                    targets.append(eng_summary)\n",
    "                    inputs.append(PREFIX_HIN + article)\n",
    "                    targets.append(hin_summary)\n",
    "            return {'inputs': inputs, 'targets': targets}\n",
    "\n",
    "        processed_dataset = raw_dataset.map(\n",
    "            format_dataset, batched=True, remove_columns=raw_dataset.column_names\n",
    "        ).flatten()\n",
    "\n",
    "        train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        final_datasets = DatasetDict({\n",
    "            'train': train_test_split['train'],\n",
    "            'test': train_test_split['test']\n",
    "        })\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            model_inputs = tokenizer(examples['inputs'], max_length=1024, truncation=True)\n",
    "            labels = tokenizer(text_target=examples['targets'], max_length=MAX_SUMMARY_LENGTH_EVAL, truncation=True)\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "\n",
    "        tokenized_datasets = final_datasets.map(tokenize_function, batched=True, remove_columns=['inputs', 'targets'])\n",
    "        \n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=NEW_MODEL_OUTPUT_DIR,\n",
    "            num_train_epochs=NUM_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            logging_dir=f\"{NEW_MODEL_OUTPUT_DIR}/logs\",\n",
    "            logging_steps=50,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=NUM_EPOCHS,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            load_best_model_at_end=False,\n",
    "            metric_for_best_model=\"rouge2\",\n",
    "            generation_max_length=MAX_SUMMARY_LENGTH_EVAL,\n",
    "            generation_num_beams=NUM_BEAMS_EVAL,\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "            return {k: round(v * 100, 4) for k, v in result.items()}\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        logging.info(\"Starting fine-tuning...\")\n",
    "        trainer.train()\n",
    "        logging.info(\"Fine-tuning finished successfully.\")\n",
    "        \n",
    "        find_and_save_best_model(NEW_MODEL_OUTPUT_DIR)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during the main process: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59eec270",
   "metadata": {},
   "source": [
    "model fine tuning v7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24de59cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 15:45:30,431 [INFO] - --- Performing Pre-Run Checks ---\n",
      "2025-10-06 15:45:30,441 [INFO] - --- All pre-run checks passed. ---\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "c:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c26fa1c6daf5433ab20c91772f353c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f566a0c170e468b886d18464b5ea92e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b262951d18e4289b819a345dd1299b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.33G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\admin\\.cache\\huggingface\\hub\\models--google--mt5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0565856e7e054561b3f74e8171972e4a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 15:46:28,849 [ERROR] - An unexpected error occurred during the main process: Seq2SeqTrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_20984\\3107943505.py\", line 167, in main\n",
      "    training_args = Seq2SeqTrainingArguments(\n",
      "TypeError: Seq2SeqTrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = \"google/mt5-base\"\n",
    "NEW_MODEL_OUTPUT_DIR = \"mt5-base-cnn-summarizer-en-hi_v8\"\n",
    "NEW_DATA_PATH = \"../Dataset/new_large_CNN_dataset.csv\"\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "WEIGHT_DECAY = 0.2\n",
    "NUM_BEAMS_EVAL = 6\n",
    "MAX_SUMMARY_LENGTH_EVAL = 256\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"scratch_training_log_v8_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "    handlers=[logging.FileHandler(log_filename), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "def pre_run_checks(data_path, output_dir):\n",
    "    \"\"\"Performs checks for data path and output permissions before starting.\"\"\"\n",
    "    logging.info(\"--- Performing Pre-Run Checks ---\")\n",
    "    all_checks_passed = True\n",
    "\n",
    "    if not os.path.isfile(data_path):\n",
    "        logging.error(f\"Data file not found: {data_path}\")\n",
    "        all_checks_passed = False\n",
    "\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        test_file_path = os.path.join(output_dir, \".permission_test\")\n",
    "        with open(test_file_path, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Output directory '{output_dir}' is not writable. Error: {e}\")\n",
    "        all_checks_passed = False\n",
    "\n",
    "    if all_checks_passed:\n",
    "        logging.info(\"--- All pre-run checks passed. ---\")\n",
    "    else:\n",
    "        logging.error(\"--- Pre-run checks failed. Halting execution. ---\")\n",
    "\n",
    "    return all_checks_passed\n",
    "\n",
    "def find_and_save_best_model(output_dir):\n",
    "    \"\"\"Finds the best checkpoint and saves it to a 'final_model' directory.\"\"\"\n",
    "    try:\n",
    "        state_path = os.path.join(output_dir, \"trainer_state.json\")\n",
    "        with open(state_path, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "        \n",
    "        best_checkpoint_path = state.get(\"best_model_checkpoint\")\n",
    "        if not best_checkpoint_path:\n",
    "            logging.error(\"Could not find 'best_model_checkpoint' in trainer_state.json.\")\n",
    "            return\n",
    "\n",
    "        best_checkpoint_step = int(best_checkpoint_path.split('-')[-1])\n",
    "        best_eval_log = {}\n",
    "        for log in state[\"log_history\"]:\n",
    "            if log.get(\"step\") == best_checkpoint_step and \"eval_loss\" in log:\n",
    "                best_eval_log = log\n",
    "                break\n",
    "        \n",
    "        logging.info(f\"Best checkpoint found: {best_checkpoint_path}\")\n",
    "        logging.info(f\"Metrics for best checkpoint: {best_eval_log}\")\n",
    "\n",
    "        final_model_path = os.path.join(output_dir, \"final_model\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            shutil.rmtree(final_model_path)\n",
    "            \n",
    "        shutil.copytree(best_checkpoint_path, final_model_path)\n",
    "        logging.info(f\"Best model copied to {final_model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not save the best model due to: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not pre_run_checks(NEW_DATA_PATH, NEW_MODEL_OUTPUT_DIR):\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_PATH)\n",
    "\n",
    "        df_new = pd.read_csv(NEW_DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        df_new.dropna(subset=['raw_news_article', 'english_summary', 'hindi_summary'], inplace=True)\n",
    "        df_new.reset_index(drop=True, inplace=True)\n",
    "        raw_dataset = Dataset.from_pandas(df_new)\n",
    "\n",
    "        PREFIX_ENG = \"summarize English: \"\n",
    "        PREFIX_HIN = \"summarize Hindi: \"\n",
    "\n",
    "        def format_dataset(batch):\n",
    "            inputs, targets = [], []\n",
    "            for article, eng_summary, hin_summary in zip(\n",
    "                batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']\n",
    "            ):\n",
    "                if isinstance(article, str):\n",
    "                    inputs.append(PREFIX_ENG + article)\n",
    "                    targets.append(eng_summary)\n",
    "                    inputs.append(PREFIX_HIN + article)\n",
    "                    targets.append(hin_summary)\n",
    "            return {'inputs': inputs, 'targets': targets}\n",
    "\n",
    "        processed_dataset = raw_dataset.map(\n",
    "            format_dataset, batched=True, remove_columns=raw_dataset.column_names\n",
    "        ).flatten()\n",
    "\n",
    "        train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        final_datasets = DatasetDict({\n",
    "            'train': train_test_split['train'],\n",
    "            'test': train_test_split['test']\n",
    "        })\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            model_inputs = tokenizer(examples['inputs'], max_length=1024, truncation=True)\n",
    "            labels = tokenizer(text_target=examples['targets'], max_length=MAX_SUMMARY_LENGTH_EVAL, truncation=True)\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "\n",
    "        tokenized_datasets = final_datasets.map(tokenize_function, batched=True, remove_columns=['inputs', 'targets'])\n",
    "        \n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "        bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "            bert_result = bertscore_metric.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "            \n",
    "            result = {}\n",
    "            for key, value in rouge_result.items():\n",
    "                result[f\"{key}\"] = round(value * 100, 4)\n",
    "\n",
    "            result[\"bertscore_f1\"] = round(np.mean(bert_result[\"f1\"]) * 100, 4)\n",
    "\n",
    "            return result\n",
    "\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=NEW_MODEL_OUTPUT_DIR,\n",
    "            num_train_epochs=NUM_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            logging_dir=f\"{NEW_MODEL_OUTPUT_DIR}/logs\",\n",
    "            logging_steps=50,\n",
    "            evaluation_strategy=\"epoch\", # This line is added back\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=NUM_EPOCHS,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"bertscore_f1\",\n",
    "            generation_max_length=MAX_SUMMARY_LENGTH_EVAL,\n",
    "            generation_num_beams=NUM_BEAMS_EVAL,\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        logging.info(\"Starting training from scratch...\")\n",
    "        trainer.train()\n",
    "        logging.info(\"Training finished successfully.\")\n",
    "        \n",
    "        find_and_save_best_model(NEW_MODEL_OUTPUT_DIR)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during the main process: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f4d0dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 15:49:01,116 [INFO] - --- Performing Pre-Run Checks ---\n",
      "2025-10-06 15:49:01,127 [INFO] - --- All pre-run checks passed. ---\n",
      "c:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90356876fee748b4b71ac09c6a071447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94e09386140a42228f64f7589e5ec2a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f06f0a719cac4448a1561d6203b455a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_20984\\3435410667.py:203: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "2025-10-06 15:49:44,597 [INFO] - Starting training from scratch...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1852' max='10380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1852/10380 26:35 < 2:02:33, 1.16 it/s, Epoch 0.89/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 223\u001b[0m\n\u001b[0;32m    220\u001b[0m         logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn unexpected error occurred during the main process: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 223\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 214\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    203\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m    204\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    205\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    210\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m    211\u001b[0m )\n\u001b[0;32m    213\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training from scratch...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 214\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    215\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining finished successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    217\u001b[0m find_and_save_best_model(NEW_MODEL_OUTPUT_DIR)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:2328\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2326\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:2672\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2665\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2666\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2667\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2668\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2669\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2670\u001b[0m )\n\u001b[0;32m   2671\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2672\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2675\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2676\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2677\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2678\u001b[0m ):\n\u001b[0;32m   2679\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2680\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:4009\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   4006\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   4008\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 4009\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4011\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   4012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4013\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4014\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   4015\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:4099\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   4097\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   4098\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m-> 4099\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   4101\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   4102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\accelerate\\utils\\operations.py:818\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\accelerate\\utils\\operations.py:806\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\amp\\autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\models\\mt5\\modeling_mt5.py:1749\u001b[0m, in \u001b[0;36mMT5ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# Encode if needed (training, first prediction pass)\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m encoder_outputs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1748\u001b[0m     \u001b[38;5;66;03m# Convert encoder inputs in embeddings if needed\u001b[39;00m\n\u001b[1;32m-> 1749\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1750\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1751\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1752\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1753\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1754\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1755\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1756\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1757\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(encoder_outputs, BaseModelOutput):\n\u001b[0;32m   1759\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1760\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1761\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1762\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1763\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\models\\mt5\\modeling_mt5.py:1086\u001b[0m, in \u001b[0;36mMT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m   1084\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[1;32m-> 1086\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[0;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1102\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;66;03m# We share the position biases between the layers - the first layer store them\u001b[39;00m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;66;03m# layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),\u001b[39;00m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\models\\mt5\\modeling_mt5.py:564\u001b[0m, in \u001b[0;36mMT5Block.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_values, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_value\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.58\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    562\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    563\u001b[0m ):\n\u001b[1;32m--> 564\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    565\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    566\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    568\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    569\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    570\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    571\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    572\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    573\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    574\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    575\u001b[0m     attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\models\\mt5\\modeling_mt5.py:478\u001b[0m, in \u001b[0;36mMT5LayerSelfAttention.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_values, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_value\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.58\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    475\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    476\u001b[0m ):\n\u001b[0;32m    477\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m--> 478\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSelfAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    488\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    489\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (hidden_states,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\models\\mt5\\modeling_mt5.py:403\u001b[0m, in \u001b[0;36mMT5Attention.forward\u001b[1;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_values, layer_head_mask, query_length, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    400\u001b[0m             past_key_values\u001b[38;5;241m.\u001b[39mis_updated[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    402\u001b[0m \u001b[38;5;66;03m# compute scores, equivalent of torch.einsum(\"bnqd,bnkd->bnqk\", query_states, key_states), compatible with onnx op>9\u001b[39;00m\n\u001b[1;32m--> 403\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    405\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m position_bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    406\u001b[0m     key_length \u001b[38;5;241m=\u001b[39m key_states\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = \"google/mt5-base\"\n",
    "NEW_MODEL_OUTPUT_DIR = \"mt5-base-cnn-summarizer-en-hi_v8\"\n",
    "NEW_DATA_PATH = \"../Dataset/new_large_CNN_dataset.csv\"\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "WEIGHT_DECAY = 0.2\n",
    "NUM_BEAMS_EVAL = 6\n",
    "MAX_SUMMARY_LENGTH_EVAL = 256\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"scratch_training_log_v8_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "    handlers=[logging.FileHandler(log_filename), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "def pre_run_checks(data_path, output_dir):\n",
    "    \"\"\"Performs checks for data path and output permissions before starting.\"\"\"\n",
    "    logging.info(\"--- Performing Pre-Run Checks ---\")\n",
    "    all_checks_passed = True\n",
    "\n",
    "    if not os.path.isfile(data_path):\n",
    "        logging.error(f\"Data file not found: {data_path}\")\n",
    "        all_checks_passed = False\n",
    "\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        test_file_path = os.path.join(output_dir, \".permission_test\")\n",
    "        with open(test_file_path, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Output directory '{output_dir}' is not writable. Error: {e}\")\n",
    "        all_checks_passed = False\n",
    "\n",
    "    if all_checks_passed:\n",
    "        logging.info(\"--- All pre-run checks passed. ---\")\n",
    "    else:\n",
    "        logging.error(\"--- Pre-run checks failed. Halting execution. ---\")\n",
    "\n",
    "    return all_checks_passed\n",
    "\n",
    "def find_and_save_best_model(output_dir):\n",
    "    \"\"\"Finds the best checkpoint and saves it to a 'final_model' directory.\"\"\"\n",
    "    try:\n",
    "        state_path = os.path.join(output_dir, \"trainer_state.json\")\n",
    "        with open(state_path, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "        \n",
    "        # Find the evaluation log with the best (lowest) eval_loss or highest metric\n",
    "        best_metric_value = None\n",
    "        best_checkpoint_path = None\n",
    "\n",
    "        # Determine metric for comparison, prioritizing 'eval_bertscore_f1' then 'eval_loss'\n",
    "        metric_to_check = f\"eval_{'bertscore_f1'}\" # Assumes metric_for_best_model is bertscore_f1\n",
    "        is_loss = 'loss' in metric_to_check\n",
    "\n",
    "        for log in state[\"log_history\"]:\n",
    "            if metric_to_check in log:\n",
    "                metric_value = log[metric_to_check]\n",
    "                \n",
    "                if best_metric_value is None or \\\n",
    "                   (is_loss and metric_value < best_metric_value) or \\\n",
    "                   (not is_loss and metric_value > best_metric_value):\n",
    "                    best_metric_value = metric_value\n",
    "                    step = log['step']\n",
    "                    # Construct checkpoint path based on step\n",
    "                    potential_path = os.path.join(output_dir, f\"checkpoint-{step}\")\n",
    "                    if os.path.exists(potential_path):\n",
    "                         best_checkpoint_path = potential_path\n",
    "\n",
    "\n",
    "        if not best_checkpoint_path:\n",
    "            logging.error(\"Could not find a valid best checkpoint path from logs.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"Best checkpoint found: {best_checkpoint_path} with {metric_to_check}: {best_metric_value}\")\n",
    "\n",
    "        final_model_path = os.path.join(output_dir, \"final_model\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            shutil.rmtree(final_model_path)\n",
    "            \n",
    "        shutil.copytree(best_checkpoint_path, final_model_path)\n",
    "        logging.info(f\"Best model copied to {final_model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not save the best model due to: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not pre_run_checks(NEW_DATA_PATH, NEW_MODEL_OUTPUT_DIR):\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_PATH)\n",
    "\n",
    "        df_new = pd.read_csv(NEW_DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        df_new.dropna(subset=['raw_news_article', 'english_summary', 'hindi_summary'], inplace=True)\n",
    "        df_new.reset_index(drop=True, inplace=True)\n",
    "        raw_dataset = Dataset.from_pandas(df_new)\n",
    "\n",
    "        PREFIX_ENG = \"summarize English: \"\n",
    "        PREFIX_HIN = \"summarize Hindi: \"\n",
    "\n",
    "        def format_dataset(batch):\n",
    "            inputs, targets = [], []\n",
    "            for article, eng_summary, hin_summary in zip(\n",
    "                batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']\n",
    "            ):\n",
    "                if isinstance(article, str):\n",
    "                    inputs.append(PREFIX_ENG + article)\n",
    "                    targets.append(eng_summary)\n",
    "                    inputs.append(PREFIX_HIN + article)\n",
    "                    targets.append(hin_summary)\n",
    "            return {'inputs': inputs, 'targets': targets}\n",
    "\n",
    "        processed_dataset = raw_dataset.map(\n",
    "            format_dataset, batched=True, remove_columns=raw_dataset.column_names\n",
    "        ).flatten()\n",
    "\n",
    "        train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        final_datasets = DatasetDict({\n",
    "            'train': train_test_split['train'],\n",
    "            'test': train_test_split['test']\n",
    "        })\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            model_inputs = tokenizer(examples['inputs'], max_length=1024, truncation=True)\n",
    "            labels = tokenizer(text_target=examples['targets'], max_length=MAX_SUMMARY_LENGTH_EVAL, truncation=True)\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "\n",
    "        tokenized_datasets = final_datasets.map(tokenize_function, batched=True, remove_columns=['inputs', 'targets'])\n",
    "        \n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "        bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "            bert_result = bertscore_metric.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "            \n",
    "            result = {}\n",
    "            for key, value in rouge_result.items():\n",
    "                result[f\"{key}\"] = round(value * 100, 4)\n",
    "\n",
    "            result[\"bertscore_f1\"] = round(np.mean(bert_result[\"f1\"]) * 100, 4)\n",
    "\n",
    "            return result\n",
    "\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=NEW_MODEL_OUTPUT_DIR,\n",
    "            num_train_epochs=NUM_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            logging_dir=f\"{NEW_MODEL_OUTPUT_DIR}/logs\",\n",
    "            logging_steps=50,\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=NUM_EPOCHS,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            load_best_model_at_end=False, # Set to False\n",
    "            # metric_for_best_model is not needed when load_best_model_at_end is False\n",
    "            generation_max_length=MAX_SUMMARY_LENGTH_EVAL,\n",
    "            generation_num_beams=NUM_BEAMS_EVAL,\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        logging.info(\"Starting training from scratch...\")\n",
    "        trainer.train()\n",
    "        logging.info(\"Training finished successfully.\")\n",
    "        \n",
    "        find_and_save_best_model(NEW_MODEL_OUTPUT_DIR)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during the main process: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f80a304d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 16:18:01,928 [INFO] - --- Performing Pre-Run Checks ---\n",
      "2025-10-06 16:18:02,043 [INFO] - --- All pre-run checks passed. ---\n",
      "c:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c220d65b3304623bc680a86664c34f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 16:18:12,136 [INFO] - --- Checking a few examples from the training set for data leakage ---\n",
      "2025-10-06 16:18:12,136 [INFO] - \n",
      "--- Example 1 ---\n",
      "2025-10-06 16:18:12,136 [INFO] - INPUT: summarize Hindi: Paris Saint-Germain face Nice on Saturday, hoping to take Ligue 1's top spot from Lyon but do so with a host of key stars missing, including captain Thiago Silva who is recuperating at home from a thigh injury. Zlatan Ibrahimovic, Marco Verratti and Thiago Motta all join Silva on the sidelines for the trip to the Mediterranean coast, while David Luiz is still not fully fit as he recovers from a thigh problem, although he is still set to start. Silva was pictured nursing his prob...\n",
      "2025-10-06 16:18:12,141 [INFO] - TARGET: पेरिस सेंट-जर्मेन (पीएसजी) शनिवार को नाइस के खिलाफ खेलेगा, जिसका लक्ष्य लियोन से लीग 1 का शीर्ष स्थान हासिल करना है। टीम को ज़्लाटन इब्राहिमोविच, मार्को वेराट्टी, थियागो मोट्टा और कप्तान थियागो सिल्वा सहित महत्वपूर्ण खिलाड़ियों की अनुपस्थिति का सामना करना पड़ेगा। सिल्वा जांघ की मामूली चोट से उबर रहे हैं। डेविड लुइज़, हालांकि पूरी तरह से फिट नहीं हैं, लेकिन अपनी जांघ की समस्या के बावजूद खेलने की उम्मीद है। एक जीत पीएसजी को वर्तमान नेता लियोन से एक अंक आगे कर देगी, जो बाद में सेंट एटिएन से खेलेंगे, जिससे पीएसजी के पास एक मैच हाथ में होगा। इस बीच, शुक्रवार को नांतेस के खिलाफ मार्सेय की 1-0 से हार ने प्रभावी रूप से उनके खिताब की उम्मीदों को समाप्त कर दिया, जैसा कि कोच मार्सेलो बिएल्सा ने पुष्टि की। पीएसजी को अब लीग 1 का खिताब बरकरार रखने के लिए पसंदीदा माना जा रहा है, जिसमें लियोन और मोनाको उनके मुख्य प्रतिद्वंद्वी हैं।\n",
      "2025-10-06 16:18:12,142 [INFO] - \n",
      "--- Example 2 ---\n",
      "2025-10-06 16:18:12,143 [INFO] - INPUT: summarize Hindi: Arsenal are only three signings away from winning the title and should start by pinching Chelsea goalkeeper Petr Cech, according to former Gunners midfielder Ray Parlour. Cech is likely to be watching from the substitutes bench when Arsenal face Chelsea at the Emirates Stadium on Sunday, having made only four league starts this season. Jose Mourinho's men are 10 points clear going into the game this weekend but Parlour believes signing Cech in the summer can help Arsenal overcom...\n",
      "2025-10-06 16:18:12,143 [INFO] - TARGET: आर्सेनल के पूर्व मिडफील्डर रे पार्लर का मानना है कि आर्सेनल प्रीमियर लीग का खिताब जीतने से केवल तीन खिलाड़ियों की खरीद दूर है, जिसमें चेल्सी के गोलकीपर पेट्र चेक प्राथमिकता पर हैं। पार्लर का सुझाव है कि आर्सेनल को अनुभवी 33 वर्षीय चेक को लगभग £10 मिलियन में खरीदना चाहिए, जिन्होंने इस सीज़न में केवल चार लीग मैच शुरू किए हैं, जबकि वोज्शिएक स्ज़ेसनी को £6 मिलियन में बेचना चाहिए। वह चेक के बड़े मैचों के अनुभव और उनके फुटबॉल करियर में बचे हुए वर्षों पर जोर देते हैं। पार्लर यह भी कहते हैं कि आर्सेनल को 'टीम की रीढ़' को मजबूत करने की आवश्यकता है, जिसमें एक सेंटर-हाफ, एक सेंट्रल मिडफील्डर और एक गोलकीपर शामिल हैं। एलेक्सिस सांचेज़ के शानदार सीज़न पर प्रकाश डालते हुए, वह उनकी क्षमता के अधिक खिलाड़ियों की आवश्यकता पर जोर देते हैं। जबकि इस सीज़न में चेल्सी के लीग जीतने की उम्मीद है, पार्लर इन गर्मी की खरीद को आर्सेनल के अगले साल के खिताब चुनौती के लिए महत्वपूर्ण मानते हैं, हालांकि वह सवाल करते हैं कि क्या चेल्सी चेक को किसी प्रतिद्वंद्वी को बेचेगी।\n",
      "2025-10-06 16:18:12,143 [INFO] - \n",
      "--- Example 3 ---\n",
      "2025-10-06 16:18:12,143 [INFO] - INPUT: summarize Hindi: MEXICO CITY, Mexico The Mexican navy said Wednesday that it rescued five Ecuadorians who had been adrift without supplies in a fishing boat for more than two weeks off the coast of the southern state of Chiapas. Mexican medical personel examine two of five Ecuadorians rescued at sea. Mexican authorities initiated the rescue, which occurred Tuesday, after the U.S. Coast Guard alerted them that sailors aboard a fishing boat located 45 nautical miles (83 km) southeast of Port Chiap...\n",
      "2025-10-06 16:18:12,143 [INFO] - TARGET: मैक्सिकन नौसेना ने बुधवार को घोषणा की कि उसने चियापास के दक्षिणी राज्य के तट पर दो सप्ताह से अधिक समय से बिना आपूर्ति के फंसे हुए पांच इक्वाडोरियन नाविकों को बचाया है। यह बचाव अभियान, जो मंगलवार को हुआ था, अमेरिकी तटरक्षक बल द्वारा मैक्सिकन अधिकारियों को पोर्ट चियापास से 45 समुद्री मील (83 किमी) दक्षिण-पूर्व में एक मछली पकड़ने वाली नाव से मदद के संकेत मिलने के बाद शुरू किया गया था। मैक्सिकन नौसेना के एक हेलीकॉप्टर ने 15 मीटर लंबे पोत का पता लगाया और पांच व्यक्तियों को हवाई मार्ग से बचाया: कप्तान जेमी आर्टुरो अलाबा शावेज (35), कुक विक्टर ह्यूगो अलाबा शावेज (32), नाविक एडिसन प्राडो अलाबा (27) और कार्लोस चेमे वाज़क्वेज़ (37), और मशीनरी विशेषज्ञ राउल कॉन्ट्रेरा वेरा (64)। उन्हें प्यूर्टो चियापास के नौसैनिक सेनेटोरियम ले जाया गया, जहाँ डॉक्टरों ने उनका निर्जलीकरण के लिए इलाज किया। नाविकों ने बताया कि वे 6 मई को कोस्टा रिका से निकले थे, लेकिन उनका मोटर 11 मई को काम करना बंद कर दिया था, जिसके बाद से वे फंसे हुए और बिना भोजन के थे। एक नौसैनिक गश्ती नाव ने मछली पकड़ने वाली नाव को प्यूर्टो चियापास तक खींचा, जहाँ किसी भी संभावित अवैध गतिविधि के लिए इसकी जांच की जाएगी।\n",
      "2025-10-06 16:18:12,143 [INFO] - \n",
      "================================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5a06306d6254c92b7556a1dfe9cd607",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a56760d88da545fe9f24bf9da4910cc6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_20984\\1157840164.py:213: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "2025-10-06 16:18:48,002 [INFO] - Starting training from scratch...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1294' max='10380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1294/10380 18:42 < 2:11:35, 1.15 it/s, Epoch 0.62/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 233\u001b[0m\n\u001b[0;32m    230\u001b[0m         logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn unexpected error occurred during the main process: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 233\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 224\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    213\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m    214\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    215\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    220\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m    221\u001b[0m )\n\u001b[0;32m    223\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training from scratch...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 224\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    225\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining finished successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    227\u001b[0m find_and_save_best_model(NEW_MODEL_OUTPUT_DIR)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:2328\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2326\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:2672\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2665\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2666\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2667\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2668\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2669\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2670\u001b[0m )\n\u001b[0;32m   2671\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2672\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2675\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2676\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2677\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2678\u001b[0m ):\n\u001b[0;32m   2679\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2680\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:4009\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   4006\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   4008\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 4009\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4011\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   4012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4013\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4014\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   4015\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:4099\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   4097\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   4098\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m-> 4099\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   4101\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   4102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\accelerate\\utils\\operations.py:818\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\accelerate\\utils\\operations.py:806\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\amp\\autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\models\\mt5\\modeling_mt5.py:1786\u001b[0m, in \u001b[0;36mMT5ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1783\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[1;32m-> 1786\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1787\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1789\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1795\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1796\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1797\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1802\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1804\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\models\\mt5\\modeling_mt5.py:1086\u001b[0m, in \u001b[0;36mMT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m   1084\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[1;32m-> 1086\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[0;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1102\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;66;03m# We share the position biases between the layers - the first layer store them\u001b[39;00m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;66;03m# layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),\u001b[39;00m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\models\\mt5\\modeling_mt5.py:588\u001b[0m, in \u001b[0;36mMT5Block.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_values, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    586\u001b[0m do_cross_attention \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_decoder \u001b[38;5;129;01mand\u001b[39;00m encoder_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    587\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_cross_attention:\n\u001b[1;32m--> 588\u001b[0m     cross_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    589\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    590\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    591\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    592\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    593\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    594\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    595\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    596\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    597\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    598\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    599\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    601\u001b[0m     \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\models\\mt5\\modeling_mt5.py:516\u001b[0m, in \u001b[0;36mMT5LayerCrossAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, attention_mask, position_bias, layer_head_mask, past_key_values, use_cache, query_length, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    501\u001b[0m \u001b[38;5;129m@deprecate_kwarg\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_value\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m, version\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.58\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    513\u001b[0m     cache_position\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    514\u001b[0m ):\n\u001b[0;32m    515\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[1;32m--> 516\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEncDecAttention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnormed_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_value_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_value_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    523\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    525\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    527\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    528\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    529\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m attention_output[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\models\\mt5\\modeling_mt5.py:435\u001b[0m, in \u001b[0;36mMT5Attention.forward\u001b[1;34m(self, hidden_states, mask, key_value_states, position_bias, past_key_values, layer_head_mask, query_length, use_cache, output_attentions, cache_position)\u001b[0m\n\u001b[0;32m    432\u001b[0m scores \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m position_bias_masked\n\u001b[0;32m    434\u001b[0m \u001b[38;5;66;03m# (batch_size, n_heads, seq_length, key_length)\u001b[39;00m\n\u001b[1;32m--> 435\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(\u001b[43mscores\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mtype_as(scores)\n\u001b[0;32m    436\u001b[0m attn_weights \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(attn_weights, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    438\u001b[0m \u001b[38;5;66;03m# Mask heads if we want to\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = \"google/mt5-base\"\n",
    "NEW_MODEL_OUTPUT_DIR = \"mt5-base-cnn-summarizer-en-hi_v8\"\n",
    "NEW_DATA_PATH = \"../Dataset/new_large_CNN_dataset.csv\"\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "WEIGHT_DECAY = 0.2\n",
    "NUM_BEAMS_EVAL = 6\n",
    "MAX_SUMMARY_LENGTH_EVAL = 256\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"scratch_training_log_v8_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "    handlers=[logging.FileHandler(log_filename), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "def pre_run_checks(data_path, output_dir):\n",
    "    \"\"\"Performs checks for data path and output permissions before starting.\"\"\"\n",
    "    logging.info(\"--- Performing Pre-Run Checks ---\")\n",
    "    all_checks_passed = True\n",
    "\n",
    "    if not os.path.isfile(data_path):\n",
    "        logging.error(f\"Data file not found: {data_path}\")\n",
    "        all_checks_passed = False\n",
    "\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        test_file_path = os.path.join(output_dir, \".permission_test\")\n",
    "        with open(test_file_path, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Output directory '{output_dir}' is not writable. Error: {e}\")\n",
    "        all_checks_passed = False\n",
    "\n",
    "    if all_checks_passed:\n",
    "        logging.info(\"--- All pre-run checks passed. ---\")\n",
    "    else:\n",
    "        logging.error(\"--- Pre-run checks failed. Halting execution. ---\")\n",
    "\n",
    "    return all_checks_passed\n",
    "\n",
    "def find_and_save_best_model(output_dir):\n",
    "    \"\"\"Finds the best checkpoint and saves it to a 'final_model' directory.\"\"\"\n",
    "    try:\n",
    "        state_path = os.path.join(output_dir, \"trainer_state.json\")\n",
    "        with open(state_path, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "        \n",
    "        best_metric_value = None\n",
    "        best_checkpoint_path = None\n",
    "\n",
    "        metric_to_check = f\"eval_{'bertscore_f1'}\" \n",
    "        is_loss = 'loss' in metric_to_check\n",
    "\n",
    "        for log in state[\"log_history\"]:\n",
    "            if metric_to_check in log:\n",
    "                metric_value = log[metric_to_check]\n",
    "                \n",
    "                if best_metric_value is None or \\\n",
    "                   (is_loss and metric_value < best_metric_value) or \\\n",
    "                   (not is_loss and metric_value > best_metric_value):\n",
    "                    best_metric_value = metric_value\n",
    "                    step = log['step']\n",
    "                    potential_path = os.path.join(output_dir, f\"checkpoint-{step}\")\n",
    "                    if os.path.exists(potential_path):\n",
    "                         best_checkpoint_path = potential_path\n",
    "\n",
    "        if not best_checkpoint_path:\n",
    "            logging.error(\"Could not find a valid best checkpoint path from logs.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"Best checkpoint found: {best_checkpoint_path} with {metric_to_check}: {best_metric_value}\")\n",
    "\n",
    "        final_model_path = os.path.join(output_dir, \"final_model\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            shutil.rmtree(final_model_path)\n",
    "            \n",
    "        shutil.copytree(best_checkpoint_path, final_model_path)\n",
    "        logging.info(f\"Best model copied to {final_model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not save the best model due to: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not pre_run_checks(NEW_DATA_PATH, NEW_MODEL_OUTPUT_DIR):\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_PATH)\n",
    "\n",
    "        df_new = pd.read_csv(NEW_DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        df_new.dropna(subset=['raw_news_article', 'english_summary', 'hindi_summary'], inplace=True)\n",
    "        \n",
    "        # More robust cleaning: drop rows where any of the key columns are just whitespace\n",
    "        df_new = df_new[df_new['raw_news_article'].str.strip().astype(bool)]\n",
    "        df_new = df_new[df_new['english_summary'].str.strip().astype(bool)]\n",
    "        df_new = df_new[df_new['hindi_summary'].str.strip().astype(bool)]\n",
    "\n",
    "        df_new.reset_index(drop=True, inplace=True)\n",
    "        raw_dataset = Dataset.from_pandas(df_new)\n",
    "\n",
    "        PREFIX_ENG = \"summarize English: \"\n",
    "        PREFIX_HIN = \"summarize Hindi: \"\n",
    "\n",
    "        def format_dataset(batch):\n",
    "            inputs, targets = [], []\n",
    "            for article, eng_summary, hin_summary in zip(\n",
    "                batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']\n",
    "            ):\n",
    "                if isinstance(article, str):\n",
    "                    inputs.append(PREFIX_ENG + article)\n",
    "                    targets.append(eng_summary)\n",
    "                    inputs.append(PREFIX_HIN + article)\n",
    "                    targets.append(hin_summary)\n",
    "            return {'inputs': inputs, 'targets': targets}\n",
    "\n",
    "        processed_dataset = raw_dataset.map(\n",
    "            format_dataset, batched=True, remove_columns=raw_dataset.column_names\n",
    "        ).flatten()\n",
    "\n",
    "        train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        final_datasets = DatasetDict({\n",
    "            'train': train_test_split['train'],\n",
    "            'test': train_test_split['test']\n",
    "        })\n",
    "\n",
    "        # --- Diagnostic Step: Print a few examples to check for data leakage ---\n",
    "        logging.info(\"--- Checking a few examples from the training set for data leakage ---\")\n",
    "        for i in range(3):\n",
    "            logging.info(f\"\\n--- Example {i+1} ---\")\n",
    "            logging.info(f\"INPUT: {final_datasets['train'][i]['inputs'][:500]}...\") # Print first 500 chars\n",
    "            logging.info(f\"TARGET: {final_datasets['train'][i]['targets']}\")\n",
    "        logging.info(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "        # --- End of Diagnostic Step ---\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            model_inputs = tokenizer(examples['inputs'], max_length=1024, truncation=True)\n",
    "            labels = tokenizer(text_target=examples['targets'], max_length=MAX_SUMMARY_LENGTH_EVAL, truncation=True)\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "\n",
    "        tokenized_datasets = final_datasets.map(tokenize_function, batched=True, remove_columns=['inputs', 'targets'])\n",
    "        \n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "        bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "            bert_result = bertscore_metric.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "            \n",
    "            result = {}\n",
    "            for key, value in rouge_result.items():\n",
    "                result[f\"{key}\"] = round(value * 100, 4)\n",
    "\n",
    "            result[\"bertscore_f1\"] = round(np.mean(bert_result[\"f1\"]) * 100, 4)\n",
    "\n",
    "            return result\n",
    "\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=NEW_MODEL_OUTPUT_DIR,\n",
    "            num_train_epochs=NUM_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            logging_dir=f\"{NEW_MODEL_OUTPUT_DIR}/logs\",\n",
    "            logging_steps=50,\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=NUM_EPOCHS,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            load_best_model_at_end=False,\n",
    "            generation_max_length=MAX_SUMMARY_LENGTH_EVAL,\n",
    "            generation_num_beams=NUM_BEAMS_EVAL,\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        logging.info(\"Starting training from scratch...\")\n",
    "        trainer.train()\n",
    "        logging.info(\"Training finished successfully.\")\n",
    "        \n",
    "        find_and_save_best_model(NEW_MODEL_OUTPUT_DIR)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during the main process: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80718bac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19f843e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 16:48:26,485 [INFO] - --- Performing Pre-Run Checks ---\n",
      "2025-10-06 16:48:26,751 [INFO] - --- All pre-run checks passed. ---\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'T5Tokenizer'. \n",
      "The class this function is called from is 'MT5Tokenizer'.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.mt5.tokenization_mt5.MT5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d554d33706ff4f9f9da8c0dfcf88cf1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "694dbe9625324a86abb9aec96719b119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:4007: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b8cc5c8aaf41bb90340f9ee8f28e28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 16:50:05,181 [ERROR] - An unexpected error occurred during the main process: Seq2SeqTrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_21372\\912464866.py\", line 146, in main\n",
      "    training_args = Seq2SeqTrainingArguments(\n",
      "TypeError: Seq2SeqTrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    MT5Tokenizer,\n",
    "    MT5ForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = \"google/mt5-base\"\n",
    "NEW_MODEL_OUTPUT_DIR = \"mt5-base-cnn-summarizer-en-hi_v8\"\n",
    "NEW_DATA_PATH = \"../Dataset/new_large_CNN_dataset.csv\"\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "WEIGHT_DECAY = 0.25\n",
    "NUM_BEAMS_EVAL = 6\n",
    "MAX_SUMMARY_LENGTH_EVAL = 256\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"scratch_training_log_v8_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "    handlers=[logging.FileHandler(log_filename), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "def pre_run_checks(data_path, output_dir):\n",
    "    \"\"\"Performs checks for data path and output permissions before starting.\"\"\"\n",
    "    logging.info(\"--- Performing Pre-Run Checks ---\")\n",
    "    all_checks_passed = True\n",
    "\n",
    "    if not os.path.isfile(data_path):\n",
    "        logging.error(f\"Data file not found: {data_path}\")\n",
    "        all_checks_passed = False\n",
    "\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        test_file_path = os.path.join(output_dir, \".permission_test\")\n",
    "        with open(test_file_path, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Output directory '{output_dir}' is not writable. Error: {e}\")\n",
    "        all_checks_passed = False\n",
    "\n",
    "    if all_checks_passed:\n",
    "        logging.info(\"--- All pre-run checks passed. ---\")\n",
    "    else:\n",
    "        logging.error(\"--- Pre-run checks failed. Halting execution. ---\")\n",
    "\n",
    "    return all_checks_passed\n",
    "\n",
    "def main():\n",
    "    if not pre_run_checks(NEW_DATA_PATH, NEW_MODEL_OUTPUT_DIR):\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Using explicit MT5Tokenizer for clarity and correctness\n",
    "        tokenizer = MT5Tokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "        model = MT5ForConditionalGeneration.from_pretrained(BASE_MODEL_PATH)\n",
    "\n",
    "        df_new = pd.read_csv(NEW_DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        df_new.dropna(subset=['raw_news_article', 'english_summary', 'hindi_summary'], inplace=True)\n",
    "        \n",
    "        df_new = df_new[df_new['raw_news_article'].str.strip().astype(bool)]\n",
    "        df_new = df_new[df_new['english_summary'].str.strip().astype(bool)]\n",
    "        df_new = df_new[df_new['hindi_summary'].str.strip().astype(bool)]\n",
    "\n",
    "        df_new.reset_index(drop=True, inplace=True)\n",
    "        raw_dataset = Dataset.from_pandas(df_new)\n",
    "\n",
    "        PREFIX_ENG = \"summarize English: \"\n",
    "        PREFIX_HIN = \"summarize Hindi: \"\n",
    "\n",
    "        def format_dataset(batch):\n",
    "            inputs, targets = [], []\n",
    "            for article, eng_summary, hin_summary in zip(\n",
    "                batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']\n",
    "            ):\n",
    "                if isinstance(article, str):\n",
    "                    inputs.append(PREFIX_ENG + article)\n",
    "                    targets.append(eng_summary)\n",
    "                    inputs.append(PREFIX_HIN + article)\n",
    "                    targets.append(hin_summary)\n",
    "            return {'inputs': inputs, 'targets': targets}\n",
    "\n",
    "        processed_dataset = raw_dataset.map(\n",
    "            format_dataset, batched=True, remove_columns=raw_dataset.column_names\n",
    "        ).flatten()\n",
    "\n",
    "        train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        final_datasets = DatasetDict({\n",
    "            'train': train_test_split['train'],\n",
    "            'test': train_test_split['test']\n",
    "        })\n",
    "        \n",
    "        # --- THE CRITICAL FIX IS HERE ---\n",
    "        def tokenize_function(examples):\n",
    "            model_inputs = tokenizer(examples['inputs'], max_length=1024, truncation=True)\n",
    "            \n",
    "            # This context manager is essential for T5-based models.\n",
    "            # It ensures the labels are tokenized correctly for the decoder.\n",
    "            with tokenizer.as_target_tokenizer():\n",
    "                labels = tokenizer(examples['targets'], max_length=MAX_SUMMARY_LENGTH_EVAL, truncation=True)\n",
    "                \n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "\n",
    "        tokenized_datasets = final_datasets.map(tokenize_function, batched=True, remove_columns=['inputs', 'targets'])\n",
    "        \n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "        bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "            bert_result = bertscore_metric.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "            \n",
    "            result = {}\n",
    "            for key, value in rouge_result.items():\n",
    "                result[f\"rouge_{key}\"] = round(value * 100, 4)\n",
    "\n",
    "            result[\"bertscore_f1\"] = round(np.mean(bert_result[\"f1\"]) * 100, 4)\n",
    "\n",
    "            return result\n",
    "\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=NEW_MODEL_OUTPUT_DIR,\n",
    "            num_train_epochs=NUM_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            logging_dir=f\"{NEW_MODEL_OUTPUT_DIR}/logs\",\n",
    "            logging_steps=50,\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=2,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            load_best_model_at_end=True,\n",
    "            metric_for_best_model=\"bertscore_f1\",\n",
    "            generation_max_length=MAX_SUMMARY_LENGTH_EVAL,\n",
    "            generation_num_beams=NUM_BEAMS_EVAL,\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        logging.info(\"Starting training from scratch...\")\n",
    "        trainer.train()\n",
    "        logging.info(\"Training finished successfully.\")\n",
    "        \n",
    "        final_model_path = os.path.join(NEW_MODEL_OUTPUT_DIR, \"final_model\")\n",
    "        trainer.save_model(final_model_path)\n",
    "        logging.info(f\"Best model saved to {final_model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during the main process: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe0fe99f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 16:56:53,044 [INFO] - --- Performing Pre-Run Checks ---\n",
      "2025-10-06 16:56:53,176 [INFO] - --- All pre-run checks passed. ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a264771644354a03acc3127a7b2fa80b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d5166934af482b85208b395000126a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef4ba9b81ec493794507efd95567c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_21372\\3005676553.py:206: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "2025-10-06 16:58:34,019 [INFO] - Starting training from scratch...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='175' max='10380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  175/10380 02:28 < 2:25:39, 1.17 it/s, Epoch 0.08/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 228\u001b[0m\n\u001b[0;32m    225\u001b[0m         logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn unexpected error occurred during the main process: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 228\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 217\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    206\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m    207\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    208\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    213\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m    214\u001b[0m )\n\u001b[0;32m    216\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training from scratch...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 217\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining finished successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m# Manually find and save the best model from all checkpoints\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:2328\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2326\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:2672\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2665\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2666\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2667\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2668\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2669\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2670\u001b[0m )\n\u001b[0;32m   2671\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2672\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2675\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2676\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2677\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2678\u001b[0m ):\n\u001b[0;32m   2679\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2680\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:4060\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   4057\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   4058\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 4060\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   4062\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\accelerate\\accelerator.py:2730\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2728\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2729\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2730\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    MT5ForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = \"google/mt5-base\"\n",
    "NEW_MODEL_OUTPUT_DIR = \"mt5-base-cnn-summarizer-en-hi_v8\"\n",
    "NEW_DATA_PATH = \"../Dataset/new_large_CNN_dataset.csv\"\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "WEIGHT_DECAY = 0.25\n",
    "NUM_BEAMS_EVAL = 6\n",
    "MAX_SUMMARY_LENGTH_EVAL = 256\n",
    "METRIC_FOR_BEST_MODEL = \"bertscore_f1\" # Define metric for manual saving\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"scratch_training_log_v8_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "    handlers=[logging.FileHandler(log_filename), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "def pre_run_checks(data_path, output_dir):\n",
    "    \"\"\"Performs checks for data path and output permissions before starting.\"\"\"\n",
    "    logging.info(\"--- Performing Pre-Run Checks ---\")\n",
    "    all_checks_passed = True\n",
    "\n",
    "    if not os.path.isfile(data_path):\n",
    "        logging.error(f\"Data file not found: {data_path}\")\n",
    "        all_checks_passed = False\n",
    "\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        test_file_path = os.path.join(output_dir, \".permission_test\")\n",
    "        with open(test_file_path, \"w\") as f:\n",
    "            f.write(\"test\")\n",
    "        os.remove(test_file_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Output directory '{output_dir}' is not writable. Error: {e}\")\n",
    "        all_checks_passed = False\n",
    "\n",
    "    if all_checks_passed:\n",
    "        logging.info(\"--- All pre-run checks passed. ---\")\n",
    "    else:\n",
    "        logging.error(\"--- Pre-run checks failed. Halting execution. ---\")\n",
    "\n",
    "    return all_checks_passed\n",
    "\n",
    "def find_and_save_best_model(output_dir, metric_name):\n",
    "    \"\"\"Finds the best checkpoint from trainer_state.json and saves it.\"\"\"\n",
    "    try:\n",
    "        state_path = os.path.join(output_dir, \"trainer_state.json\")\n",
    "        with open(state_path, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "        \n",
    "        best_metric_value = None\n",
    "        best_checkpoint_path = None\n",
    "        metric_to_check = f\"eval_{metric_name}\"\n",
    "        is_loss = 'loss' in metric_to_check\n",
    "\n",
    "        for log in state[\"log_history\"]:\n",
    "            if metric_to_check in log:\n",
    "                metric_value = log[metric_to_check]\n",
    "                if best_metric_value is None or \\\n",
    "                   (is_loss and metric_value < best_metric_value) or \\\n",
    "                   (not is_loss and metric_value > best_metric_value):\n",
    "                    best_metric_value = metric_value\n",
    "                    step = log.get('step')\n",
    "                    if step:\n",
    "                        potential_path = os.path.join(output_dir, f\"checkpoint-{step}\")\n",
    "                        if os.path.exists(potential_path):\n",
    "                            best_checkpoint_path = potential_path\n",
    "\n",
    "        if not best_checkpoint_path:\n",
    "            logging.error(\"Could not find the best checkpoint from the logs.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"Best checkpoint found: {best_checkpoint_path} with {metric_to_check}: {best_metric_value}\")\n",
    "\n",
    "        final_model_path = os.path.join(output_dir, \"final_model\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            shutil.rmtree(final_model_path)\n",
    "            \n",
    "        shutil.copytree(best_checkpoint_path, final_model_path)\n",
    "        logging.info(f\"Best model copied to {final_model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not save the best model due to: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    if not pre_run_checks(NEW_DATA_PATH, NEW_MODEL_OUTPUT_DIR):\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        # Using T5Tokenizer to match the 'google/mt5-base' checkpoint\n",
    "        # Setting legacy=False to address potential tokenizer bugs\n",
    "        tokenizer = T5Tokenizer.from_pretrained(BASE_MODEL_PATH, legacy=False)\n",
    "        model = MT5ForConditionalGeneration.from_pretrained(BASE_MODEL_PATH)\n",
    "\n",
    "        df_new = pd.read_csv(NEW_DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        df_new.dropna(subset=['raw_news_article', 'english_summary', 'hindi_summary'], inplace=True)\n",
    "        \n",
    "        df_new = df_new[df_new['raw_news_article'].str.strip().astype(bool)]\n",
    "        df_new = df_new[df_new['english_summary'].str.strip().astype(bool)]\n",
    "        df_new = df_new[df_new['hindi_summary'].str.strip().astype(bool)]\n",
    "\n",
    "        df_new.reset_index(drop=True, inplace=True)\n",
    "        raw_dataset = Dataset.from_pandas(df_new)\n",
    "\n",
    "        PREFIX_ENG = \"summarize English: \"\n",
    "        PREFIX_HIN = \"summarize Hindi: \"\n",
    "\n",
    "        def format_dataset(batch):\n",
    "            inputs, targets = [], []\n",
    "            for article, eng_summary, hin_summary in zip(\n",
    "                batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']\n",
    "            ):\n",
    "                if isinstance(article, str):\n",
    "                    inputs.append(PREFIX_ENG + article)\n",
    "                    targets.append(eng_summary)\n",
    "                    inputs.append(PREFIX_HIN + article)\n",
    "                    targets.append(hin_summary)\n",
    "            return {'inputs': inputs, 'targets': targets}\n",
    "\n",
    "        processed_dataset = raw_dataset.map(\n",
    "            format_dataset, batched=True, remove_columns=raw_dataset.column_names\n",
    "        ).flatten()\n",
    "\n",
    "        train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        final_datasets = DatasetDict({\n",
    "            'train': train_test_split['train'],\n",
    "            'test': train_test_split['test']\n",
    "        })\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            # Modern way to tokenize inputs and labels separately\n",
    "            model_inputs = tokenizer(examples['inputs'], max_length=1024, truncation=True)\n",
    "            labels = tokenizer(text_target=examples['targets'], max_length=MAX_SUMMARY_LENGTH_EVAL, truncation=True)\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "\n",
    "        tokenized_datasets = final_datasets.map(tokenize_function, batched=True, remove_columns=['inputs', 'targets'])\n",
    "        \n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "        bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "            bert_result = bertscore_metric.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "            \n",
    "            result = {}\n",
    "            for key, value in rouge_result.items():\n",
    "                result[f\"rouge_{key}\"] = round(value * 100, 4)\n",
    "\n",
    "            result[\"bertscore_f1\"] = round(np.mean(bert_result[\"f1\"]) * 100, 4)\n",
    "\n",
    "            return result\n",
    "\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=NEW_MODEL_OUTPUT_DIR,\n",
    "            num_train_epochs=NUM_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            logging_dir=f\"{NEW_MODEL_OUTPUT_DIR}/logs\",\n",
    "            logging_steps=50,\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=NUM_EPOCHS, # Save all checkpoints to find the best one\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            load_best_model_at_end=False, # Set to False for compatibility\n",
    "            generation_max_length=MAX_SUMMARY_LENGTH_EVAL,\n",
    "            generation_num_beams=NUM_BEAMS_EVAL,\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        logging.info(\"Starting training from scratch...\")\n",
    "        trainer.train()\n",
    "        logging.info(\"Training finished successfully.\")\n",
    "        \n",
    "        # Manually find and save the best model from all checkpoints\n",
    "        logging.info(\"Finding and saving the best model...\")\n",
    "        find_and_save_best_model(NEW_MODEL_OUTPUT_DIR, METRIC_FOR_BEST_MODEL)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during the main process: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa2bd828",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 17:02:38,784 [INFO] - --- RUNNING SANITY CHECK WITH BUILT-IN OFFLINE DATASET ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42a4915c04df4cb496f6291fab43e42c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9367cb056cbf49e7aba38b6094e4f8ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_21372\\2052585587.py:161: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "2025-10-06 17:02:42,479 [INFO] - Starting training from scratch on OFFLINE dataset...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 01:50, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>14.190800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 17:04:34,222 [INFO] - Training finished successfully.\n",
      "2025-10-06 17:04:34,224 [INFO] - Sanity check complete. If loss decreased, the code is working.\n",
      "2025-10-06 17:04:34,224 [INFO] - You can now re-enable your local dataset and investigate it for issues.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    MT5ForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = \"google/mt5-base\"\n",
    "NEW_MODEL_OUTPUT_DIR = \"mt5-base-cnn-summarizer-en-hi_v8\"\n",
    "# NEW_DATA_PATH = \"../Dataset/new_large_CNN_dataset.csv\" # Disabled for sanity check\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "WEIGHT_DECAY = 0.25\n",
    "NUM_BEAMS_EVAL = 6\n",
    "MAX_SUMMARY_LENGTH_EVAL = 256\n",
    "METRIC_FOR_BEST_MODEL = \"rouge_rouge1\" # Using ROUGE as it's simpler for this check\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"scratch_training_log_v8_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "    handlers=[logging.FileHandler(log_filename), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "def find_and_save_best_model(output_dir, metric_name):\n",
    "    \"\"\"Finds the best checkpoint from trainer_state.json and saves it.\"\"\"\n",
    "    try:\n",
    "        state_path = os.path.join(output_dir, \"trainer_state.json\")\n",
    "        with open(state_path, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "        \n",
    "        best_metric_value = None\n",
    "        best_checkpoint_path = None\n",
    "        metric_to_check = f\"eval_{metric_name}\"\n",
    "        is_loss = 'loss' in metric_to_check\n",
    "\n",
    "        for log in state[\"log_history\"]:\n",
    "            if metric_to_check in log:\n",
    "                metric_value = log[metric_to_check]\n",
    "                if best_metric_value is None or \\\n",
    "                   (is_loss and metric_value < best_metric_value) or \\\n",
    "                   (not is_loss and metric_value > best_metric_value):\n",
    "                    best_metric_value = metric_value\n",
    "                    step = log.get('step')\n",
    "                    if step:\n",
    "                        potential_path = os.path.join(output_dir, f\"checkpoint-{step}\")\n",
    "                        if os.path.exists(potential_path):\n",
    "                            best_checkpoint_path = potential_path\n",
    "\n",
    "        if not best_checkpoint_path:\n",
    "            logging.error(\"Could not find the best checkpoint from the logs.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"Best checkpoint found: {best_checkpoint_path} with {metric_to_check}: {best_metric_value}\")\n",
    "\n",
    "        final_model_path = os.path.join(output_dir, \"final_model\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            shutil.rmtree(final_model_path)\n",
    "            \n",
    "        shutil.copytree(best_checkpoint_path, final_model_path)\n",
    "        logging.info(f\"Best model copied to {final_model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not save the best model due to: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        tokenizer = T5Tokenizer.from_pretrained(BASE_MODEL_PATH, legacy=False)\n",
    "        model = MT5ForConditionalGeneration.from_pretrained(BASE_MODEL_PATH)\n",
    "\n",
    "        # --- SANITY CHECK: Using a built-in, offline dataset ---\n",
    "        logging.info(\"--- RUNNING SANITY CHECK WITH BUILT-IN OFFLINE DATASET ---\")\n",
    "        \n",
    "        # Create a small, clean dataset in memory to bypass network issues\n",
    "        dummy_data = {\n",
    "            \"train\": {\n",
    "                \"dialogue\": [\n",
    "                    \"Amanda: I baked cookies. Do you want some?\\nJerry: Sure!\\nAmanda: I'll bring you some tomorrow.\",\n",
    "                    \"Olivia: I'm so tired. I stayed up all night studying.\\nLeo: You should get some rest.\"\n",
    "                ],\n",
    "                \"summary\": [\n",
    "                    \"Amanda baked cookies and will bring Jerry some tomorrow.\",\n",
    "                    \"Olivia is tired from studying and Leo suggests she should rest.\"\n",
    "                ]\n",
    "            },\n",
    "            \"test\": {\n",
    "                \"dialogue\": [\n",
    "                    \"Will: I'm going to the store. Do you need anything?\\nJane: Yes, can you get some milk?\"\n",
    "                ],\n",
    "                \"summary\": [\n",
    "                    \"Jane needs milk from the store.\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        train_dataset = Dataset.from_dict(dummy_data[\"train\"])\n",
    "        test_dataset = Dataset.from_dict(dummy_data[\"test\"])\n",
    "        offline_dataset = DatasetDict({\"train\": train_dataset, \"test\": test_dataset})\n",
    "        \n",
    "        PREFIX = \"summarize: \"\n",
    "\n",
    "        def format_offline_dataset(examples):\n",
    "            inputs = [PREFIX + doc for doc in examples[\"dialogue\"]]\n",
    "            model_inputs = tokenizer(inputs, max_length=512, truncation=True) # Reduced length for dummy data\n",
    "            labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "\n",
    "        tokenized_datasets = offline_dataset.map(format_offline_dataset, batched=True)\n",
    "        \n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "            return {f\"rouge_{key}\": value for key, value in rouge_result.items()}\n",
    "\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=NEW_MODEL_OUTPUT_DIR,\n",
    "            num_train_epochs=3, # Reduced epochs for a quick test\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            per_device_train_batch_size=1, # Smaller batch for tiny dataset\n",
    "            per_device_eval_batch_size=1,\n",
    "            gradient_accumulation_steps=1,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            logging_dir=f\"{NEW_MODEL_OUTPUT_DIR}/logs\",\n",
    "            logging_steps=1,\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=1,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            load_best_model_at_end=False,\n",
    "            generation_max_length=MAX_SUMMARY_LENGTH_EVAL,\n",
    "            generation_num_beams=NUM_BEAMS_EVAL,\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        logging.info(\"Starting training from scratch on OFFLINE dataset...\")\n",
    "        trainer.train()\n",
    "        logging.info(\"Training finished successfully.\")\n",
    "        \n",
    "        logging.info(\"Sanity check complete. If loss decreased, the code is working.\")\n",
    "        logging.info(\"You can now re-enable your local dataset and investigate it for issues.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during the main process: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "790623f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 17:07:20,969 [INFO] - --- Starting Data Validation ---\n",
      "2025-10-06 17:07:20,985 [INFO] - --- Data Validation Passed. No obvious issues found in the first {num_rows_to_check} rows. ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ef27d3eae444f6da58d959fbad7b82b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e224766a194e28b49f7b0c35b23086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dbaf05e470a48258e247ab33d3109d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_21372\\2800903324.py:209: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "2025-10-06 17:08:49,766 [INFO] - Starting training from scratch...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='211' max='10380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  211/10380 02:57 < 2:24:00, 1.18 it/s, Epoch 0.10/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 230\u001b[0m\n\u001b[0;32m    227\u001b[0m         logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn unexpected error occurred during the main process: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 230\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 220\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    209\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m    210\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    211\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    216\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m    217\u001b[0m )\n\u001b[0;32m    219\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training from scratch...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 220\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    221\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining finished successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    223\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinding and saving the best model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:2328\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2326\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:2672\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2665\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2666\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2667\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2668\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2669\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2670\u001b[0m )\n\u001b[0;32m   2671\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2672\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2675\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2676\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2677\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2678\u001b[0m ):\n\u001b[0;32m   2679\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2680\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:4060\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   4057\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   4058\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 4060\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   4062\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\accelerate\\accelerator.py:2730\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2728\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2729\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2730\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    MT5ForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = \"google/mt5-base\"\n",
    "NEW_MODEL_OUTPUT_DIR = \"mt5-base-cnn-summarizer-en-hi_v8\"\n",
    "NEW_DATA_PATH = \"../Dataset/new_large_CNN_dataset.csv\" # Re-enabled user's data path\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "WEIGHT_DECAY = 0.25\n",
    "NUM_BEAMS_EVAL = 6\n",
    "MAX_SUMMARY_LENGTH_EVAL = 256\n",
    "METRIC_FOR_BEST_MODEL = \"bertscore_f1\"\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"scratch_training_log_v8_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "    handlers=[logging.FileHandler(log_filename), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "def data_validation_check(df, num_rows_to_check=100):\n",
    "    \"\"\"\n",
    "    Performs validation checks on the dataframe to find potential data leakage.\n",
    "    \"\"\"\n",
    "    logging.info(\"--- Starting Data Validation ---\")\n",
    "    is_issue_found = False\n",
    "    for i, row in df.head(num_rows_to_check).iterrows():\n",
    "        article = str(row['raw_news_article'])\n",
    "        eng_summary = str(row['english_summary'])\n",
    "        hin_summary = str(row['hindi_summary'])\n",
    "\n",
    "        # Check for summary text within the article text\n",
    "        if eng_summary in article:\n",
    "            logging.warning(f\"[DATA VALIDATION WARNING] Row {i}: English summary found in article text.\")\n",
    "            is_issue_found = True\n",
    "        if hin_summary in article:\n",
    "            logging.warning(f\"[DATA VALIDATION WARNING] Row {i}: Hindi summary found in article text.\")\n",
    "            is_issue_found = True\n",
    "        \n",
    "        # Check for unusually short content\n",
    "        if len(article.split()) < 20:\n",
    "            logging.warning(f\"[DATA VALIDATION WARNING] Row {i}: Article text is very short ({len(article.split())} words).\")\n",
    "            is_issue_found = True\n",
    "        if len(eng_summary.split()) < 5:\n",
    "            logging.warning(f\"[DATA VALIDATION WARNING] Row {i}: English summary is very short ({len(eng_summary.split())} words).\")\n",
    "            is_issue_found = True\n",
    "\n",
    "    if not is_issue_found:\n",
    "        logging.info(\"--- Data Validation Passed. No obvious issues found in the first {num_rows_to_check} rows. ---\")\n",
    "    else:\n",
    "        logging.error(\"--- Data Validation Failed. Please review warnings above and clean your CSV file. ---\")\n",
    "    return not is_issue_found\n",
    "\n",
    "\n",
    "def find_and_save_best_model(output_dir, metric_name):\n",
    "    \"\"\"Finds the best checkpoint from trainer_state.json and saves it.\"\"\"\n",
    "    try:\n",
    "        state_path = os.path.join(output_dir, \"trainer_state.json\")\n",
    "        with open(state_path, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "        \n",
    "        best_metric_value = None\n",
    "        best_checkpoint_path = None\n",
    "        metric_to_check = f\"eval_{metric_name}\"\n",
    "        is_loss = 'loss' in metric_to_check\n",
    "\n",
    "        for log in state[\"log_history\"]:\n",
    "            if metric_to_check in log:\n",
    "                metric_value = log[metric_to_check]\n",
    "                if best_metric_value is None or \\\n",
    "                   (is_loss and metric_value < best_metric_value) or \\\n",
    "                   (not is_loss and metric_value > best_metric_value):\n",
    "                    best_metric_value = metric_value\n",
    "                    step = log.get('step')\n",
    "                    if step:\n",
    "                        potential_path = os.path.join(output_dir, f\"checkpoint-{step}\")\n",
    "                        if os.path.exists(potential_path):\n",
    "                            best_checkpoint_path = potential_path\n",
    "\n",
    "        if not best_checkpoint_path:\n",
    "            logging.error(\"Could not find the best checkpoint from the logs.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"Best checkpoint found: {best_checkpoint_path} with {metric_to_check}: {best_metric_value}\")\n",
    "\n",
    "        final_model_path = os.path.join(output_dir, \"final_model\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            shutil.rmtree(final_model_path)\n",
    "            \n",
    "        shutil.copytree(best_checkpoint_path, final_model_path)\n",
    "        logging.info(f\"Best model copied to {final_model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not save the best model due to: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        tokenizer = T5Tokenizer.from_pretrained(BASE_MODEL_PATH, legacy=False)\n",
    "        model = MT5ForConditionalGeneration.from_pretrained(BASE_MODEL_PATH)\n",
    "\n",
    "        # Load and validate the user's dataset\n",
    "        df_new = pd.read_csv(NEW_DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        df_new.dropna(subset=['raw_news_article', 'english_summary', 'hindi_summary'], inplace=True)\n",
    "        \n",
    "        if not data_validation_check(df_new):\n",
    "            logging.error(\"Halting execution due to data validation issues.\")\n",
    "            return\n",
    "\n",
    "        df_new.reset_index(drop=True, inplace=True)\n",
    "        raw_dataset = Dataset.from_pandas(df_new)\n",
    "\n",
    "        PREFIX_ENG = \"summarize English: \"\n",
    "        PREFIX_HIN = \"summarize Hindi: \"\n",
    "\n",
    "        def format_dataset(batch):\n",
    "            inputs, targets = [], []\n",
    "            for article, eng_summary, hin_summary in zip(\n",
    "                batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']\n",
    "            ):\n",
    "                if isinstance(article, str):\n",
    "                    inputs.append(PREFIX_ENG + article)\n",
    "                    targets.append(eng_summary)\n",
    "                    inputs.append(PREFIX_HIN + article)\n",
    "                    targets.append(hin_summary)\n",
    "            return {'inputs': inputs, 'targets': targets}\n",
    "\n",
    "        processed_dataset = raw_dataset.map(\n",
    "            format_dataset, batched=True, remove_columns=raw_dataset.column_names\n",
    "        ).flatten()\n",
    "\n",
    "        train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        final_datasets = DatasetDict({\n",
    "            'train': train_test_split['train'],\n",
    "            'test': train_test_split['test']\n",
    "        })\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            model_inputs = tokenizer(examples['inputs'], max_length=1024, truncation=True)\n",
    "            labels = tokenizer(text_target=examples['targets'], max_length=MAX_SUMMARY_LENGTH_EVAL, truncation=True)\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "\n",
    "        tokenized_datasets = final_datasets.map(tokenize_function, batched=True, remove_columns=['inputs', 'targets'])\n",
    "        \n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "        bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "            bert_result = bertscore_metric.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "            \n",
    "            result = {}\n",
    "            for key, value in rouge_result.items():\n",
    "                result[f\"rouge_{key}\"] = round(value * 100, 4)\n",
    "\n",
    "            result[\"bertscore_f1\"] = round(np.mean(bert_result[\"f1\"]) * 100, 4)\n",
    "\n",
    "            return result\n",
    "\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=NEW_MODEL_OUTPUT_DIR,\n",
    "            num_train_epochs=NUM_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            logging_dir=f\"{NEW_MODEL_OUTPUT_DIR}/logs\",\n",
    "            logging_steps=50,\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=NUM_EPOCHS,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            load_best_model_at_end=False,\n",
    "            generation_max_length=MAX_SUMMARY_LENGTH_EVAL,\n",
    "            generation_num_beams=NUM_BEAMS_EVAL,\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        logging.info(\"Starting training from scratch...\")\n",
    "        trainer.train()\n",
    "        logging.info(\"Training finished successfully.\")\n",
    "        \n",
    "        logging.info(\"Finding and saving the best model...\")\n",
    "        find_and_save_best_model(NEW_MODEL_OUTPUT_DIR, METRIC_FOR_BEST_MODEL)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during the main process: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "436fce80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 17:12:27,685 [INFO] - --- Starting Aggressive Data Cleaning to Remove Leaks ---\n",
      "2025-10-06 17:12:28,093 [INFO] - --- Aggressive Data Cleaning Finished ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8838da50ebe84c6cb47726b493438903",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6bd40464a4cf432f828f381638792577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2532f6012c2e4b708369a56798ac9541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_21372\\4061990187.py:188: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "2025-10-06 17:13:57,977 [INFO] - Starting training from scratch...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='227' max='10380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  227/10380 03:11 < 2:24:18, 1.17 it/s, Epoch 0.11/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 209\u001b[0m\n\u001b[0;32m    206\u001b[0m         logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn unexpected error occurred during the main process: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 209\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[8], line 199\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    188\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m    189\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    190\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    195\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m    196\u001b[0m )\n\u001b[0;32m    198\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training from scratch...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 199\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining finished successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    202\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinding and saving the best model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:2328\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2326\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:2672\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2665\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2666\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2667\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2668\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2669\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2670\u001b[0m )\n\u001b[0;32m   2671\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2672\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2675\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2676\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2677\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2678\u001b[0m ):\n\u001b[0;32m   2679\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2680\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:4009\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   4006\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   4008\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 4009\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4011\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   4012\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4013\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4014\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   4015\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:4099\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   4097\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   4098\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m-> 4099\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   4100\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   4101\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   4102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\accelerate\\utils\\operations.py:818\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\accelerate\\utils\\operations.py:806\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\amp\\autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\models\\mt5\\modeling_mt5.py:1786\u001b[0m, in \u001b[0;36mMT5ForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1783\u001b[0m         decoder_attention_mask \u001b[38;5;241m=\u001b[39m decoder_attention_mask\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder\u001b[38;5;241m.\u001b[39mfirst_device)\n\u001b[0;32m   1785\u001b[0m \u001b[38;5;66;03m# Decode\u001b[39;00m\n\u001b[1;32m-> 1786\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1787\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1789\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1795\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1796\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1797\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1800\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1802\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m decoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1804\u001b[0m \u001b[38;5;66;03m# Set device for model parallelism\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\models\\mt5\\modeling_mt5.py:1086\u001b[0m, in \u001b[0;36mMT5Stack.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1083\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[0;32m   1084\u001b[0m     all_hidden_states \u001b[38;5;241m=\u001b[39m all_hidden_states \u001b[38;5;241m+\u001b[39m (hidden_states,)\n\u001b[1;32m-> 1086\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1090\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1091\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1092\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[0;32m   1093\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1094\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1095\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1096\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1097\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1098\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1099\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1100\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1102\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1104\u001b[0m \u001b[38;5;66;03m# We share the position biases between the layers - the first layer store them\u001b[39;00m\n\u001b[0;32m   1105\u001b[0m \u001b[38;5;66;03m# layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),\u001b[39;00m\n\u001b[0;32m   1106\u001b[0m \u001b[38;5;66;03m# (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\models\\mt5\\modeling_mt5.py:614\u001b[0m, in \u001b[0;36mMT5Block.forward\u001b[1;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_values, use_cache, output_attentions, return_dict, cache_position)\u001b[0m\n\u001b[0;32m    611\u001b[0m     attention_outputs \u001b[38;5;241m=\u001b[39m attention_outputs \u001b[38;5;241m+\u001b[39m cross_attention_outputs[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m    613\u001b[0m \u001b[38;5;66;03m# Apply Feed Forward layer\u001b[39;00m\n\u001b[1;32m--> 614\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    616\u001b[0m \u001b[38;5;66;03m# clamp inf values to enable fp16 training\u001b[39;00m\n\u001b[0;32m    617\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hidden_states\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\models\\mt5\\modeling_mt5.py:218\u001b[0m, in \u001b[0;36mMT5LayerFF.forward\u001b[1;34m(self, hidden_states)\u001b[0m\n\u001b[0;32m    216\u001b[0m forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayer_norm(hidden_states)\n\u001b[0;32m    217\u001b[0m forwarded_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDenseReluDense(forwarded_states)\n\u001b[1;32m--> 218\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mforwarded_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1749\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1757\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1758\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1760\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1761\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1762\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1764\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1765\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\modules\\dropout.py:70\u001b[0m, in \u001b[0;36mDropout.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\functional.py:1425\u001b[0m, in \u001b[0;36mdropout\u001b[1;34m(input, p, training, inplace)\u001b[0m\n\u001b[0;32m   1422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1.0\u001b[39m:\n\u001b[0;32m   1423\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1424\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m-> 1425\u001b[0m     _VF\u001b[38;5;241m.\u001b[39mdropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1426\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    MT5ForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = \"google/mt5-base\"\n",
    "NEW_MODEL_OUTPUT_DIR = \"mt5-base-cnn-summarizer-en-hi_v8\"\n",
    "NEW_DATA_PATH = \"../Dataset/new_large_CNN_dataset.csv\" \n",
    "\n",
    "# --- Hyperparameters ---\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "WEIGHT_DECAY = 0.25\n",
    "NUM_BEAMS_EVAL = 6\n",
    "MAX_SUMMARY_LENGTH_EVAL = 256\n",
    "METRIC_FOR_BEST_MODEL = \"bertscore_f1\"\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"scratch_training_log_v8_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "    handlers=[logging.FileHandler(log_filename), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "\n",
    "def find_and_save_best_model(output_dir, metric_name):\n",
    "    \"\"\"Finds the best checkpoint from trainer_state.json and saves it.\"\"\"\n",
    "    try:\n",
    "        state_path = os.path.join(output_dir, \"trainer_state.json\")\n",
    "        with open(state_path, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "        \n",
    "        best_metric_value = None\n",
    "        best_checkpoint_path = None\n",
    "        metric_to_check = f\"eval_{metric_name}\"\n",
    "        is_loss = 'loss' in metric_to_check\n",
    "\n",
    "        for log in state[\"log_history\"]:\n",
    "            if metric_to_check in log:\n",
    "                metric_value = log[metric_to_check]\n",
    "                if best_metric_value is None or \\\n",
    "                   (is_loss and metric_value < best_metric_value) or \\\n",
    "                   (not is_loss and metric_value > best_metric_value):\n",
    "                    best_metric_value = metric_value\n",
    "                    step = log.get('step')\n",
    "                    if step:\n",
    "                        potential_path = os.path.join(output_dir, f\"checkpoint-{step}\")\n",
    "                        if os.path.exists(potential_path):\n",
    "                            best_checkpoint_path = potential_path\n",
    "\n",
    "        if not best_checkpoint_path:\n",
    "            logging.error(\"Could not find the best checkpoint from the logs.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"Best checkpoint found: {best_checkpoint_path} with {metric_to_check}: {best_metric_value}\")\n",
    "\n",
    "        final_model_path = os.path.join(output_dir, \"final_model\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            shutil.rmtree(final_model_path)\n",
    "            \n",
    "        shutil.copytree(best_checkpoint_path, final_model_path)\n",
    "        logging.info(f\"Best model copied to {final_model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not save the best model due to: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        tokenizer = T5Tokenizer.from_pretrained(BASE_MODEL_PATH, legacy=False)\n",
    "        model = MT5ForConditionalGeneration.from_pretrained(BASE_MODEL_PATH)\n",
    "\n",
    "        # Load the user's dataset\n",
    "        df_new = pd.read_csv(NEW_DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        df_new.dropna(subset=['raw_news_article', 'english_summary', 'hindi_summary'], inplace=True)\n",
    "        \n",
    "        # --- NEW: Aggressive Data Cleaning Step ---\n",
    "        logging.info(\"--- Starting Aggressive Data Cleaning to Remove Leaks ---\")\n",
    "        cleaned_articles = []\n",
    "        for i, row in df_new.iterrows():\n",
    "            article = str(row['raw_news_article'])\n",
    "            eng_summary = str(row['english_summary'])\n",
    "            hin_summary = str(row['hindi_summary'])\n",
    "            \n",
    "            # Surgically remove summary text from the article text\n",
    "            article = article.replace(eng_summary, \"\")\n",
    "            article = article.replace(hin_summary, \"\")\n",
    "            cleaned_articles.append(article)\n",
    "        \n",
    "        df_new['raw_news_article'] = cleaned_articles\n",
    "        logging.info(\"--- Aggressive Data Cleaning Finished ---\")\n",
    "\n",
    "        df_new.reset_index(drop=True, inplace=True)\n",
    "        raw_dataset = Dataset.from_pandas(df_new)\n",
    "\n",
    "        PREFIX_ENG = \"summarize English: \"\n",
    "        PREFIX_HIN = \"summarize Hindi: \"\n",
    "\n",
    "        def format_dataset(batch):\n",
    "            inputs, targets = [], []\n",
    "            for article, eng_summary, hin_summary in zip(\n",
    "                batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']\n",
    "            ):\n",
    "                if isinstance(article, str):\n",
    "                    inputs.append(PREFIX_ENG + article)\n",
    "                    targets.append(eng_summary)\n",
    "                    inputs.append(PREFIX_HIN + article)\n",
    "                    targets.append(hin_summary)\n",
    "            return {'inputs': inputs, 'targets': targets}\n",
    "\n",
    "        processed_dataset = raw_dataset.map(\n",
    "            format_dataset, batched=True, remove_columns=raw_dataset.column_names\n",
    "        ).flatten()\n",
    "\n",
    "        train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        final_datasets = DatasetDict({\n",
    "            'train': train_test_split['train'],\n",
    "            'test': train_test_split['test']\n",
    "        })\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            model_inputs = tokenizer(examples['inputs'], max_length=1024, truncation=True)\n",
    "            labels = tokenizer(text_target=examples['targets'], max_length=MAX_SUMMARY_LENGTH_EVAL, truncation=True)\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "\n",
    "        tokenized_datasets = final_datasets.map(tokenize_function, batched=True, remove_columns=['inputs', 'targets'])\n",
    "        \n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "        bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "            bert_result = bertscore_metric.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "            \n",
    "            result = {}\n",
    "            for key, value in rouge_result.items():\n",
    "                result[f\"rouge_{key}\"] = round(value * 100, 4)\n",
    "\n",
    "            result[\"bertscore_f1\"] = round(np.mean(bert_result[\"f1\"]) * 100, 4)\n",
    "\n",
    "            return result\n",
    "\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=NEW_MODEL_OUTPUT_DIR,\n",
    "            num_train_epochs=NUM_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            logging_dir=f\"{NEW_MODEL_OUTPUT_DIR}/logs\",\n",
    "            logging_steps=50,\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=NUM_EPOCHS,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            load_best_model_at_end=False,\n",
    "            generation_max_length=MAX_SUMMARY_LENGTH_EVAL,\n",
    "            generation_num_beams=NUM_BEAMS_EVAL,\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        logging.info(\"Starting training from scratch...\")\n",
    "        trainer.train()\n",
    "        logging.info(\"Training finished successfully.\")\n",
    "        \n",
    "        logging.info(\"Finding and saving the best model...\")\n",
    "        find_and_save_best_model(NEW_MODEL_OUTPUT_DIR, METRIC_FOR_BEST_MODEL)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during the main process: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3984e8bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 17:17:35,116 [INFO] - --- Starting Aggressive Data Cleaning to Remove Leaks ---\n",
      "2025-10-06 17:17:35,521 [INFO] - --- Aggressive Data Cleaning Finished ---\n",
      "2025-10-06 17:17:35,532 [INFO] - \n",
      "\n",
      "==================== GROUND TRUTH DIAGNOSTIC ====================\n",
      "2025-10-06 17:17:35,534 [INFO] - --- Cleaned Article (First Row) ---\n",
      "Voters are still 'in the dark' about the scale and depth of spending cuts being planned by all the main parties with just two weeks until polling day, economic experts warned today. Analysts from the Institute for Fiscal Studies said none of the major parties had given 'anything like full details' on how they will tackle the nations' debts after the election. The Tories were accused of giving 'no detail' about their deficit reduction plan, which relies on £30billion of cuts, while Labour has left the door open to borrowing an extra £26billion-a-year. Scroll down for video . The Institute for Fiscal Studies analysed the policies of all the main parties to see how their policies would increase borrowing . The IFS warned that the promise of tackling the deficit in the next Parliament is based on 'almost entirely unspecified spending cuts and tax increases'. The think-tank reached its conclusions after a detailed study of the party manifestos ahead of May's General Election. It accused Tory Chancellor George Osborne and his Labour opponent Ed Balls of failing to spell out exactly where the axe will fall. IFS deputy director Carl Emmerson said: 'There are genuinely big differences between the main parties' fiscal plans. 'The electorate has a real choice, although it can at best see only the broad outlines of that choice. 'Conservative plans involve a significantly larger reduction in borrowing and debt than Labour plans. 'But they are predicated on substantial and almost entirely unspecified spending cuts and tax increases. 'While Labour has been considerably less clear about its overall fiscal ambition, its stated position appears to be consistent with little in the way of further spending cuts after this year.' Chancellor George Osborne and his Labour opponent Ed Balls are accused of not spelling out how they will tackle the deficit . George Osborne received a pre-election boost today as official figures showed he beat his target for reducing annual public sector borrowing for the latest financial year by nearly £3 billion. Borrowing - excluding the effect of bank bailouts - was £87.3 billion for the year to the end of March, down from £98.5 billion in 2013/14, according to the Office for National Statistics (ONS). The result undershot the latest target of £90.2 billion set by the independent Office for Budget Responsibility (OBR) at the time of last month's Budget. It means that annual borrowing (GDP) has fallen by more than £60 billion from £153.5 billion in 2009/10 just before the Coalition came to power. As a percentage of gross domestic product (GDP) it has dropped by half from 10.2 per cent to 4.8 per cent. However, underlying debt of £1.48 trillion is more than £500 billion higher than the 2009/10 figure of £956 billion. The nation's debt represents 80.4 per cent of GDP, up from 62 per cent five years ago. The IFS analysis said the Tories planned the largest reduction in borrowing over the course of the next Parliament. It said the party would require large spending cuts or tax increases to achieve this. Research economist Soumaya Keynes said: 'The Conservatives have said they want to eliminate the deficit but provided next to no detail on how they would do it. 'They should be forthcoming on the £5 billion of largely unspecified clampdown on tax avoidance, the £10 billion of unspecified cuts to social security spending and, according to our calculations, further real cuts to unprotected departments of around £30 billion.' Turning to Labour, the IFS said the Opposition had been 'considerably more vague' about how much it wants to borrow. The pledge to produce a surplus but without specifying by when or how much could be consistent with a reduction in borrowing totalling 3.6 per cent of national income. Senior research economist Rowena Crawford said: 'Labour's proposed measures might be broadly enough to meet their target for only borrowing to invest. 'But this would leave borrowing at £26billion a year in today's terms. 'If Labour wanted to reduce borrowing to a lower level than this they would have to spell out more detail of how they would get there.' The IFS said the Liberal Democrats had been more transparent about overall fiscal plans to 2017-18, revealing that they are aiming for a tightening more than Labour but less than the Conservatives. The SNP's figures imply the same reduction in borrowing as Labour, the IFS said, although the reduction would be slower. This means the SNP is proposing a slower but longer period of austerity, the think-tank said. The independent analysis came as the main parties clashed over the economy, with Labour accusing the Tories of planning the biggest spending cuts in any of the world's advanced economies and the Conservatives renewing warnings that Britain's economy would suffer from an 'SNP/Miliband nightmare'. Labour's Shadow Chancellor Ed Balls said: 'The IFS has confirmed that the Tories are committed to the most extreme spending plans of any political party, with bigger cuts than any other advanced economy in the next three years. 'The Tories might be able to make the cuts but the last five years show they will fail to cut the deficit as they claim. They have borrowed £200 billion more than they planned because their failure to boost living standards has led to tax revenues falling short.' But Mr Osborne today warned that every family in the country will be left £350 worse off if the SNP is left calling the shots in a Labour government. The Chancellor warned of a 'dangerous cocktail' if Ed Miliband is at the mercy of SNP leader Nicola Sturgeon. With polls showing no party will win an overall majority, Mr Osborne claimed it would spark a 'constitutional crisis' if Scottish nationalist MPs held the balance of power in the UK government. Launching the SNP manifesto this week, Ms Sturgeon named her price for supporting Labour in any post-election deal. She demanded her MPs have a say over the running of the whole of the UK, raising the prospect of Scottish nationalists calling the shots on policies which only apply south of the border. And she unveiled a £140billion 'ransom note' for Britain to drag Labour to the left, including an end to austerity, rising benefits payments and the abolition of Trident as the price Mr Miliband would have to pay for getting into Number 10. Mr Osborne seized on the demand for more spending and more borrowing and warned that Treasury analysis suggests it would add £6billion to Britain's interest bill, worth £350 per family. Mr Osborne told BBC Radio 4's Today programme: 'It's a new feature of this election that a Labour party cannot win an overall majority, so they would rely on this deeply unstable block of Scottish Nationalists. 'We've got a strong plan for a national recovery, but it's perfectly reasonable to point out with two weeks to go until this election the alternative is what Gordon Brown described last night as 'constitutional chaos'. 'The Scottish Nationalist leaders would clearly be the stronger force in a Labour minority government propped up by them.' Jonathan Isaby, chief executive of the TaxPayers' Alliance, said: 'The politicians seeking our votes owe it to taxpayers to come clean about what spending the country can, and cannot, afford. 'This candour has so far been noticeably absent in the election campaign, with politicians failing to acknowledge the dire state of the public finances and instead clambering over each other to make additional and apparently unfunded spending pledges.' CONSERVATIVES . LABOUR . LIB DEMS . UKIP . GREENS .\n",
      "\n",
      "2025-10-06 17:17:35,535 [INFO] - --- English Summary (First Row) ---\n",
      "Economic experts from the Institute for Fiscal Studies (IFS) warned that voters remain largely uninformed about the scale of spending cuts planned by major UK political parties just two weeks before polling day. The IFS report, based on detailed manifesto studies, criticized the Conservatives for lacking specifics on their £30 billion deficit reduction plan, which relies heavily on unspecified spending cuts and tax increases, while Labour has indicated a willingness to borrow an additional £26 billion annually. Although Chancellor George Osborne recently announced that public sector borrowing for the last financial year undershot its target by nearly £3 billion, the national debt remains over £1.48 trillion, representing 80.4% of GDP. The IFS highlighted that Tory plans involve the largest borrowing reduction but require significant, mostly unspecified, cuts, including £10 billion from social security. Labour's fiscal ambition was deemed vaguer, potentially leaving annual borrowing at £26 billion. The Liberal Democrats showed more transparency in their fiscal plans than Labour, while the SNP's figures implied a slower, longer period of austerity. The article also covered the political contention, with Labour accusing Tories of extreme spending cuts and Conservatives warning against an \"SNP/Miliband nightmare\" and potential constitutional crisis if the SNP held the balance of power, citing SNP leader Nicola Sturgeon's demands for increased spending and an end to austerity as a condition for supporting Labour, which Osborne claimed would add £6 billion to Britain's interest bill.\n",
      "\n",
      "2025-10-06 17:17:35,536 [INFO] - --- Hindi Summary (First Row) ---\n",
      "चुनाव से सिर्फ दो हफ्ते पहले, इंस्टीट्यूट फॉर फिस्कल स्टडीज (IFS) के आर्थिक विशेषज्ञों ने चेतावनी दी है कि मतदाता प्रमुख ब्रिटिश राजनीतिक दलों द्वारा नियोजित खर्च कटौती के पैमाने और गहराई के बारे में अनभिज्ञ हैं। IFS की विस्तृत घोषणापत्र अध्ययनों पर आधारित रिपोर्ट ने कंजर्वेटिव्स की £30 बिलियन की घाटा कटौती योजना पर विवरण की कमी की आलोचना की, जो बड़े पैमाने पर अनिर्दिष्ट खर्च कटौती और कर वृद्धि पर निर्भर करती है, जबकि लेबर ने प्रति वर्ष अतिरिक्त £26 बिलियन उधार लेने की इच्छा व्यक्त की है। हालांकि चांसलर जॉर्ज ओसबोर्न ने हाल ही में घोषणा की कि पिछले वित्तीय वर्ष के लिए सार्वजनिक क्षेत्र का उधार अपने लक्ष्य से लगभग £3 बिलियन कम था, फिर भी राष्ट्रीय ऋण £1.48 ट्रिलियन से अधिक है, जो सकल घरेलू उत्पाद (GDP) का 80.4% है। IFS ने बताया कि टोरी योजनाओं में सबसे बड़ी उधार कटौती शामिल है, लेकिन इसके लिए महत्वपूर्ण, काफी हद तक अनिर्दिष्ट कटौती की आवश्यकता होगी, जिसमें सामाजिक सुरक्षा से £10 बिलियन शामिल हैं। लेबर की वित्तीय महत्वाकांक्षा को अधिक अस्पष्ट माना गया, जिससे वार्षिक उधार £26 बिलियन रह सकता है। लिबरल डेमोक्रेट्स ने लेबर की तुलना में अपनी वित्तीय योजनाओं में अधिक पारदर्शिता दिखाई, जबकि स्कॉटिश नेशनल पार्टी (SNP) के आंकड़ों से किफायत (ऑस्टेरिटी) की धीमी लेकिन लंबी अवधि का संकेत मिलता है। लेख में राजनीतिक टकराव को भी शामिल किया गया है, जिसमें लेबर ने टोरियों पर अत्यधिक खर्च कटौती की योजना बनाने का आरोप लगाया, और कंजर्वेटिव्स ने \"एसएनपी/मिलीबैंड बुरे सपने\" और संभावित संवैधानिक संकट के प्रति आगाह किया यदि एसएनपी सत्ता के संतुलन को नियंत्रित करती है, जॉर्ज ओसबोर्न ने एसएनपी नेता निकोला स्टर्जन की मांगों का हवाला दिया, जिसमें लेबर सरकार का समर्थन करने की शर्त के रूप में बढ़े हुए खर्च और किफायत के अंत की मांग की गई थी, जिसके बारे में ओसबोर्न ने दावा किया कि इससे ब्रिटेन के ब्याज बिल में £6 बिलियन की वृद्धि होगी।\n",
      "\n",
      "2025-10-06 17:17:35,536 [INFO] - =================================================================\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9af2203b10bc44b18319d9410de5b201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be130d92affb4d12a82a60cce39185cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bcad83e5f314d94955dcbfb7c26ba24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_21372\\3294868241.py:197: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "2025-10-06 17:19:04,583 [INFO] - Starting training from scratch...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='114' max='10380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  114/10380 01:35 < 2:25:44, 1.17 it/s, Epoch 0.05/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 218\u001b[0m\n\u001b[0;32m    215\u001b[0m         logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn unexpected error occurred during the main process: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 218\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 208\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    197\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m    198\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    199\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    204\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m    205\u001b[0m )\n\u001b[0;32m    207\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training from scratch...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 208\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining finished successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    211\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinding and saving the best model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:2328\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2326\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:2623\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2621\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2622\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[1;32m-> 2623\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2624\u001b[0m \u001b[38;5;66;03m# Store the number of batches for current gradient accumulation\u001b[39;00m\n\u001b[0;32m   2625\u001b[0m \u001b[38;5;66;03m# This is used to correctly scale the loss when the last accumulation step has fewer batches\u001b[39;00m\n\u001b[0;32m   2626\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_gradient_accumulation_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:5581\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[1;34m(self, epoch_iterator, num_batches, device)\u001b[0m\n\u001b[0;32m   5579\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[0;32m   5580\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 5581\u001b[0m         batch_samples\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   5582\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   5583\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\accelerate\\data_loader.py:579\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    577\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m send_to_device(current_batch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, non_blocking\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_non_blocking)\n\u001b[0;32m    578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_state_dict()\n\u001b[1;32m--> 579\u001b[0m next_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mskip_batches:\n\u001b[0;32m    581\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m current_batch\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    734\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    735\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    736\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    737\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    739\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:789\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    787\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    788\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    790\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    791\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\data\\data_collator.py:757\u001b[0m, in \u001b[0;36mDataCollatorForSeq2Seq.__call__\u001b[1;34m(self, features, return_tensors)\u001b[0m\n\u001b[0;32m    751\u001b[0m \u001b[38;5;66;03m# prepare decoder_input_ids\u001b[39;00m\n\u001b[0;32m    752\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    753\u001b[0m     labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    754\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    755\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprepare_decoder_input_ids_from_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    756\u001b[0m ):\n\u001b[1;32m--> 757\u001b[0m     decoder_input_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_decoder_input_ids_from_labels\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    758\u001b[0m     batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecoder_input_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m decoder_input_ids\n\u001b[0;32m    760\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\models\\mt5\\modeling_mt5.py:1843\u001b[0m, in \u001b[0;36mMT5ForConditionalGeneration.prepare_decoder_input_ids_from_labels\u001b[1;34m(self, labels)\u001b[0m\n\u001b[0;32m   1842\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mprepare_decoder_input_ids_from_labels\u001b[39m(\u001b[38;5;28mself\u001b[39m, labels: torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[1;32m-> 1843\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_shift_right\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\models\\mt5\\modeling_mt5.py:858\u001b[0m, in \u001b[0;36mMT5PreTrainedModel._shift_right\u001b[1;34m(self, input_ids)\u001b[0m\n\u001b[0;32m    856\u001b[0m     shifted_input_ids \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([shifted_input_ids, input_ids[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    857\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 858\u001b[0m     shifted_input_ids \u001b[38;5;241m=\u001b[39m \u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_zeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    859\u001b[0m     shifted_input_ids[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m=\u001b[39m input_ids[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m    860\u001b[0m     shifted_input_ids[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m decoder_start_token_id\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    MT5ForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = \"google/mt5-base\"\n",
    "NEW_MODEL_OUTPUT_DIR = \"mt5-base-cnn-summarizer-en-hi_v8\"\n",
    "NEW_DATA_PATH = \"../Dataset/new_large_CNN_dataset.csv\"\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "WEIGHT_DECAY = 0.25\n",
    "NUM_BEAMS_EVAL = 6\n",
    "MAX_SUMMARY_LENGTH_EVAL = 256\n",
    "METRIC_FOR_BEST_MODEL = \"bertscore_f1\"\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"scratch_training_log_v8_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "    handlers=[logging.FileHandler(log_filename), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "\n",
    "def find_and_save_best_model(output_dir, metric_name):\n",
    "    \"\"\"Finds the best checkpoint from trainer_state.json and saves it.\"\"\"\n",
    "    try:\n",
    "        state_path = os.path.join(output_dir, \"trainer_state.json\")\n",
    "        with open(state_path, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "        \n",
    "        best_metric_value = None\n",
    "        best_checkpoint_path = None\n",
    "        metric_to_check = f\"eval_{metric_name}\"\n",
    "        is_loss = 'loss' in metric_to_check\n",
    "\n",
    "        for log in state[\"log_history\"]:\n",
    "            if metric_to_check in log:\n",
    "                metric_value = log[metric_to_check]\n",
    "                if best_metric_value is None or \\\n",
    "                   (is_loss and metric_value < best_metric_value) or \\\n",
    "                   (not is_loss and metric_value > best_metric_value):\n",
    "                    best_metric_value = metric_value\n",
    "                    step = log.get('step')\n",
    "                    if step:\n",
    "                        potential_path = os.path.join(output_dir, f\"checkpoint-{step}\")\n",
    "                        if os.path.exists(potential_path):\n",
    "                            best_checkpoint_path = potential_path\n",
    "\n",
    "        if not best_checkpoint_path:\n",
    "            logging.error(\"Could not find the best checkpoint from the logs.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"Best checkpoint found: {best_checkpoint_path} with {metric_to_check}: {best_metric_value}\")\n",
    "\n",
    "        final_model_path = os.path.join(output_dir, \"final_model\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            shutil.rmtree(final_model_path)\n",
    "            \n",
    "        shutil.copytree(best_checkpoint_path, final_model_path)\n",
    "        logging.info(f\"Best model copied to {final_model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not save the best model due to: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        tokenizer = T5Tokenizer.from_pretrained(BASE_MODEL_PATH, legacy=False)\n",
    "        model = MT5ForConditionalGeneration.from_pretrained(BASE_MODEL_PATH)\n",
    "\n",
    "        # Load the user's dataset\n",
    "        df_new = pd.read_csv(NEW_DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        df_new.dropna(subset=['raw_news_article', 'english_summary', 'hindi_summary'], inplace=True)\n",
    "        \n",
    "        # --- Aggressive Data Cleaning Step ---\n",
    "        logging.info(\"--- Starting Aggressive Data Cleaning to Remove Leaks ---\")\n",
    "        cleaned_articles = []\n",
    "        for i, row in df_new.iterrows():\n",
    "            article = str(row['raw_news_article'])\n",
    "            eng_summary = str(row['english_summary'])\n",
    "            hin_summary = str(row['hindi_summary'])\n",
    "            \n",
    "            # Surgically remove summary text from the article text\n",
    "            article = article.replace(eng_summary, \"\")\n",
    "            article = article.replace(hin_summary, \"\")\n",
    "            cleaned_articles.append(article)\n",
    "        \n",
    "        df_new['raw_news_article'] = cleaned_articles\n",
    "        logging.info(\"--- Aggressive Data Cleaning Finished ---\")\n",
    "\n",
    "        # --- NEW: Ground Truth Diagnostic ---\n",
    "        logging.info(\"\\n\\n==================== GROUND TRUTH DIAGNOSTIC ====================\")\n",
    "        first_row = df_new.iloc[0]\n",
    "        logging.info(f\"--- Cleaned Article (First Row) ---\\n{first_row['raw_news_article']}\\n\")\n",
    "        logging.info(f\"--- English Summary (First Row) ---\\n{first_row['english_summary']}\\n\")\n",
    "        logging.info(f\"--- Hindi Summary (First Row) ---\\n{first_row['hindi_summary']}\\n\")\n",
    "        logging.info(\"=================================================================\\n\\n\")\n",
    "\n",
    "\n",
    "        df_new.reset_index(drop=True, inplace=True)\n",
    "        raw_dataset = Dataset.from_pandas(df_new)\n",
    "\n",
    "        PREFIX_ENG = \"summarize English: \"\n",
    "        PREFIX_HIN = \"summarize Hindi: \"\n",
    "\n",
    "        def format_dataset(batch):\n",
    "            inputs, targets = [], []\n",
    "            for article, eng_summary, hin_summary in zip(\n",
    "                batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']\n",
    "            ):\n",
    "                if isinstance(article, str):\n",
    "                    inputs.append(PREFIX_ENG + article)\n",
    "                    targets.append(eng_summary)\n",
    "                    inputs.append(PREFIX_HIN + article)\n",
    "                    targets.append(hin_summary)\n",
    "            return {'inputs': inputs, 'targets': targets}\n",
    "\n",
    "        processed_dataset = raw_dataset.map(\n",
    "            format_dataset, batched=True, remove_columns=raw_dataset.column_names\n",
    "        ).flatten()\n",
    "\n",
    "        train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        final_datasets = DatasetDict({\n",
    "            'train': train_test_split['train'],\n",
    "            'test': train_test_split['test']\n",
    "        })\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            model_inputs = tokenizer(examples['inputs'], max_length=1024, truncation=True)\n",
    "            labels = tokenizer(text_target=examples['targets'], max_length=MAX_SUMMARY_LENGTH_EVAL, truncation=True)\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "\n",
    "        tokenized_datasets = final_datasets.map(tokenize_function, batched=True, remove_columns=['inputs', 'targets'])\n",
    "        \n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "        bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "            bert_result = bertscore_metric.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "            \n",
    "            result = {}\n",
    "            for key, value in rouge_result.items():\n",
    "                result[f\"rouge_{key}\"] = round(value * 100, 4)\n",
    "\n",
    "            result[\"bertscore_f1\"] = round(np.mean(bert_result[\"f1\"]) * 100, 4)\n",
    "\n",
    "            return result\n",
    "\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=NEW_MODEL_OUTPUT_DIR,\n",
    "            num_train_epochs=NUM_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            logging_dir=f\"{NEW_MODEL_OUTPUT_DIR}/logs\",\n",
    "            logging_steps=50,\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=NUM_EPOCHS,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            load_best_model_at_end=False,\n",
    "            generation_max_length=MAX_SUMMARY_LENGTH_EVAL,\n",
    "            generation_num_beams=NUM_BEAMS_EVAL,\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        logging.info(\"Starting training from scratch...\")\n",
    "        trainer.train()\n",
    "        logging.info(\"Training finished successfully.\")\n",
    "        \n",
    "        logging.info(\"Finding and saving the best model...\")\n",
    "        find_and_save_best_model(NEW_MODEL_OUTPUT_DIR, METRIC_FOR_BEST_MODEL)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during the main process: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3b0c8aa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 17:43:10,027 [INFO] - --- Starting Text Normalization (English Only) ---\n",
      "2025-10-06 17:43:10,871 [INFO] - --- Text Normalization Finished ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "132eba849e944e58a6b52800ebf7aab4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9237 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e318c88981e43f9b820c9e4cd87157f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8313 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e29cf1af8c30415db4b12f5a854177e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/924 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_21372\\334502138.py:188: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "2025-10-06 17:43:58,487 [INFO] - Starting training from scratch (English Only)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1851' max='5200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1851/5200 27:10 < 49:12, 1.13 it/s, Epoch 1.78/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 209\u001b[0m\n\u001b[0;32m    206\u001b[0m         logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn unexpected error occurred during the main process: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 209\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[11], line 199\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    188\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m    189\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    190\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    195\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m    196\u001b[0m )\n\u001b[0;32m    198\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training from scratch (English Only)...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 199\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    200\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining finished successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    202\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinding and saving the best model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:2328\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2326\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:2672\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2665\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2666\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2667\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2668\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2669\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2670\u001b[0m )\n\u001b[0;32m   2671\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2672\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2675\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2676\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2677\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2678\u001b[0m ):\n\u001b[0;32m   2679\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2680\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:4060\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   4057\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   4058\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 4060\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   4062\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\accelerate\\accelerator.py:2730\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2728\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2729\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2730\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "import unicodedata\n",
    "from datetime import datetime\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    MT5ForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = \"google/mt5-base\"\n",
    "NEW_MODEL_OUTPUT_DIR = \"mt5-base-cnn-summarizer-en-hi_v8\"\n",
    "NEW_DATA_PATH = \"../Dataset/new_large_CNN_dataset.csv\"\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "WEIGHT_DECAY = 0.25\n",
    "NUM_BEAMS_EVAL = 6\n",
    "MAX_SUMMARY_LENGTH_EVAL = 256\n",
    "METRIC_FOR_BEST_MODEL = \"bertscore_f1\"\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"scratch_training_log_v8_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "    handlers=[logging.FileHandler(log_filename), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "\n",
    "def find_and_save_best_model(output_dir, metric_name):\n",
    "    \"\"\"Finds the best checkpoint from trainer_state.json and saves it.\"\"\"\n",
    "    try:\n",
    "        state_path = os.path.join(output_dir, \"trainer_state.json\")\n",
    "        with open(state_path, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "        \n",
    "        best_metric_value = None\n",
    "        best_checkpoint_path = None\n",
    "        metric_to_check = f\"eval_{metric_name}\"\n",
    "        is_loss = 'loss' in metric_to_check\n",
    "\n",
    "        for log in state[\"log_history\"]:\n",
    "            if metric_to_check in log:\n",
    "                metric_value = log[metric_to_check]\n",
    "                if best_metric_value is None or \\\n",
    "                   (is_loss and metric_value < best_metric_value) or \\\n",
    "                   (not is_loss and metric_value > best_metric_value):\n",
    "                    best_metric_value = metric_value\n",
    "                    step = log.get('step')\n",
    "                    if step:\n",
    "                        potential_path = os.path.join(output_dir, f\"checkpoint-{step}\")\n",
    "                        if os.path.exists(potential_path):\n",
    "                            best_checkpoint_path = potential_path\n",
    "\n",
    "        if not best_checkpoint_path:\n",
    "            logging.error(\"Could not find the best checkpoint from the logs.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"Best checkpoint found: {best_checkpoint_path} with {metric_to_check}: {best_metric_value}\")\n",
    "\n",
    "        final_model_path = os.path.join(output_dir, \"final_model\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            shutil.rmtree(final_model_path)\n",
    "            \n",
    "        shutil.copytree(best_checkpoint_path, final_model_path)\n",
    "        logging.info(f\"Best model copied to {final_model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not save the best model due to: {e}\", exc_info=True)\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and normalizes text to remove inconsistencies and hidden characters.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        tokenizer = T5Tokenizer.from_pretrained(BASE_MODEL_PATH, legacy=False)\n",
    "        model = MT5ForConditionalGeneration.from_pretrained(BASE_MODEL_PATH)\n",
    "\n",
    "        df_new = pd.read_csv(NEW_DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        # --- MODIFIED: Use only English columns for this test ---\n",
    "        df_new.dropna(subset=['raw_news_article', 'english_summary'], inplace=True)\n",
    "        \n",
    "        logging.info(\"--- Starting Text Normalization (English Only) ---\")\n",
    "        df_new['raw_news_article'] = df_new['raw_news_article'].apply(normalize_text)\n",
    "        df_new['english_summary'] = df_new['english_summary'].apply(normalize_text)\n",
    "        logging.info(\"--- Text Normalization Finished ---\")\n",
    "\n",
    "        # --- MODIFIED: Create dataset with only English columns ---\n",
    "        df_eng_only = df_new[['raw_news_article', 'english_summary']].copy()\n",
    "        df_eng_only.reset_index(drop=True, inplace=True)\n",
    "        raw_dataset = Dataset.from_pandas(df_eng_only)\n",
    "\n",
    "        PREFIX_ENG = \"summarize English: \"\n",
    "\n",
    "        # --- MODIFIED: Format dataset for English only ---\n",
    "        def format_dataset_eng_only(batch):\n",
    "            inputs, targets = [], []\n",
    "            for article, eng_summary in zip(\n",
    "                batch['raw_news_article'], batch['english_summary']\n",
    "            ):\n",
    "                if isinstance(article, str) and article:\n",
    "                    inputs.append(PREFIX_ENG + article)\n",
    "                    targets.append(eng_summary)\n",
    "            return {'inputs': inputs, 'targets': targets}\n",
    "\n",
    "        processed_dataset = raw_dataset.map(\n",
    "            format_dataset_eng_only, batched=True, remove_columns=raw_dataset.column_names\n",
    "        )\n",
    "\n",
    "        train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        final_datasets = DatasetDict({\n",
    "            'train': train_test_split['train'],\n",
    "            'test': train_test_split['test']\n",
    "        })\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            model_inputs = tokenizer(examples['inputs'], max_length=1024, truncation=True)\n",
    "            labels = tokenizer(text_target=examples['targets'], max_length=MAX_SUMMARY_LENGTH_EVAL, truncation=True)\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "\n",
    "        tokenized_datasets = final_datasets.map(tokenize_function, batched=True, remove_columns=['inputs', 'targets'])\n",
    "        \n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "        bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "            bert_result = bertscore_metric.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "            \n",
    "            result = {}\n",
    "            for key, value in rouge_result.items():\n",
    "                result[f\"rouge_{key}\"] = round(value * 100, 4)\n",
    "\n",
    "            result[\"bertscore_f1\"] = round(np.mean(bert_result[\"f1\"]) * 100, 4)\n",
    "\n",
    "            return result\n",
    "\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=NEW_MODEL_OUTPUT_DIR,\n",
    "            num_train_epochs=NUM_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            logging_dir=f\"{NEW_MODEL_OUTPUT_DIR}/logs\",\n",
    "            logging_steps=50,\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=NUM_EPOCHS,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            load_best_model_at_end=False,\n",
    "            generation_max_length=MAX_SUMMARY_LENGTH_EVAL,\n",
    "            generation_num_beams=NUM_BEAMS_EVAL,\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        logging.info(\"Starting training from scratch (English Only)...\")\n",
    "        trainer.train()\n",
    "        logging.info(\"Training finished successfully.\")\n",
    "        \n",
    "        logging.info(\"Finding and saving the best model...\")\n",
    "        find_and_save_best_model(NEW_MODEL_OUTPUT_DIR, METRIC_FOR_BEST_MODEL)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during the main process: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc83384f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 18:11:20,519 [INFO] - --- STARTING SINGLE EXAMPLE OVERFIT TEST ---\n",
      "2025-10-06 18:11:28,896 [INFO] - --- Starting Text Normalization ---\n",
      "2025-10-06 18:11:29,753 [INFO] - --- Text Normalization Finished ---\n",
      "2025-10-06 18:11:29,753 [INFO] - --- Using single example for overfitting test: ---\n",
      "2025-10-06 18:11:29,753 [INFO] - ARTICLE: Voters are still 'in the dark' about the scale and depth of spending cuts being planned by all the main parties with just two weeks until polling day, economic experts warned today. Analysts from the Institute for Fiscal Studies said none of the major parties had given 'anything like full details' on how they will tackle the nations' debts after the election. The Tories were accused of giving 'no detail' about their deficit reduction plan, which relies on £30billion of cuts, while Labour has lef...\n",
      "2025-10-06 18:11:29,753 [INFO] - SUMMARY: Economic experts from the Institute for Fiscal Studies (IFS) warned that voters remain largely uninformed about the scale of spending cuts planned by major UK political parties just two weeks before polling day. The IFS report, based on detailed manifesto studies, criticized the Conservatives for lacking specifics on their £30 billion deficit reduction plan, which relies heavily on unspecified spending cuts and tax increases, while Labour has indicated a willingness to borrow an additional £26 billion annually. Although Chancellor George Osborne recently announced that public sector borrowing for the last financial year undershot its target by nearly £3 billion, the national debt remains over £1.48 trillion, representing 80.4% of GDP. The IFS highlighted that Tory plans involve the largest borrowing reduction but require significant, mostly unspecified, cuts, including £10 billion from social security. Labour's fiscal ambition was deemed vaguer, potentially leaving annual borrowing at £26 billion. The Liberal Democrats showed more transparency in their fiscal plans than Labour, while the SNP's figures implied a slower, longer period of austerity. The article also covered the political contention, with Labour accusing Tories of extreme spending cuts and Conservatives warning against an \"SNP/Miliband nightmare\" and potential constitutional crisis if the SNP held the balance of power, citing SNP leader Nicola Sturgeon's demands for increased spending and an end to austerity as a condition for supporting Labour, which Osborne claimed would add £6 billion to Britain's interest bill.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2f56f070b494f548a30caf37a80b439",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64526acd60b04a0c9d523f047083f38c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "031b75bf230a404ebb508dea88c22b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_21372\\3127258116.py:189: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "2025-10-06 18:11:36,911 [INFO] - Starting training (overfit test on single example)...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 06:06, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 18:17:44,376 [INFO] - Overfit test finished.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "import unicodedata\n",
    "from datetime import datetime\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    MT5ForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = \"google/mt5-base\"\n",
    "NEW_MODEL_OUTPUT_DIR = \"mt5-base-cnn-summarizer-en-hi_v8\"\n",
    "NEW_DATA_PATH = \"../Dataset/new_large_CNN_dataset.csv\"\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 1 # Use batch size of 1 for single example test\n",
    "GRADIENT_ACCUMULATION_STEPS = 1\n",
    "WEIGHT_DECAY = 0.25\n",
    "NUM_BEAMS_EVAL = 6\n",
    "MAX_SUMMARY_LENGTH_EVAL = 256\n",
    "METRIC_FOR_BEST_MODEL = \"bertscore_f1\"\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"scratch_training_log_v8_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "    handlers=[logging.FileHandler(log_filename), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "\n",
    "def find_and_save_best_model(output_dir, metric_name):\n",
    "    \"\"\"Finds the best checkpoint from trainer_state.json and saves it.\"\"\"\n",
    "    try:\n",
    "        state_path = os.path.join(output_dir, \"trainer_state.json\")\n",
    "        with open(state_path, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "        \n",
    "        best_metric_value = None\n",
    "        best_checkpoint_path = None\n",
    "        metric_to_check = f\"eval_{metric_name}\"\n",
    "        is_loss = 'loss' in metric_to_check\n",
    "\n",
    "        for log in state[\"log_history\"]:\n",
    "            if metric_to_check in log:\n",
    "                metric_value = log[metric_to_check]\n",
    "                if best_metric_value is None or \\\n",
    "                   (is_loss and metric_value < best_metric_value) or \\\n",
    "                   (not is_loss and metric_value > best_metric_value):\n",
    "                    best_metric_value = metric_value\n",
    "                    step = log.get('step')\n",
    "                    if step:\n",
    "                        potential_path = os.path.join(output_dir, f\"checkpoint-{step}\")\n",
    "                        if os.path.exists(potential_path):\n",
    "                            best_checkpoint_path = potential_path\n",
    "\n",
    "        if not best_checkpoint_path:\n",
    "            logging.error(\"Could not find the best checkpoint from the logs.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"Best checkpoint found: {best_checkpoint_path} with {metric_to_check}: {best_metric_value}\")\n",
    "\n",
    "        final_model_path = os.path.join(output_dir, \"final_model\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            shutil.rmtree(final_model_path)\n",
    "            \n",
    "        shutil.copytree(best_checkpoint_path, final_model_path)\n",
    "        logging.info(f\"Best model copied to {final_model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not save the best model due to: {e}\", exc_info=True)\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and normalizes text to remove inconsistencies and hidden characters.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        logging.info(\"--- STARTING SINGLE EXAMPLE OVERFIT TEST ---\")\n",
    "        tokenizer = T5Tokenizer.from_pretrained(BASE_MODEL_PATH, legacy=False)\n",
    "        model = MT5ForConditionalGeneration.from_pretrained(BASE_MODEL_PATH)\n",
    "\n",
    "        df_new = pd.read_csv(NEW_DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        df_new.dropna(subset=['raw_news_article', 'english_summary'], inplace=True)\n",
    "        \n",
    "        logging.info(\"--- Starting Text Normalization ---\")\n",
    "        df_new['raw_news_article'] = df_new['raw_news_article'].apply(normalize_text)\n",
    "        df_new['english_summary'] = df_new['english_summary'].apply(normalize_text)\n",
    "        logging.info(\"--- Text Normalization Finished ---\")\n",
    "\n",
    "        # --- MODIFIED: Select only the FIRST row for the test ---\n",
    "        df_single_example = df_new[['raw_news_article', 'english_summary']].head(1).copy()\n",
    "        logging.info(\"--- Using single example for overfitting test: ---\")\n",
    "        logging.info(f\"ARTICLE: {df_single_example.iloc[0]['raw_news_article'][:500]}...\")\n",
    "        logging.info(f\"SUMMARY: {df_single_example.iloc[0]['english_summary']}\")\n",
    "\n",
    "        \n",
    "        raw_dataset = Dataset.from_pandas(df_single_example)\n",
    "\n",
    "        PREFIX_ENG = \"summarize English: \"\n",
    "\n",
    "        def format_dataset_eng_only(batch):\n",
    "            inputs, targets = [], []\n",
    "            for article, eng_summary in zip(\n",
    "                batch['raw_news_article'], batch['english_summary']\n",
    "            ):\n",
    "                if isinstance(article, str) and article:\n",
    "                    inputs.append(PREFIX_ENG + article)\n",
    "                    targets.append(eng_summary)\n",
    "            return {'inputs': inputs, 'targets': targets}\n",
    "\n",
    "        processed_dataset = raw_dataset.map(\n",
    "            format_dataset_eng_only, batched=True, remove_columns=raw_dataset.column_names\n",
    "        )\n",
    "\n",
    "        # --- MODIFIED: No train/test split needed for a single example ---\n",
    "        final_datasets = DatasetDict({\n",
    "            'train': processed_dataset,\n",
    "            'test': processed_dataset # Use the same example for evaluation\n",
    "        })\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            model_inputs = tokenizer(examples['inputs'], max_length=1024, truncation=True)\n",
    "            labels = tokenizer(text_target=examples['targets'], max_length=MAX_SUMMARY_LENGTH_EVAL, truncation=True)\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "\n",
    "        tokenized_datasets = final_datasets.map(tokenize_function, batched=True, remove_columns=['inputs', 'targets'])\n",
    "        \n",
    "        # Metrics are not critical for this test, but we keep them for consistency\n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "        bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            # ... (compute_metrics function remains the same)\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "            bert_result = bertscore_metric.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "            \n",
    "            result = {}\n",
    "            for key, value in rouge_result.items():\n",
    "                result[f\"rouge_{key}\"] = round(value * 100, 4)\n",
    "\n",
    "            result[\"bertscore_f1\"] = round(np.mean(bert_result[\"f1\"]) * 100, 4)\n",
    "            return result\n",
    "\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=NEW_MODEL_OUTPUT_DIR,\n",
    "            num_train_epochs=10, # More epochs to ensure overfitting\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            logging_dir=f\"{NEW_MODEL_OUTPUT_DIR}/logs\",\n",
    "            logging_steps=1, # Log every step\n",
    "            save_strategy=\"epoch\",\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            load_best_model_at_end=False\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        logging.info(\"Starting training (overfit test on single example)...\")\n",
    "        trainer.train()\n",
    "        logging.info(\"Overfit test finished.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during the main process: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "12c31929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 18:17:56,244 [INFO] - --- FINAL DIAGNOSTIC: INSPECTING TOKEN TENSORS ---\n",
      "2025-10-06 18:18:00,226 [INFO] - --- Starting Text Normalization ---\n",
      "2025-10-06 18:18:01,092 [INFO] - --- Text Normalization Finished ---\n",
      "2025-10-06 18:18:01,109 [INFO] - --- Using single example for token inspection: ---\n",
      "2025-10-06 18:18:01,110 [INFO] - ARTICLE: Voters are still 'in the dark' about the scale and depth of spending cuts being planned by all the main parties with just two weeks until polling day, economic experts warned today. Analysts from the Institute for Fiscal Studies said none of the major parties had given 'anything like full details' on how they will tackle the nations' debts after the election. The Tories were accused of giving 'no detail' about their deficit reduction plan, which relies on £30billion of cuts, while Labour has lef...\n",
      "2025-10-06 18:18:01,111 [INFO] - SUMMARY: Economic experts from the Institute for Fiscal Studies (IFS) warned that voters remain largely uninformed about the scale of spending cuts planned by major UK political parties just two weeks before polling day. The IFS report, based on detailed manifesto studies, criticized the Conservatives for lacking specifics on their £30 billion deficit reduction plan, which relies heavily on unspecified spending cuts and tax increases, while Labour has indicated a willingness to borrow an additional £26 billion annually. Although Chancellor George Osborne recently announced that public sector borrowing for the last financial year undershot its target by nearly £3 billion, the national debt remains over £1.48 trillion, representing 80.4% of GDP. The IFS highlighted that Tory plans involve the largest borrowing reduction but require significant, mostly unspecified, cuts, including £10 billion from social security. Labour's fiscal ambition was deemed vaguer, potentially leaving annual borrowing at £26 billion. The Liberal Democrats showed more transparency in their fiscal plans than Labour, while the SNP's figures implied a slower, longer period of austerity. The article also covered the political contention, with Labour accusing Tories of extreme spending cuts and Conservatives warning against an \"SNP/Miliband nightmare\" and potential constitutional crisis if the SNP held the balance of power, citing SNP leader Nicola Sturgeon's demands for increased spending and an end to austerity as a condition for supporting Labour, which Osborne claimed would add £6 billion to Britain's interest bill.\n",
      "2025-10-06 18:18:01,113 [INFO] - \n",
      "==================== TOKEN INSPECTION ====================\n",
      "2025-10-06 18:18:01,146 [INFO] - \n",
      "--- DECODED INPUT TOKENS ---\n",
      "summarize English: Voters are still 'in the dark' about the scale and depth of spending cuts being planned by all the main parties with just two weeks until polling day, economic experts warned today. Analysts from the Institute for Fiscal Studies said none of the major parties had given 'anything like full details' on how they will tackle the nations' debts after the election. The Tories were accused of giving 'no detail' about their deficit reduction plan, which relies on £30billion of cuts, while Labour has left the door open to borrowing an extra £26billion-a-year. Scroll down for video . The Institute for Fiscal Studies analysed the policies of all the main parties to see how their policies would increase borrowing . The IFS warned that the promise of tackling the deficit in the next Parliament is based on 'almost entirely unspecified spending cuts and tax increases'. The think-tank reached its conclusions after a detailed study of the party manifestos ahead of May's General Election. It accused Tory Chancellor George Osborne and his Labour opponent Ed Balls of failing to spell out exactly where the axe will fall. IFS deputy director Carl Emmerson said: 'There are genuinely big differences between the main parties' fiscal plans. 'The electorate has a real choice, although it can at best see only the broad outlines of that choice. 'Conservative plans involve a significantly larger reduction in borrowing and debt than Labour plans. 'But they are predicated on substantial and almost entirely unspecified spending cuts and tax increases. 'While Labour has been considerably less clear about its overall fiscal ambition, its stated position appears to be consistent with little in the way of further spending cuts after this year.' Chancellor George Osborne and his Labour opponent Ed Balls are accused of not spelling out how they will tackle the deficit . George Osborne received a pre-election boost today as official figures showed he beat his target for reducing annual public sector borrowing for the latest financial year by nearly £3 billion. Borrowing - excluding the effect of bank bailouts - was £87.3 billion for the year to the end of March, down from £98.5 billion in 2013/14, according to the Office for National Statistics (ONS). The result undershot the latest target of £90.2 billion set by the independent Office for Budget Responsibility (OBR) at the time of last month's Budget. It means that annual borrowing (GDP) has fallen by more than £60 billion from £153.5 billion in 2009/10 just before the Coalition came to power. As a percentage of gross domestic product (GDP) it has dropped by half from 10.2 per cent to 4.8 per cent. However, underlying debt of £1.48 trillion is more than £500 billion higher than the 2009/10 figure of £956 billion. The nation's debt represents 80.4 per cent of GDP, up from 62 per cent five years ago. The IFS analysis said the Tories planned the largest reduction in borrowing over the course of the next Parliament. It said the party would require large spending cuts or tax increases to achieve this. Research economist Soumaya Keynes said: 'The Conservatives have said they want to eliminate the deficit but provided next to no detail on how they would do it. 'They should be forthcoming on the £5 billion of largely unspecified clampdown on tax avoidance, the £10 billion of unspecified cuts to social security spending and, according to our calculations, further real cuts to unprotected departments of around £30 billion.' Turning to Labour, the IFS said the Opposition had been 'considerably more vague' about how much it wants to borrow. The pledge to produce a surplus but without specifying by when or how much could be consistent with a reduction in borrowing totalling 3.6 per cent of national income. Senior research economist Rowena Crawford said: 'Labour's proposed measures might be broad\n",
      "2025-10-06 18:18:01,149 [INFO] - \n",
      "--- DECODED LABEL TOKENS ---\n",
      "Economic experts from the Institute for Fiscal Studies (IFS) warned that voters remain largely uninformed about the scale of spending cuts planned by major UK political parties just two weeks before polling day. The IFS report, based on detailed manifesto studies, criticized the Conservatives for lacking specifics on their £30 billion deficit reduction plan, which relies heavily on unspecified spending cuts and tax increases, while Labour has indicated a willingness to borrow an additional £26 billion annually. Although Chancellor George Osborne recently announced that public sector borrowing for the last financial year undershot its target by nearly £3 billion, the national debt remains over £1.48 trillion, representing 80.4% of GDP. The IFS highlighted that Tory plans involve the largest borrowing reduction but require significant, mostly unspecified, cuts, including £10 billion from social security. Labour's fiscal ambition was deemed vaguer, potentially leaving annual borrowing at £26 billion.\n",
      "2025-10-06 18:18:01,149 [INFO] - \n",
      "--- RAW INPUT IDS ---\n",
      "[196098, 10701, 5413, 267, 54644, 1207, 418, 5387, 259, 277, 348, 287, 21557, 277, 1388, 287, 39009, 305, 259, 44477, 304, 259, 263, 32532, 259, 105933, 259, 5330, 2127, 12957, 455, 751, 287, 4397, 259, 48051, 514, 1627, 2956, 259, 29426, 259, 13283, 259, 58601, 3117, 261, 12936, 259, 45345, 119279, 345, 7883, 260, 259, 100778, 263, 702, 287, 16927, 332, 259, 63845, 43445, 2426, 259, 3018, 304, 287, 9677, 259, 48051, 1425, 259, 12981, 259, 277, 20882, 1469, 3622, 15637, 277, 351, 2606, 287, 276, 898, 51478, 468, 287, 259, 113441, 277, 269, 76227, 3354, 287, 57046, 260, 486, 926, 6033, 2109, 37979, 345, 304, 259, 31745, 259, 277, 505, 17932, 277, 1388, 259, 1616, 259, 104776, 259, 48320, 2127, 261, 259, 1542, 23324, 299, 351, 5883, 1249, 316, 86704, 304, 259, 105933, 261, 259, 4944, 501, 38894, 1070, 12255, 287, 2328, 5169, 288, 5617, 134817, 461, 4610, 5883, 2427, 316, 86704, 264, 262, 264, 12388, 260, 259, 33967, 5123, 332, 1552, 259, 260, 486, 16927, 332, 259, 63845, 43445, 48358, 285, 287, 259, 62391, 304, 751, 287, 4397, 259, 48051, 288, 2354, 2606, 259, 1616, 259, 62391, 259, 2220, 259, 15397, 5617, 134817, 259, 260, 486, 259, 221309, 119279, 345, 533, 287, 259, 72714, 304, 576, 178962, 287, 259, 104776, 281, 287, 6844, 259, 101234, 339, 259, 5621, 351, 259, 277, 262, 28746, 259, 21202, 484, 335, 212210, 285, 259, 263, 32532, 259, 105933, 305, 11577, 259, 15397, 263, 277, 260, 486, 5231, 264, 90175, 259, 11125, 345, 2476, 259, 49666, 263, 3354, 259, 262, 259, 17244, 10380, 304, 287, 13899, 16062, 337, 259, 48813, 304, 1797, 277, 263, 4724, 259, 74257, 260, 1385, 37979, 345, 6858, 276, 43010, 93135, 10934, 3677, 63886, 305, 1638, 501, 38894, 585, 111763, 6308, 9592, 263, 304, 28807, 347, 288, 97970, 1350, 12431, 484, 259, 3001, 287, 259, 51363, 898, 8271, 260, 259, 221309, 51286, 276, 10753, 14644, 3639, 191947, 2426, 267, 259, 277, 65237, 418, 259, 18782, 484, 5133, 25297, 263, 259, 4964, 287, 4397, 259, 48051, 277, 17182, 21494, 260, 259, 277, 2009, 362, 51622, 1614, 1070, 259, 262, 2784, 18201, 261, 259, 262, 47906, 609, 738, 344, 1920, 2354, 2469, 287, 61955, 259, 68431, 263, 304, 533, 18201, 260, 259, 277, 424, 174544, 21494, 259, 146404, 259, 262, 259, 20364, 484, 259, 56652, 259, 48320, 281, 5617, 134817, 305, 259, 64644, 2421, 501, 38894, 21494, 260, 259, 277, 64210, 287, 276, 418, 786, 38876, 351, 52863, 473, 305, 259, 262, 28746, 259, 21202, 484, 335, 212210, 285, 259, 263, 32532, 259, 105933, 305, 11577, 259, 15397, 263, 260, 259, 277, 22841, 501, 38894, 1070, 2101, 5071, 23751, 24691, 14007, 1388, 2476, 259, 38329, 17182, 259, 98457, 261, 2476, 6509, 285, 13446, 259, 15484, 263, 288, 390, 259, 45436, 514, 7084, 281, 287, 3230, 304, 259, 17756, 259, 263, 32532, 259, 105933, 3354, 714, 3721, 260, 277, 43010, 93135, 10934, 3677, 63886, 305, 1638, 501, 38894, 585, 111763, 6308, 9592, 263, 418, 37979, 345, 304, 776, 259, 131640, 1350, 2606, 287, 276, 898, 51478, 468, 287, 259, 104776, 259, 260, 10934, 3677, 63886, 11243, 285, 259, 262, 786, 264, 265, 139825, 54627, 7883, 527, 21421, 11212, 263, 3153, 345, 790, 29821, 1638, 13296, 332, 22944, 347, 259, 35548, 2821, 9844, 5617, 134817, 332, 287, 259, 10324, 259, 18703, 3721, 455, 9137, 484, 147763, 43887, 260, 9657, 134817, 259, 264, 69073, 10646, 287, 11232, 304, 4896, 62737, 83342, 259, 264, 639, 5883, 449, 112093, 43887, 332, 287, 3721, 288, 287, 3162, 304, 3618, 261, 5123, 702, 5883, 61503, 428, 43887, 281, 815, 42606, 261, 259, 18775, 288, 287, 7341, 332, 5139, 259, 74584, 274, 72337, 483, 486, 8106, 1711, 45972, 287, 259, 10324, 13296, 304, 5883, 80152, 338, 43887, 2718, 455, 287, 22285, 7341, 332, 49861, 259, 228461, 276, 274, 646, 13970, 271, 344, 287, 1459, 304, 3167, 11400, 277, 263, 49861, 260, 1385, 259, 12909, 533, 259, 35548, 5617, 134817, 274, 61667, 271, 1070, 259, 45395, 455, 1097, 2421, 5883, 2886, 43887, 702, 5883, 1343, 25127, 43887, 281, 259, 215575, 1627, 5038, 287, 371, 100180, 15740, 288, 6665, 260, 1477, 259, 262, 259, 33733, 304, 33720, 54042, 5689, 274, 61667, 271, 609, 1070, 331, 102270, 455, 15068, 702, 115070, 393, 22233, 288, 80278, 393, 22233, 260, 259, 14833, 261, 1711, 81022, 259, 64644, 304, 5883, 168485, 534, 86704, 339, 1097, 2421, 5883, 4257, 43887, 259, 10954, 2421, 287, 259, 215575, 11212, 304, 5883, 52151, 43887, 260, 486, 30341, 277, 263, 259, 64644, 15427, 263, 630, 54536, 393, 22233, 304, 259, 61667, 261, 1150, 702, 8950, 393, 22233, 19425, 3127, 2780, 260, 486, 259, 221309, 17892, 2426, 287, 926, 6033, 2127, 12957, 287, 259, 42983, 259, 48320, 281, 5617, 134817, 910, 287, 9095, 304, 287, 6844, 259, 101234, 260, 1385, 2426, 287, 13899, 259, 2220, 27906, 8057, 259, 263, 32532, 259, 105933, 631, 11577, 259, 15397, 263, 288, 38071, 714, 260, 11980, 259, 141712, 16856, 11557, 10001, 1865, 2426, 267, 259, 277, 2009, 371, 174544, 263, 783, 2426, 287, 276, 3007, 288, 73110, 265, 287, 259, 104776, 1156, 259, 15644, 6844, 288, 375, 17932, 351, 2606, 287, 276, 259, 2220, 342, 609, 260, 259, 277, 10837, 3609, 390, 259, 107418, 9702, 351, 287, 156259, 43887, 304, 8057, 484, 335, 212210, 285, 317, 47744, 11073, 351, 11577, 259, 35897, 3359, 261, 287, 201987, 43887, 304, 335, 212210, 285, 259, 105933, 288, 2943, 20317, 259, 263, 32532, 305, 261, 259, 18775, 288, 1406, 259, 131438, 263, 261, 259, 17756, 2784, 259, 105933, 288, 335, 131467, 259, 188923, 304, 5945, 5883, 1249, 43887, 260, 277, 23117, 347, 288, 501, 38894, 261, 287, 259, 221309, 2426, 287, 3120, 2517, 1425, 2101, 259, 277, 160417, 23751, 1097, 712, 7285, 277, 1388, 2606, 2829, 609, 3007, 263, 288, 330, 124669, 260, 486, 421, 106732, 288, 14804, 259, 262, 865, 11372, 1156, 259, 5767, 104488, 14521, 455, 259, 1909, 631, 2606, 2829, 259, 3659, 390, 259, 45436, 514, 259, 262, 259, 48320, 281, 5617, 134817, 2725, 2309, 56492, 393, 22233, 304, 10811, 259, 31978, 260, 24715, 8348, 259, 141712, 43372, 278, 262, 259, 137343, 7508, 2426, 267, 259, 277, 2470, 38894, 277, 263, 5471, 345, 27264, 263, 259, 8622, 390, 61955, 1]\n",
      "2025-10-06 18:18:01,149 [INFO] - \n",
      "--- RAW LABEL IDS ---\n",
      "[38395, 259, 45345, 702, 287, 16927, 332, 259, 63845, 43445, 274, 221309, 271, 119279, 345, 533, 5217, 1207, 259, 17145, 8057, 484, 335, 34990, 345, 1388, 287, 39009, 304, 259, 263, 32532, 259, 105933, 2127, 12957, 455, 9677, 7225, 259, 28735, 259, 48051, 1627, 2956, 259, 29426, 5038, 259, 58601, 3117, 260, 486, 259, 221309, 8988, 261, 259, 5621, 351, 259, 17244, 16062, 268, 31488, 261, 34709, 10627, 287, 371, 174544, 263, 332, 283, 51294, 8552, 263, 351, 259, 1616, 5883, 1249, 43887, 259, 104776, 259, 48320, 2127, 261, 259, 1542, 23324, 299, 39943, 107341, 351, 335, 212210, 285, 259, 263, 32532, 259, 105933, 305, 11577, 259, 15397, 263, 261, 259, 4944, 501, 38894, 1070, 281, 38876, 259, 262, 259, 81527, 5516, 288, 330, 124669, 461, 259, 14896, 5883, 2427, 43887, 259, 35548, 484, 260, 259, 47482, 43010, 93135, 10934, 3677, 63886, 5376, 484, 259, 50496, 533, 2821, 9844, 5617, 134817, 332, 287, 3167, 259, 18703, 3721, 1711, 45972, 2476, 13296, 455, 9137, 484, 147763, 43887, 261, 287, 10811, 259, 64644, 259, 17145, 263, 910, 5883, 168485, 534, 86704, 261, 15427, 347, 2295, 260, 8116, 304, 259, 61667, 260, 486, 259, 221309, 80992, 345, 533, 6858, 276, 21494, 259, 146404, 287, 259, 42983, 5617, 134817, 259, 48320, 1156, 27906, 259, 20364, 261, 2250, 484, 335, 212210, 285, 261, 259, 105933, 261, 259, 7035, 201987, 43887, 702, 2943, 20317, 260, 501, 38894, 277, 263, 17182, 259, 98457, 639, 269, 129872, 712, 52184, 261, 19497, 484, 340, 33619, 259, 35548, 5617, 134817, 344, 5883, 2427, 43887, 260, 1]\n",
      "2025-10-06 18:18:01,149 [INFO] - \n",
      "Number of Input Tokens: 1024\n",
      "2025-10-06 18:18:01,149 [INFO] - Number of Label Tokens: 256\n",
      "2025-10-06 18:18:01,159 [INFO] - \n",
      "--- FINAL VERDICT ---\n",
      "2025-10-06 18:18:01,160 [INFO] - SUCCESS: The input_ids and labels tensors are DIFFERENT.\n",
      "2025-10-06 18:18:01,160 [INFO] - This is the expected behavior. If the loss is still zero, the issue is exceptionally unusual.\n",
      "2025-10-06 18:18:01,161 [INFO] - \n",
      "==========================================================\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import unicodedata\n",
    "from datetime import datetime\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    MT5ForConditionalGeneration\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = \"google/mt5-base\"\n",
    "NEW_DATA_PATH = \"../Dataset/new_large_CNN_dataset.csv\"\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"scratch_training_log_v8_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "    handlers=[logging.FileHandler(log_filename), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and normalizes text to remove inconsistencies and hidden characters.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        logging.info(\"--- FINAL DIAGNOSTIC: INSPECTING TOKEN TENSORS ---\")\n",
    "        tokenizer = T5Tokenizer.from_pretrained(BASE_MODEL_PATH, legacy=False)\n",
    "\n",
    "        df_new = pd.read_csv(NEW_DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        df_new.dropna(subset=['raw_news_article', 'english_summary'], inplace=True)\n",
    "        \n",
    "        logging.info(\"--- Starting Text Normalization ---\")\n",
    "        df_new['raw_news_article'] = df_new['raw_news_article'].apply(normalize_text)\n",
    "        df_new['english_summary'] = df_new['english_summary'].apply(normalize_text)\n",
    "        logging.info(\"--- Text Normalization Finished ---\")\n",
    "\n",
    "        # Select only the FIRST row for the test\n",
    "        df_single_example = df_new[['raw_news_article', 'english_summary']].head(1).copy()\n",
    "        \n",
    "        article_text = df_single_example.iloc[0]['raw_news_article']\n",
    "        summary_text = df_single_example.iloc[0]['english_summary']\n",
    "\n",
    "        logging.info(\"--- Using single example for token inspection: ---\")\n",
    "        logging.info(f\"ARTICLE: {article_text[:500]}...\")\n",
    "        logging.info(f\"SUMMARY: {summary_text}\")\n",
    "        \n",
    "        PREFIX_ENG = \"summarize English: \"\n",
    "\n",
    "        # Tokenize the input and the target separately\n",
    "        input_encoding = tokenizer(PREFIX_ENG + article_text, max_length=1024, truncation=True, return_tensors=\"pt\")\n",
    "        target_encoding = tokenizer(text_target=summary_text, max_length=256, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "        input_ids = input_encoding.input_ids[0]\n",
    "        labels = target_encoding.input_ids[0]\n",
    "\n",
    "        logging.info(\"\\n==================== TOKEN INSPECTION ====================\")\n",
    "        \n",
    "        logging.info(f\"\\n--- DECODED INPUT TOKENS ---\\n{tokenizer.decode(input_ids, skip_special_tokens=True)}\")\n",
    "        logging.info(f\"\\n--- DECODED LABEL TOKENS ---\\n{tokenizer.decode(labels, skip_special_tokens=True)}\")\n",
    "\n",
    "        logging.info(f\"\\n--- RAW INPUT IDS ---\\n{input_ids.tolist()}\")\n",
    "        logging.info(f\"\\n--- RAW LABEL IDS ---\\n{labels.tolist()}\")\n",
    "        \n",
    "        logging.info(f\"\\nNumber of Input Tokens: {len(input_ids)}\")\n",
    "        logging.info(f\"Number of Label Tokens: {len(labels)}\")\n",
    "        \n",
    "        are_tensors_equal = torch.equal(input_ids, labels)\n",
    "        \n",
    "        logging.info(\"\\n--- FINAL VERDICT ---\")\n",
    "        if are_tensors_equal:\n",
    "            logging.error(\"CRITICAL ERROR: The input_ids and labels tensors are IDENTICAL.\")\n",
    "            logging.error(\"This is the cause of the zero loss. The tokenizer is producing the same token sequence for the article and the summary.\")\n",
    "        else:\n",
    "            logging.info(\"SUCCESS: The input_ids and labels tensors are DIFFERENT.\")\n",
    "            logging.info(\"This is the expected behavior. If the loss is still zero, the issue is exceptionally unusual.\")\n",
    "            \n",
    "        logging.info(\"\\n==========================================================\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during the diagnostic script: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "63f3be23",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2e9fe132b1145bab0345967b4738bb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\admin\\.cache\\huggingface\\hub\\models--t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16a560b07a714573a364a36554c04aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fda543fc98414e268dcd4a47f43a1533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3db179a8b0594e0fb6b178d59ca060d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/892M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f943149332449d5a1d04bdc81d94d02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 18:21:01,544 [INFO] - --- Starting Text Normalization ---\n",
      "2025-10-06 18:21:03,028 [INFO] - --- Text Normalization Finished ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b338d61e8b84a2999a3178ffa684267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2ef1e407adc4989a14789b0ed3857a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8906c56087aa480bab7b0c3bc806f9f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_21372\\1384657842.py:190: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "2025-10-06 18:22:27,912 [INFO] - Starting training from scratch...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1084' max='10380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 1084/10380 13:21 < 1:54:45, 1.35 it/s, Epoch 0.52/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.410900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.451600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.207400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.163000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.289800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.066600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>1.161800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.086100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.037400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.054600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>1.097100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.073400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.992400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.067400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.033600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.048400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.986000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.030600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.017700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.043100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.973500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 211\u001b[0m\n\u001b[0;32m    208\u001b[0m         logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn unexpected error occurred during the main process: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 211\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[14], line 201\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    190\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m    191\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    192\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    197\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m    198\u001b[0m )\n\u001b[0;32m    200\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training from scratch...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 201\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    202\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining finished successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    204\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinding and saving the best model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:2328\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2326\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:2672\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2665\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2666\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2667\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2668\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2669\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2670\u001b[0m )\n\u001b[0;32m   2671\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2672\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2674\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2675\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2676\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2677\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2678\u001b[0m ):\n\u001b[0;32m   2679\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2680\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:4060\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   4057\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   4058\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 4060\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   4062\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\accelerate\\accelerator.py:2730\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2728\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2729\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2730\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "import unicodedata\n",
    "from datetime import datetime\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration, # Changed from MT5\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "# --- MODIFIED: Switched to t5-base model ---\n",
    "BASE_MODEL_PATH = \"t5-base\"\n",
    "NEW_MODEL_OUTPUT_DIR = \"t5-base-cnn-summarizer-en-hi_v9\"\n",
    "NEW_DATA_PATH = \"../Dataset/new_large_CNN_dataset.csv\"\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "WEIGHT_DECAY = 0.25\n",
    "NUM_BEAMS_EVAL = 6\n",
    "MAX_SUMMARY_LENGTH_EVAL = 256\n",
    "METRIC_FOR_BEST_MODEL = \"bertscore_f1\"\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"scratch_training_log_v9_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "    handlers=[logging.FileHandler(log_filename), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "\n",
    "def find_and_save_best_model(output_dir, metric_name):\n",
    "    \"\"\"Finds the best checkpoint from trainer_state.json and saves it.\"\"\"\n",
    "    try:\n",
    "        state_path = os.path.join(output_dir, \"trainer_state.json\")\n",
    "        with open(state_path, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "        \n",
    "        best_metric_value = None\n",
    "        best_checkpoint_path = None\n",
    "        metric_to_check = f\"eval_{metric_name}\"\n",
    "        is_loss = 'loss' in metric_to_check\n",
    "\n",
    "        for log in state[\"log_history\"]:\n",
    "            if metric_to_check in log:\n",
    "                metric_value = log[metric_to_check]\n",
    "                if best_metric_value is None or \\\n",
    "                   (is_loss and metric_value < best_metric_value) or \\\n",
    "                   (not is_loss and metric_value > best_metric_value):\n",
    "                    best_metric_value = metric_value\n",
    "                    step = log.get('step')\n",
    "                    if step:\n",
    "                        potential_path = os.path.join(output_dir, f\"checkpoint-{step}\")\n",
    "                        if os.path.exists(potential_path):\n",
    "                            best_checkpoint_path = potential_path\n",
    "\n",
    "        if not best_checkpoint_path:\n",
    "            logging.error(\"Could not find the best checkpoint from the logs.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"Best checkpoint found: {best_checkpoint_path} with {metric_to_check}: {best_metric_value}\")\n",
    "\n",
    "        final_model_path = os.path.join(output_dir, \"final_model\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            shutil.rmtree(final_model_path)\n",
    "            \n",
    "        shutil.copytree(best_checkpoint_path, final_model_path)\n",
    "        logging.info(f\"Best model copied to {final_model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not save the best model due to: {e}\", exc_info=True)\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and normalizes text to remove inconsistencies and hidden characters.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        tokenizer = T5Tokenizer.from_pretrained(BASE_MODEL_PATH, legacy=False)\n",
    "        # --- MODIFIED: Using T5ForConditionalGeneration ---\n",
    "        model = T5ForConditionalGeneration.from_pretrained(BASE_MODEL_PATH)\n",
    "\n",
    "        df_new = pd.read_csv(NEW_DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        df_new.dropna(subset=['raw_news_article', 'english_summary', 'hindi_summary'], inplace=True)\n",
    "        \n",
    "        logging.info(\"--- Starting Text Normalization ---\")\n",
    "        df_new['raw_news_article'] = df_new['raw_news_article'].apply(normalize_text)\n",
    "        df_new['english_summary'] = df_new['english_summary'].apply(normalize_text)\n",
    "        df_new['hindi_summary'] = df_new['hindi_summary'].apply(normalize_text)\n",
    "        logging.info(\"--- Text Normalization Finished ---\")\n",
    "        \n",
    "        raw_dataset = Dataset.from_pandas(df_new)\n",
    "\n",
    "        PREFIX_ENG = \"summarize English: \"\n",
    "        PREFIX_HIN = \"summarize Hindi: \"\n",
    "\n",
    "        def format_dataset(batch):\n",
    "            inputs, targets = [], []\n",
    "            for article, eng_summary, hin_summary in zip(\n",
    "                batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']\n",
    "            ):\n",
    "                if isinstance(article, str) and article:\n",
    "                    inputs.append(PREFIX_ENG + article)\n",
    "                    targets.append(eng_summary)\n",
    "                    inputs.append(PREFIX_HIN + article)\n",
    "                    targets.append(hin_summary)\n",
    "            return {'inputs': inputs, 'targets': targets}\n",
    "\n",
    "        processed_dataset = raw_dataset.map(\n",
    "            format_dataset, batched=True, remove_columns=raw_dataset.column_names\n",
    "        ).flatten()\n",
    "\n",
    "        train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        final_datasets = DatasetDict({\n",
    "            'train': train_test_split['train'],\n",
    "            'test': train_test_split['test']\n",
    "        })\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            model_inputs = tokenizer(examples['inputs'], max_length=1024, truncation=True)\n",
    "            labels = tokenizer(text_target=examples['targets'], max_length=MAX_SUMMARY_LENGTH_EVAL, truncation=True)\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "\n",
    "        tokenized_datasets = final_datasets.map(tokenize_function, batched=True, remove_columns=['inputs', 'targets'])\n",
    "        \n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "        bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "            # Note: BERTScore lang should ideally be dynamic, but 'en' is a safe default.\n",
    "            bert_result = bertscore_metric.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "            \n",
    "            result = {}\n",
    "            for key, value in rouge_result.items():\n",
    "                result[f\"rouge_{key}\"] = round(value * 100, 4)\n",
    "\n",
    "            result[\"bertscore_f1\"] = round(np.mean(bert_result[\"f1\"]) * 100, 4)\n",
    "\n",
    "            return result\n",
    "\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=NEW_MODEL_OUTPUT_DIR,\n",
    "            num_train_epochs=NUM_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            logging_dir=f\"{NEW_MODEL_OUTPUT_DIR}/logs\",\n",
    "            logging_steps=50,\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=NUM_EPOCHS,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            load_best_model_at_end=False,\n",
    "            generation_max_length=MAX_SUMMARY_LENGTH_EVAL,\n",
    "            generation_num_beams=NUM_BEAMS_EVAL,\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        logging.info(\"Starting training from scratch...\")\n",
    "        trainer.train()\n",
    "        logging.info(\"Training finished successfully.\")\n",
    "        \n",
    "        logging.info(\"Finding and saving the best model...\")\n",
    "        find_and_save_best_model(NEW_MODEL_OUTPUT_DIR, METRIC_FOR_BEST_MODEL)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during the main process: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ac17a19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b7a7b548edc4e82b8c02669e5a1de0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\admin\\.cache\\huggingface\\hub\\models--google--flan-t5-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39b98aed1f044e67b2f601c8b51f6eef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7f78db42dc842768b7706157b588f8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81031b040587437cab0286be16342e81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feb4203169294faaa0f828668ae1e58b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ece02ca22654d3998ef7e4c86cf6a21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c5aaf5e6f34edf9ad03447945025fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 18:36:31,131 [INFO] - --- Starting Text Normalization ---\n",
      "2025-10-06 18:36:32,659 [INFO] - --- Text Normalization Finished ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80c491d436604137ab44779d9a867790",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24f41e3fa4904c7182cf117a70f1b92d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b21ecd98f94406eadcd901a4c0b1696",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_21372\\277660677.py:187: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "2025-10-06 18:37:57,713 [INFO] - Starting training from scratch...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='242' max='10380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  242/10380 03:13 < 2:16:31, 1.24 it/s, Epoch 0.12/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 208\u001b[0m\n\u001b[0;32m    205\u001b[0m         logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn unexpected error occurred during the main process: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 208\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[15], line 198\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    187\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m    188\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    189\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    194\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m    195\u001b[0m )\n\u001b[0;32m    197\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training from scratch...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 198\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    199\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining finished successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    201\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinding and saving the best model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:2328\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2326\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2328\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2331\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2332\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2333\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\trainer.py:2713\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2711\u001b[0m         grad_norm_context \u001b[38;5;241m=\u001b[39m implicit_replication\n\u001b[0;32m   2712\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m grad_norm_context():\n\u001b[1;32m-> 2713\u001b[0m         _grad_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2714\u001b[0m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2715\u001b[0m \u001b[43m            \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_grad_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2716\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2718\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2719\u001b[0m     is_accelerate_available()\n\u001b[0;32m   2720\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2721\u001b[0m ):\n\u001b[0;32m   2722\u001b[0m     grad_norm \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_global_grad_norm()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\accelerate\\accelerator.py:2891\u001b[0m, in \u001b[0;36mAccelerator.clip_grad_norm_\u001b[1;34m(self, parameters, max_norm, norm_type)\u001b[0m\n\u001b[0;32m   2889\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mclip_grad_norm_(max_norm, norm_type)\n\u001b[0;32m   2890\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_gradients()\n\u001b[1;32m-> 2891\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_type\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:38\u001b[0m, in \u001b[0;36m_no_grad.<locals>._no_grad_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_no_grad_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:219\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[1;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[0;32m    217\u001b[0m     parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(parameters)\n\u001b[0;32m    218\u001b[0m grads \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m--> 219\u001b[0m total_norm \u001b[38;5;241m=\u001b[39m \u001b[43m_get_total_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_if_nonfinite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m _clip_grads_with_norm_(parameters, max_norm, total_norm, foreach)\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_norm\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:38\u001b[0m, in \u001b[0;36m_no_grad.<locals>._no_grad_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_no_grad_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 38\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:91\u001b[0m, in \u001b[0;36m_get_total_norm\u001b[1;34m(tensors, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (device, _), ([device_tensors], _) \u001b[38;5;129;01min\u001b[39;00m grouped_tensors\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m _has_foreach_support(device_tensors, device)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m     89\u001b[0m         foreach \u001b[38;5;129;01mand\u001b[39;00m _device_has_foreach_support(device)\n\u001b[0;32m     90\u001b[0m     ):\n\u001b[1;32m---> 91\u001b[0m         norms\u001b[38;5;241m.\u001b[39mextend(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_foreach_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m foreach:\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     94\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach=True was passed, but can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt use the foreach API on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     95\u001b[0m         )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "import unicodedata\n",
    "from datetime import datetime\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    T5Tokenizer,\n",
    "    T5ForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = \"google/flan-t5-base\"\n",
    "NEW_MODEL_OUTPUT_DIR = \"flan-t5-base-cnn-summarizer-en-hi_v10\"\n",
    "NEW_DATA_PATH = \"../Dataset/new_large_CNN_dataset.csv\"\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "WEIGHT_DECAY = 0.25\n",
    "NUM_BEAMS_EVAL = 6\n",
    "MAX_SUMMARY_LENGTH_EVAL = 256\n",
    "METRIC_FOR_BEST_MODEL = \"bertscore_f1\"\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"scratch_training_log_v10_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "    handlers=[logging.FileHandler(log_filename), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "\n",
    "def find_and_save_best_model(output_dir, metric_name):\n",
    "    \"\"\"Finds the best checkpoint from trainer_state.json and saves it.\"\"\"\n",
    "    try:\n",
    "        state_path = os.path.join(output_dir, \"trainer_state.json\")\n",
    "        with open(state_path, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "        \n",
    "        best_metric_value = None\n",
    "        best_checkpoint_path = None\n",
    "        metric_to_check = f\"eval_{metric_name}\"\n",
    "        is_loss = 'loss' in metric_to_check\n",
    "\n",
    "        for log in state[\"log_history\"]:\n",
    "            if metric_to_check in log:\n",
    "                metric_value = log[metric_to_check]\n",
    "                if best_metric_value is None or \\\n",
    "                   (is_loss and metric_value < best_metric_value) or \\\n",
    "                   (not is_loss and metric_value > best_metric_value):\n",
    "                    best_metric_value = metric_value\n",
    "                    step = log.get('step')\n",
    "                    if step:\n",
    "                        potential_path = os.path.join(output_dir, f\"checkpoint-{step}\")\n",
    "                        if os.path.exists(potential_path):\n",
    "                            best_checkpoint_path = potential_path\n",
    "\n",
    "        if not best_checkpoint_path:\n",
    "            logging.error(\"Could not find the best checkpoint from the logs.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"Best checkpoint found: {best_checkpoint_path} with {metric_to_check}: {best_metric_value}\")\n",
    "\n",
    "        final_model_path = os.path.join(output_dir, \"final_model\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            shutil.rmtree(final_model_path)\n",
    "            \n",
    "        shutil.copytree(best_checkpoint_path, final_model_path)\n",
    "        logging.info(f\"Best model copied to {final_model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not save the best model due to: {e}\", exc_info=True)\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and normalizes text to remove inconsistencies and hidden characters.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        tokenizer = T5Tokenizer.from_pretrained(BASE_MODEL_PATH, legacy=False)\n",
    "        model = T5ForConditionalGeneration.from_pretrained(BASE_MODEL_PATH)\n",
    "\n",
    "        df_new = pd.read_csv(NEW_DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        df_new.dropna(subset=['raw_news_article', 'english_summary', 'hindi_summary'], inplace=True)\n",
    "        \n",
    "        logging.info(\"--- Starting Text Normalization ---\")\n",
    "        df_new['raw_news_article'] = df_new['raw_news_article'].apply(normalize_text)\n",
    "        df_new['english_summary'] = df_new['english_summary'].apply(normalize_text)\n",
    "        df_new['hindi_summary'] = df_new['hindi_summary'].apply(normalize_text)\n",
    "        logging.info(\"--- Text Normalization Finished ---\")\n",
    "        \n",
    "        raw_dataset = Dataset.from_pandas(df_new)\n",
    "\n",
    "        PREFIX_ENG = \"summarize English: \"\n",
    "        PREFIX_HIN = \"summarize Hindi: \"\n",
    "\n",
    "        def format_dataset(batch):\n",
    "            inputs, targets = [], []\n",
    "            for article, eng_summary, hin_summary in zip(\n",
    "                batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']\n",
    "            ):\n",
    "                if isinstance(article, str) and article:\n",
    "                    inputs.append(PREFIX_ENG + article)\n",
    "                    targets.append(eng_summary)\n",
    "                    inputs.append(PREFIX_HIN + article)\n",
    "                    targets.append(hin_summary)\n",
    "            return {'inputs': inputs, 'targets': targets}\n",
    "\n",
    "        processed_dataset = raw_dataset.map(\n",
    "            format_dataset, batched=True, remove_columns=raw_dataset.column_names\n",
    "        ).flatten()\n",
    "\n",
    "        train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        final_datasets = DatasetDict({\n",
    "            'train': train_test_split['train'],\n",
    "            'test': train_test_split['test']\n",
    "        })\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            model_inputs = tokenizer(examples['inputs'], max_length=1024, truncation=True)\n",
    "            labels = tokenizer(text_target=examples['targets'], max_length=MAX_SUMMARY_LENGTH_EVAL, truncation=True)\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "\n",
    "        tokenized_datasets = final_datasets.map(tokenize_function, batched=True, remove_columns=['inputs', 'targets'])\n",
    "        \n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "        bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "            bert_result = bertscore_metric.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "            \n",
    "            result = {}\n",
    "            for key, value in rouge_result.items():\n",
    "                result[f\"rouge_{key}\"] = round(value * 100, 4)\n",
    "\n",
    "            result[\"bertscore_f1\"] = round(np.mean(bert_result[\"f1\"]) * 100, 4)\n",
    "\n",
    "            return result\n",
    "\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=NEW_MODEL_OUTPUT_DIR,\n",
    "            num_train_epochs=NUM_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            logging_dir=f\"{NEW_MODEL_OUTPUT_DIR}/logs\",\n",
    "            logging_steps=50,\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=NUM_EPOCHS,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            load_best_model_at_end=False,\n",
    "            generation_max_length=MAX_SUMMARY_LENGTH_EVAL,\n",
    "            generation_num_beams=NUM_BEAMS_EVAL,\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        logging.info(\"Starting training from scratch...\")\n",
    "        trainer.train()\n",
    "        logging.info(\"Training finished successfully.\")\n",
    "        \n",
    "        logging.info(\"Finding and saving the best model...\")\n",
    "        find_and_save_best_model(NEW_MODEL_OUTPUT_DIR, METRIC_FOR_BEST_MODEL)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during the main process: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8965508c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c263ff772fab4ee7b8e216da4b2fed47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/531 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\admin\\.cache\\huggingface\\hub\\models--facebook--mbart-large-50. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "527009e810774b099bc9ba5208e5acdd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354db8e7269842cd836e17384b3e3119",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/649 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637df61b09f3476d91bb8233055956a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd953fe054e4e1886cd22e301481f98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbaeb535c8174e59b65226714b4c24a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.44G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e939e6ad04544c539ddb2adfb148221f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/261 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 18:42:03,296 [INFO] - --- Starting Text Normalization ---\n",
      "2025-10-06 18:42:05,142 [INFO] - --- Text Normalization Finished ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28773c3a506c426fafa42699c8c75e9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "548bb95610dc42669bf35aeac48f36ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c7991f27b64ca88516f1fce42114da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_21372\\739669738.py:199: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "2025-10-06 18:42:52,595 [INFO] - Starting training from scratch with mBART model...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10380' max='10380' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10380/10380 2:18:52, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.214000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.642400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.520700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.416400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.318100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.274700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.177700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.138000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.075500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.037900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.062900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.984400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.987000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>2.010600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.966300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.953000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.931900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.901300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.926700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.923300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.889800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.892400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.868000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.849000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.905000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.872700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.827200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.852400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.784800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.836100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.809100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.771100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.832300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.806300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.762700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.808200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.758800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.763200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.783100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.761100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>1.782900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.686700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>1.582300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>1.590800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>1.585500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>1.538000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>1.554500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>1.574600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>1.585000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.600800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>1.574100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>1.564200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>1.538400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>1.593300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>1.584500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.561400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>1.568700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.578900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>1.586800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.538800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>1.572000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.561200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>1.561100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.557800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>1.563900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.547700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>1.556100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.544300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>1.539300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.538200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>1.572400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.499500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>1.547600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.500100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>1.517500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.528000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>1.505200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.520800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>1.549000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.504600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>1.505000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.510900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>1.544300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.377800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>1.352800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.360700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>1.374700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.389200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>1.366600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.386400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>1.361200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.351500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>1.367800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>1.410200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>1.408000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.376600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>1.380800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>1.385500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>1.336300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.333900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>1.341400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>1.384600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>1.352000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.350500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>1.348300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>1.388500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>1.353800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.352600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>1.348700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.332800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>1.374400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.364000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>1.348500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>1.368900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>1.360800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.385100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>1.333500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>1.322800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>1.348900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.358400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>1.320700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>1.387600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>1.333300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>1.332300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>1.249600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>1.225400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.228700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>1.258400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.244900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>1.238000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.234600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>1.219500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>1.212500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>1.214400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>1.240200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>1.250100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>1.238900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>1.240700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.239600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>1.219400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>1.247600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>1.211700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>1.207200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>1.259500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>1.250700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>1.252500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>1.227100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>1.259600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.241700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>1.244400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>1.236700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>1.231600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>1.226600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>1.201000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>1.255500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>1.259200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>1.239300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7950</td>\n",
       "      <td>1.227200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.236000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>1.265700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>1.240500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8150</td>\n",
       "      <td>1.218500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>1.256200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>1.213100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>1.212200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8350</td>\n",
       "      <td>1.152100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>1.184200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8450</td>\n",
       "      <td>1.169800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>1.141500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8550</td>\n",
       "      <td>1.128300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>1.140600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8650</td>\n",
       "      <td>1.176900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8700</td>\n",
       "      <td>1.170700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8750</td>\n",
       "      <td>1.150000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>1.146100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8850</td>\n",
       "      <td>1.141200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8900</td>\n",
       "      <td>1.176500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8950</td>\n",
       "      <td>1.149600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>1.139300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9050</td>\n",
       "      <td>1.144000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9100</td>\n",
       "      <td>1.116400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9150</td>\n",
       "      <td>1.150700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>1.190700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9250</td>\n",
       "      <td>1.171100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9300</td>\n",
       "      <td>1.171200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9350</td>\n",
       "      <td>1.171700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>1.139200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9450</td>\n",
       "      <td>1.151000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>1.158000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9550</td>\n",
       "      <td>1.173100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>1.148200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9650</td>\n",
       "      <td>1.149600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9700</td>\n",
       "      <td>1.164600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9750</td>\n",
       "      <td>1.169500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>1.163200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9850</td>\n",
       "      <td>1.124000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9900</td>\n",
       "      <td>1.158100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9950</td>\n",
       "      <td>1.154500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>1.135700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10050</td>\n",
       "      <td>1.153800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10100</td>\n",
       "      <td>1.158300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10150</td>\n",
       "      <td>1.161500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>1.151700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10250</td>\n",
       "      <td>1.154400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10300</td>\n",
       "      <td>1.163300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10350</td>\n",
       "      <td>1.141800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\modeling_utils.py:4037: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "2025-10-06 21:01:46,637 [INFO] - Training finished successfully.\n",
      "2025-10-06 21:01:46,637 [INFO] - Finding and saving the best model...\n",
      "2025-10-06 21:01:46,643 [ERROR] - Could not save the best model due to: [Errno 2] No such file or directory: 'mbart-large-50-cnn-summarizer-en-hi_v11\\\\trainer_state.json'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_21372\\739669738.py\", line 49, in find_and_save_best_model\n",
      "    with open(state_path, \"r\") as f:\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 324, in _modified_open\n",
      "    return io_open(file, *args, **kwargs)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'mbart-large-50-cnn-summarizer-en-hi_v11\\\\trainer_state.json'\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "import unicodedata\n",
    "from datetime import datetime\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "# --- MODIFIED: Switched to a different model architecture (mBART) ---\n",
    "BASE_MODEL_PATH = \"facebook/mbart-large-50\"\n",
    "NEW_MODEL_OUTPUT_DIR = \"mbart-large-50-cnn-summarizer-en-hi_v11\"\n",
    "NEW_DATA_PATH = \"../Dataset/new_large_CNN_dataset.csv\"\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "WEIGHT_DECAY = 0.25\n",
    "NUM_BEAMS_EVAL = 6\n",
    "MAX_SUMMARY_LENGTH_EVAL = 256\n",
    "METRIC_FOR_BEST_MODEL = \"bertscore_f1\"\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"scratch_training_log_v11_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "    handlers=[logging.FileHandler(log_filename), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "\n",
    "def find_and_save_best_model(output_dir, metric_name):\n",
    "    \"\"\"Finds the best checkpoint from trainer_state.json and saves it.\"\"\"\n",
    "    try:\n",
    "        state_path = os.path.join(output_dir, \"trainer_state.json\")\n",
    "        with open(state_path, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "        \n",
    "        best_metric_value = None\n",
    "        best_checkpoint_path = None\n",
    "        metric_to_check = f\"eval_{metric_name}\"\n",
    "        is_loss = 'loss' in metric_to_check\n",
    "\n",
    "        for log in state[\"log_history\"]:\n",
    "            if metric_to_check in log:\n",
    "                metric_value = log[metric_to_check]\n",
    "                if best_metric_value is None or \\\n",
    "                   (is_loss and metric_value < best_metric_value) or \\\n",
    "                   (not is_loss and metric_value > best_metric_value):\n",
    "                    best_metric_value = metric_value\n",
    "                    step = log.get('step')\n",
    "                    if step:\n",
    "                        potential_path = os.path.join(output_dir, f\"checkpoint-{step}\")\n",
    "                        if os.path.exists(potential_path):\n",
    "                            best_checkpoint_path = potential_path\n",
    "\n",
    "        if not best_checkpoint_path:\n",
    "            logging.error(\"Could not find the best checkpoint from the logs.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"Best checkpoint found: {best_checkpoint_path} with {metric_to_check}: {best_metric_value}\")\n",
    "\n",
    "        final_model_path = os.path.join(output_dir, \"final_model\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            shutil.rmtree(final_model_path)\n",
    "            \n",
    "        shutil.copytree(best_checkpoint_path, final_model_path)\n",
    "        logging.info(f\"Best model copied to {final_model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not save the best model due to: {e}\", exc_info=True)\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Cleans and normalizes text to remove inconsistencies and hidden characters.\n",
    "    \"\"\"\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = unicodedata.normalize('NFKC', text)\n",
    "    text = ' '.join(text.split())\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        # --- MODIFIED: Using mBART specific tokenizer and model ---\n",
    "        tokenizer = MBart50TokenizerFast.from_pretrained(BASE_MODEL_PATH)\n",
    "        model = MBartForConditionalGeneration.from_pretrained(BASE_MODEL_PATH)\n",
    "\n",
    "        df_new = pd.read_csv(NEW_DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        df_new.dropna(subset=['raw_news_article', 'english_summary', 'hindi_summary'], inplace=True)\n",
    "        \n",
    "        logging.info(\"--- Starting Text Normalization ---\")\n",
    "        df_new['raw_news_article'] = df_new['raw_news_article'].apply(normalize_text)\n",
    "        df_new['english_summary'] = df_new['english_summary'].apply(normalize_text)\n",
    "        df_new['hindi_summary'] = df_new['hindi_summary'].apply(normalize_text)\n",
    "        logging.info(\"--- Text Normalization Finished ---\")\n",
    "        \n",
    "        raw_dataset = Dataset.from_pandas(df_new)\n",
    "\n",
    "        # --- MODIFIED: Added 'lang' column for mBART's tokenizer ---\n",
    "        def format_dataset(batch):\n",
    "            inputs, targets, langs = [], [], []\n",
    "            for article, eng_summary, hin_summary in zip(\n",
    "                batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']\n",
    "            ):\n",
    "                if isinstance(article, str) and article:\n",
    "                    # English Task\n",
    "                    inputs.append(article)\n",
    "                    targets.append(eng_summary)\n",
    "                    langs.append(\"en_XX\")\n",
    "                    # Hindi Task\n",
    "                    inputs.append(article)\n",
    "                    targets.append(hin_summary)\n",
    "                    langs.append(\"hi_IN\")\n",
    "            return {'inputs': inputs, 'targets': targets, 'lang': langs}\n",
    "\n",
    "        processed_dataset = raw_dataset.map(\n",
    "            format_dataset, batched=True, remove_columns=raw_dataset.column_names\n",
    "        ).flatten()\n",
    "\n",
    "        train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        final_datasets = DatasetDict({\n",
    "            'train': train_test_split['train'],\n",
    "            'test': train_test_split['test']\n",
    "        })\n",
    "        \n",
    "        # --- MODIFIED: Tokenize function adapted for mBART ---\n",
    "        def tokenize_function(examples):\n",
    "            # Set source language for all articles\n",
    "            tokenizer.src_lang = \"en_XX\" \n",
    "            model_inputs = tokenizer(examples['inputs'], max_length=1024, truncation=True)\n",
    "\n",
    "            # Process labels for the whole batch, setting target language for each\n",
    "            all_labels = []\n",
    "            for i in range(len(examples['targets'])):\n",
    "                tokenizer.tgt_lang = examples['lang'][i]\n",
    "                label_ids = tokenizer(text_target=examples['targets'][i], max_length=MAX_SUMMARY_LENGTH_EVAL, truncation=True).input_ids\n",
    "                all_labels.append(label_ids)\n",
    "            model_inputs['labels'] = all_labels\n",
    "            return model_inputs\n",
    "\n",
    "        tokenized_datasets = final_datasets.map(tokenize_function, batched=True, remove_columns=['inputs', 'targets', 'lang'])\n",
    "        \n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "        bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "            # --- MODIFIED: Forcing target language for decoding predictions ---\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            \n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "            bert_result = bertscore_metric.compute(predictions=decoded_preds, references=decoded_labels, lang=\"en\")\n",
    "            \n",
    "            result = {}\n",
    "            for key, value in rouge_result.items():\n",
    "                result[f\"rouge_{key}\"] = round(value * 100, 4)\n",
    "\n",
    "            result[\"bertscore_f1\"] = round(np.mean(bert_result[\"f1\"]) * 100, 4)\n",
    "            return result\n",
    "\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=NEW_MODEL_OUTPUT_DIR,\n",
    "            num_train_epochs=NUM_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            logging_dir=f\"{NEW_MODEL_OUTPUT_DIR}/logs\",\n",
    "            logging_steps=50,\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=NUM_EPOCHS,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            load_best_model_at_end=False,\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        logging.info(\"Starting training from scratch with mBART model...\")\n",
    "        trainer.train()\n",
    "        logging.info(\"Training finished successfully.\")\n",
    "        \n",
    "        logging.info(\"Finding and saving the best model...\")\n",
    "        find_and_save_best_model(NEW_MODEL_OUTPUT_DIR, METRIC_FOR_BEST_MODEL)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during the main process: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d5dbfe87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-06 22:33:20,551 [INFO] - Attempting to find best model in: mbart-large-50-cnn-summarizer-en-hi_v11\n",
      "2025-10-06 22:33:20,551 [INFO] - Contents of 'mbart-large-50-cnn-summarizer-en-hi_v11': ['checkpoint-10380', 'checkpoint-2076', 'checkpoint-4152', 'checkpoint-6228', 'checkpoint-8304', 'logs']\n",
      "2025-10-06 22:33:20,551 [WARNING] - Could not find any evaluation metric logs.\n",
      "2025-10-06 22:33:20,551 [WARNING] - Defaulting to the LAST saved checkpoint as the best model.\n",
      "2025-10-06 22:33:20,551 [INFO] - Identified last checkpoint: mbart-large-50-cnn-summarizer-en-hi_v11\\checkpoint-10380\n",
      "2025-10-06 22:33:20,551 [INFO] - --- Model Identified for Saving ---\n",
      "2025-10-06 22:33:20,566 [INFO] - Checkpoint: mbart-large-50-cnn-summarizer-en-hi_v11\\checkpoint-10380\n",
      "2025-10-06 22:35:00,801 [INFO] - Successfully copied best model to: mbart-large-50-cnn-summarizer-en-hi_v11\\final_model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Configuration: MODIFY THESE TWO VARIABLES ---\n",
    "# Set this to the output directory of your completed (but failed to save) training run.\n",
    "OUTPUT_DIR = \"mbart-large-50-cnn-summarizer-en-hi_v11\" \n",
    "# Set this to the metric you used to determine the best model.\n",
    "METRIC_NAME = \"bertscore_f1\"\n",
    "# -------------------------------------------------\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"save_best_model_log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "    handlers=[logging.FileHandler(log_filename), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "def find_and_save_best_checkpoint(output_dir, metric_name):\n",
    "    \"\"\"\n",
    "    Reads training logs to identify the best checkpoint based on a specified metric.\n",
    "    If no metrics are found, it defaults to saving the last available checkpoint.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Attempting to find best model in: {output_dir}\")\n",
    "\n",
    "        if not os.path.isdir(output_dir):\n",
    "            logging.error(f\"FATAL: The directory '{output_dir}' does not exist.\")\n",
    "            return\n",
    "        \n",
    "        logging.info(f\"Contents of '{output_dir}': {os.listdir(output_dir)}\")\n",
    "        \n",
    "        best_metric_value = None\n",
    "        best_checkpoint_path = None\n",
    "        metric_to_check = f\"eval_{metric_name}\"\n",
    "        is_loss = 'loss' in metric_to_check.lower()\n",
    "        log_history = []\n",
    "        \n",
    "        # Strategies to find evaluation logs\n",
    "        main_state_path = os.path.join(output_dir, \"trainer_state.json\")\n",
    "        if os.path.exists(main_state_path):\n",
    "            with open(main_state_path, \"r\") as f:\n",
    "                state = json.load(f)\n",
    "            log_history = state[\"log_history\"]\n",
    "        \n",
    "        if not log_history:\n",
    "            checkpoint_dirs = [d for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")]\n",
    "            for chkpt_dir in checkpoint_dirs:\n",
    "                chkpt_state_path = os.path.join(output_dir, chkpt_dir, \"trainer_state.json\")\n",
    "                if os.path.exists(chkpt_state_path):\n",
    "                    with open(chkpt_state_path, \"r\") as f:\n",
    "                        chkpt_state = json.load(f)\n",
    "                    for log in chkpt_state[\"log_history\"]:\n",
    "                        if metric_to_check in log: log_history.append(log)\n",
    "\n",
    "        if not log_history:\n",
    "            checkpoint_dirs = sorted([d for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")], key=lambda x: int(x.split('-')[-1]))\n",
    "            for chkpt_dir in checkpoint_dirs:\n",
    "                eval_results_path = os.path.join(output_dir, chkpt_dir, \"eval_results.json\")\n",
    "                if os.path.exists(eval_results_path):\n",
    "                    with open(eval_results_path, \"r\") as f:\n",
    "                        eval_results = json.load(f)\n",
    "                    if metric_to_check in eval_results:\n",
    "                        log_history.append({\"step\": int(chkpt_dir.split('-')[-1]), metric_to_check: eval_results[metric_to_check]})\n",
    "\n",
    "        if log_history:\n",
    "            logging.info(f\"Searching for best score using metric: '{metric_to_check}'\")\n",
    "            for log in log_history:\n",
    "                if metric_to_check in log:\n",
    "                    metric_value, step = log[metric_to_check], log.get('step')\n",
    "                    if step is None: continue\n",
    "                    if best_metric_value is None or (is_loss and metric_value < best_metric_value) or (not is_loss and metric_value > best_metric_value):\n",
    "                        potential_path = os.path.join(output_dir, f\"checkpoint-{step}\")\n",
    "                        if os.path.exists(potential_path):\n",
    "                            best_metric_value, best_checkpoint_path = metric_value, potential_path\n",
    "                            logging.info(f\"New best found -> Step: {step}, {metric_to_check}: {metric_value}\")\n",
    "        else:\n",
    "            # --- NEW: FINAL FALLBACK ---\n",
    "            logging.warning(\"Could not find any evaluation metric logs.\")\n",
    "            logging.warning(\"Defaulting to the LAST saved checkpoint as the best model.\")\n",
    "            checkpoint_dirs = [d for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")]\n",
    "            if checkpoint_dirs:\n",
    "                latest_step = -1\n",
    "                for chkpt_dir in checkpoint_dirs:\n",
    "                    try:\n",
    "                        step = int(chkpt_dir.split('-')[-1])\n",
    "                        if step > latest_step:\n",
    "                            latest_step = step\n",
    "                            best_checkpoint_path = os.path.join(output_dir, chkpt_dir)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                if best_checkpoint_path:\n",
    "                     logging.info(f\"Identified last checkpoint: {best_checkpoint_path}\")\n",
    "\n",
    "        if not best_checkpoint_path:\n",
    "            logging.error(\"FATAL: Could not find any valid checkpoints to save.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"--- Model Identified for Saving ---\")\n",
    "        logging.info(f\"Checkpoint: {best_checkpoint_path}\")\n",
    "        if best_metric_value is not None:\n",
    "            logging.info(f\"Metric ({metric_to_check}): {best_metric_value}\")\n",
    "\n",
    "        final_model_path = os.path.join(output_dir, \"final_model\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            logging.warning(f\"Removing existing 'final_model' directory: {final_model_path}\")\n",
    "            shutil.rmtree(final_model_path)\n",
    "            \n",
    "        shutil.copytree(best_checkpoint_path, final_model_path)\n",
    "        logging.info(f\"Successfully copied best model to: {final_model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    find_and_save_best_checkpoint(OUTPUT_DIR, METRIC_NAME)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarizer_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
