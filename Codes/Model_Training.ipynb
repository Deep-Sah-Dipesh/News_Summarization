{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00dcb555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (4.0.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (0.2.1)\n",
      "Collecting rouge_score\n",
      "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting sacrebleu\n",
      "  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n",
      "Requirement already satisfied: transformers[torch] in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (4.41.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from transformers[torch]) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from transformers[torch]) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from transformers[torch]) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from transformers[torch]) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from transformers[torch]) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from transformers[torch]) (2025.9.1)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from transformers[torch]) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from transformers[torch]) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from transformers[torch]) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from transformers[torch]) (4.67.1)\n",
      "Requirement already satisfied: torch in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from transformers[torch]) (2.5.1+cu121)\n",
      "Collecting accelerate>=0.21.0 (from transformers[torch])\n",
      "  Downloading accelerate-1.10.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from huggingface-hub<1.0,>=0.23.0->transformers[torch]) (4.15.0)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from datasets) (2.3.2)\n",
      "Requirement already satisfied: xxhash in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Collecting absl-py (from rouge_score)\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: nltk in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from rouge_score) (1.17.0)\n",
      "Collecting portalocker (from sacrebleu)\n",
      "  Using cached portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
      "Collecting tabulate>=0.8.9 (from sacrebleu)\n",
      "  Using cached tabulate-0.9.0-py3-none-any.whl.metadata (34 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from sacrebleu) (0.4.6)\n",
      "Requirement already satisfied: lxml in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from sacrebleu) (6.0.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from accelerate>=0.21.0->transformers[torch]) (7.0.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.7)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from requests->transformers[torch]) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from requests->transformers[torch]) (2025.8.3)\n",
      "Requirement already satisfied: networkx in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from torch->transformers[torch]) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from torch->transformers[torch]) (3.1.6)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from torch->transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from sympy==1.13.1->torch->transformers[torch]) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from jinja2->torch->transformers[torch]) (3.0.2)\n",
      "Requirement already satisfied: click in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from nltk->rouge_score) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from nltk->rouge_score) (1.5.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: pywin32>=226 in c:\\users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages (from portalocker->sacrebleu) (311)\n",
      "Downloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n",
      "Downloading accelerate-1.10.1-py3-none-any.whl (374 kB)\n",
      "Using cached tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
      "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Using cached portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
      "Building wheels for collected packages: rouge_score\n",
      "  Building wheel for rouge_score (setup.py): started\n",
      "  Building wheel for rouge_score (setup.py): finished with status 'done'\n",
      "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=25027 sha256=3f2ac4bf92f92485a4d44b13c7c1aa4c7ecf1520d7b5ad362ef8acae64315548\n",
      "  Stored in directory: c:\\users\\admin\\appdata\\local\\pip\\cache\\wheels\\5f\\dd\\89\\461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
      "Successfully built rouge_score\n",
      "Installing collected packages: tabulate, portalocker, absl-py, sacrebleu, rouge_score, accelerate\n",
      "\n",
      "   ------ --------------------------------- 1/6 [portalocker]\n",
      "   ------------- -------------------------- 2/6 [absl-py]\n",
      "   ------------- -------------------------- 2/6 [absl-py]\n",
      "   -------------------- ------------------- 3/6 [sacrebleu]\n",
      "   -------------------- ------------------- 3/6 [sacrebleu]\n",
      "   -------------------- ------------------- 3/6 [sacrebleu]\n",
      "   -------------------------- ------------- 4/6 [rouge_score]\n",
      "   --------------------------------- ------ 5/6 [accelerate]\n",
      "   --------------------------------- ------ 5/6 [accelerate]\n",
      "   --------------------------------- ------ 5/6 [accelerate]\n",
      "   --------------------------------- ------ 5/6 [accelerate]\n",
      "   --------------------------------- ------ 5/6 [accelerate]\n",
      "   --------------------------------- ------ 5/6 [accelerate]\n",
      "   --------------------------------- ------ 5/6 [accelerate]\n",
      "   --------------------------------- ------ 5/6 [accelerate]\n",
      "   ---------------------------------------- 6/6 [accelerate]\n",
      "\n",
      "Successfully installed absl-py-2.3.1 accelerate-1.10.1 portalocker-3.2.0 rouge_score-0.1.2 sacrebleu-2.5.1 tabulate-0.9.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'rouge_score' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'rouge_score'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
     ]
    }
   ],
   "source": [
    "pip install transformers[torch] datasets sentencepiece rouge_score sacrebleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d49240eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1+cu121\n",
      "--- GPU Information ---\n",
      "✅ CUDA is available!\n",
      "Number of GPUs: 1\n",
      "GPU Name: NVIDIA RTX A5000\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(\"--- GPU Information ---\")\n",
    "if torch.cuda.is_available():\n",
    "    print(\"✅ CUDA is available!\")\n",
    "    print(f\"Number of GPUs: {torch.cuda.device_count()}\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    print(\"❌ CUDA is not available. PyTorch is running on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "67587978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4919/4919 [00:00<00:00, 15757.47 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['inputs', 'targets'],\n",
      "        num_rows: 8854\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['inputs', 'targets'],\n",
      "        num_rows: 984\n",
      "    })\n",
      "})\n",
      "\n",
      "--- English Task Sample ---\n",
      "Input: summarize Hindi: (CNN)Canadian actor Jonathan Crombie, who co-starred in the \"Anne of Green Gables\" TV movies, died this week at age 48. Crombie died Wednesday from complications of a brain hemorrhage, \"Anne of Green Gables\" producer Kevin Sullivan said. \"It's a real tragedy to see someone at age 48 go like that,\" he said. \"I will remember him as someone who worked extremely hard to make the roles he played onscreen come to life.\" Based on Canadian author Lucy Maud Montgomery's children's books, \"Anne of Green Gables\" debuted in Canada on CBC TV in 1984 and became a cultural touchstone. The plot focused on the adventures of fiery orphan Anne Shirley, played by Megan Follows, who is sent to live on a farm in Prince Edward Island. Crombie played Gilbert Blythe, who evolves over time from Anne's pigtail-tugging tormentor to friend to husband. Follows and Crombie reprised the roles in the sequels \"Anne of Avonlea\" (1987) and \"Anne of Green Gables: The Continuing Story\" (2000). The movies were carried in the United States by the Disney Channel and PBS, drawing a cult following beyond Canada and extending to Japan, which made its own animated series based on the books. Crombie, son of former Toronto Mayor David Crombie, was cast in the role at 17, beating out other aspiring Canadian actors of the era, including Jason Priestly, Sullivan said. Despite his lack of acting experience, Crombie's boy-next-door looks and cool demeanor made him the perfect actor to star opposite Follows, Sullivan said. \"It was an amazing chemistry between him and Megan Follows,\" Sullivan said. \"There was a lot of affection, but they kind of grounded each other.\" The movies spawned various spinoffs, including \"Road to Avonlea,\" starring child actor Sarah Polley, and turned Anne's fictional home on Prince Edward Island into a popular tourist destination. The role made Crombie a heartthrob of his time, a sentiment expressed by many fans in the wake of his death. As one person said on Twitter, \"I don't know any female Canadian from my generation that *didn't* have at least a little bit of a crush on Jonathan Crombie as Gilbert.\" Crombie went on to play roles in other American and Canadian TV shows, including \"21 Jump Street\" and \"The Good Wife,\" but even his Facebook page acknowledges he is best known for his portrayal of Gilbert Blythe. Crombie's sister told CBC News that her brother happily answered to the name Gil when greeted by fans in public. \"I think he was really proud of being Gilbert Blythe,\" she said. \"He really enjoyed that series and was happy, very proud of it -- we all were.\" People we've lost in 2015 .\n",
      "Target: कनाडाई अभिनेता जोनाथन क्रॉम्बी, जिन्हें \"ऐन ऑफ ग्रीन गेबल्स\" (Anne of Green Gables) टीवी फिल्मों में गिलबर्ट ब्लाइथ (Gilbert Blythe) की भूमिका के लिए सबसे ज्यादा जाना जाता है, का इस सप्ताह 48 वर्ष की आयु में मस्तिष्क रक्तस्राव (brain hemorrhage) की जटिलताओं के कारण निधन हो गया। क्रॉम्बी ने 1984 की मूल फिल्म और उसके सीक्वल में ऐन शर्ली (Anne Shirley) के रूप में मेगन फॉलोव्स (Megan Follows) के साथ दमदार ऑन-स्क्रीन केमिस्ट्री साझा की थी। गिलबर्ट ब्लाइथ के उनके चित्रण ने उन्हें सबका पसंदीदा बना दिया और एक सांस्कृतिक पहचान दी, खासकर कनाडा में। निर्माता केविन सुलिवान (Kevin Sullivan) ने क्रॉम्बी की कड़ी मेहनत और उनकी असामयिक मृत्यु को एक त्रासदी बताया। \"21 जंप स्ट्रीट\" (21 Jump Street) और \"द गुड वाइफ\" (The Good Wife) जैसे अन्य शो में भूमिकाओं के बावजूद, क्रॉम्बी को अपनी प्रतिष्ठित \"ऐन ऑफ ग्रीन गेबल्स\" भूमिका के लिए सबसे अधिक पहचाना जाता था और उन्हें उस पर गर्व था।\n",
      "\n",
      "--- Hindi Task Sample ---\n",
      "Input: summarize Hindi: (CNN)A popular Chinese television host known for impromptu satire is now the subject of controversy after being caught on camera cursing the late Chairman Mao Zedong. Bi Fujian, who works for state-run China Central Television, was filmed at a dinner party singing a revolutionary song that eulogizes the Communist Party's early years when he started going off script. \"The Communist Party, Chairman Mao. Don't mention that old son of a b***h. He made us suffer so bad,\" went Bi's improvised lyrics. The other dinner guests burst into laughter. Bi later apologized. \"My personal speech has led to grave social consequences, and I feel remorseful for that. I hereby sincerely apologize to the public. As a public figure, I shall learn the lesson from this incident, adhering to strict self-discipline,\" he posted on Weibo, China's Twitter-like social media platform. Making disrespectful references to China's leaders in public is considered a taboo in China, even today. And Bi's comment was directed at the man regarded by many as the country's founding father -- despite his controversial reputation. The 75-second video clip, seemingly filmed on the cellphone of another dinner guest, was uploaded on Monday. Since then, it has been removed from video-sharing sites inside China, although it was still accessible on Weibo. It's unclear when the incident occurred, or what the relationsip was between the camera person and Bi. CCTV said it would investigate. \"As a CCTV presenter, Bi Fujian's speech in the online video has led to grave social consequences,\" the network said in a statement posted on its Weibo account. CCTV did not respond to a CNN request for comment. Fondly known as \"Grandpa Bi,\" the 56-year-old TV personality was born and grew up in the Mao era. The song Bi riffed on was part of a \"red\" Peking opera that was first performed in the late 1950s. It was popularized during the Cultural Revolution of the 1960s and 1970s -- which was launched by Mao -- when China was torn apart by violence and social unrest. The video quickly divided China's online community. Critics said Bi, as an influential public figure, deserved a harsh punishment. But others rushed to his defense, arguing that Bi was simply enjoying himself in a private setting and was set up by whoever uploaded the clip. The video also emerged just a day before the new head of CCTV started his job, leading some to wonder if it were a case of \"a new broom sweeps clean.\" Mao still divides opinion in China. His giant portrait hangs on Beijing's Tiananmen Gate, and thousands flock to see his embalmed body at his mausoleum in Tiananmen Square in the heart of the Chinese capital. But despite this reverence, Mao's is a deeply flawed legacy. Many remember him as a brutal dictator who inspired fear, paranoia and famine, and whose actions resulted in tens of millions of deaths. CNN's Shen Lu contributed to this report.\n",
      "Target: चीन के सरकारी चैनल CCTV के टीवी होस्ट बी फुजियान एक निजी डिनर पार्टी में दिवंगत चेयरमैन माओ ज़ेडोंग को गाली देते हुए एक वीडियो में पकड़े जाने के बाद विवादों में घिर गए हैं। एक क्रांतिकारी गीत गाते हुए, बी फुजियान ने माओ को 'एक बूढ़ा कमीना' कहा और उन्हें लोगों को बहुत पीड़ा पहुँचाने का आरोप लगाया। ऑनलाइन अपलोड किए गए 75-सेकेंड के इस वीडियो के बाद, बी फुजियान ने वीबो पर सार्वजनिक माफी जारी की, जिसमें उन्होंने अपने भाषण के 'गंभीर सामाजिक परिणामों' के लिए पश्चाताप व्यक्त किया। CCTV ने बी फुजियान के आचरण की जांच करने की घोषणा की, इस बात पर ज़ोर दिया कि चीन में नेताओं के प्रति अपमानजनक संदर्भ वर्जित हैं, खासकर माओ के प्रति, जिन्हें उनकी विवादास्पद विरासत के बावजूद देश का संस्थापक पिता माना जाता है। इस घटना ने चीन के ऑनलाइन समुदाय को विभाजित कर दिया, कुछ ने सज़ा की मांग की जबकि अन्य ने बी फुजियान की निजी टिप्पणियों का बचाव किया। माओ की विरासत चीन में अभी भी विवाद का विषय बनी हुई है।\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load data using pandas and remove any empty rows\n",
    "df = pd.read_csv('../Dataset/final_cleaned_dataset_CNN.csv').dropna().reset_index(drop=True)\n",
    "raw_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Define the task prefixes for the model\n",
    "PREFIX_ENG = \"summarize English: \"\n",
    "PREFIX_HIN = \"summarize Hindi: \"\n",
    "\n",
    "def format_dataset(batch):\n",
    "    \"\"\"Transforms a batch of data for the two summarization tasks.\"\"\"\n",
    "    inputs, targets = [], []\n",
    "    \n",
    "    # Create alternating examples for English and Hindi summarization\n",
    "    for article, eng_summary, hin_summary in zip(\n",
    "        batch['raw_news_article'],\n",
    "        batch['english_summary'],\n",
    "        batch['hindi_summary']\n",
    "    ):\n",
    "        if isinstance(article, str):\n",
    "            # English Task\n",
    "            inputs.append(PREFIX_ENG + article)\n",
    "            targets.append(eng_summary)\n",
    "            \n",
    "            # Hindi Task\n",
    "            inputs.append(PREFIX_HIN + article)\n",
    "            targets.append(hin_summary)\n",
    "            \n",
    "    return {'inputs': inputs, 'targets': targets}\n",
    "\n",
    "# Apply the formatting and create train/test splits\n",
    "processed_dataset = raw_dataset.map(\n",
    "    format_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=raw_dataset.column_names\n",
    ").flatten()\n",
    "\n",
    "train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "final_datasets = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'test': train_test_split['test']\n",
    "})\n",
    "\n",
    "# Verify the structure and samples\n",
    "print(final_datasets)\n",
    "\n",
    "print(\"\\n--- English Task Sample ---\")\n",
    "print(\"Input:\", final_datasets['train'][0]['inputs'])\n",
    "print(\"Target:\", final_datasets['train'][0]['targets'])\n",
    "\n",
    "print(\"\\n--- Hindi Task Sample ---\")\n",
    "print(\"Input:\", final_datasets['train'][1]['inputs'])\n",
    "print(\"Target:\", final_datasets['train'][1]['targets'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e7d58b0",
   "metadata": {},
   "source": [
    "Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c00d90a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4919/4919 [00:00<00:00, 23037.16 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Verifying the first 6 samples of the SHUFFLED training data ---\n",
      "\n",
      "--- Sample 1 ---\n",
      "summarize Hindi: (CNN)Canadian actor Jonathan Crombie, who co-starred in the \"Anne of Green Gables\" TV movies, died this week at age 48. Crombie died Wednesday from complications of a brain hemorrhage, \"Anne of Green Gables\" producer Kevin Sullivan said. \"It's a real tragedy to see someone at age 48 go like that,\" he said. \"I will remember him as someone who worked extremely hard to make the roles he played onscreen come to life.\" Based on Canadian author Lucy Maud Montgomery's children's books, \"Anne of Green Gables\" debuted in Canada on CBC TV in 1984 and became a cultural touchstone. The plot focused on the adventures of fiery orphan Anne Shirley, played by Megan Follows, who is sent to live on a farm in Prince Edward Island. Crombie played Gilbert Blythe, who evolves over time from Anne's pigtail-tugging tormentor to friend to husband. Follows and Crombie reprised the roles in the sequels \"Anne of Avonlea\" (1987) and \"Anne of Green Gables: The Continuing Story\" (2000). The movies were carried in the United States by the Disney Channel and PBS, drawing a cult following beyond Canada and extending to Japan, which made its own animated series based on the books. Crombie, son of former Toronto Mayor David Crombie, was cast in the role at 17, beating out other aspiring Canadian actors of the era, including Jason Priestly, Sullivan said. Despite his lack of acting experience, Crombie's boy-next-door looks and cool demeanor made him the perfect actor to star opposite Follows, Sullivan said. \"It was an amazing chemistry between him and Megan Follows,\" Sullivan said. \"There was a lot of affection, but they kind of grounded each other.\" The movies spawned various spinoffs, including \"Road to Avonlea,\" starring child actor Sarah Polley, and turned Anne's fictional home on Prince Edward Island into a popular tourist destination. The role made Crombie a heartthrob of his time, a sentiment expressed by many fans in the wake of his death. As one person said on Twitter, \"I don't know any female Canadian from my generation that *didn't* have at least a little bit of a crush on Jonathan Crombie as Gilbert.\" Crombie went on to play roles in other American and Canadian TV shows, including \"21 Jump Street\" and \"The Good Wife,\" but even his Facebook page acknowledges he is best known for his portrayal of Gilbert Blythe. Crombie's sister told CBC News that her brother happily answered to the name Gil when greeted by fans in public. \"I think he was really proud of being Gilbert Blythe,\" she said. \"He really enjoyed that series and was happy, very proud of it -- we all were.\" People we've lost in 2015 .\n",
      "--------------------\n",
      "\n",
      "--- Sample 2 ---\n",
      "summarize Hindi: (CNN)A popular Chinese television host known for impromptu satire is now the subject of controversy after being caught on camera cursing the late Chairman Mao Zedong. Bi Fujian, who works for state-run China Central Television, was filmed at a dinner party singing a revolutionary song that eulogizes the Communist Party's early years when he started going off script. \"The Communist Party, Chairman Mao. Don't mention that old son of a b***h. He made us suffer so bad,\" went Bi's improvised lyrics. The other dinner guests burst into laughter. Bi later apologized. \"My personal speech has led to grave social consequences, and I feel remorseful for that. I hereby sincerely apologize to the public. As a public figure, I shall learn the lesson from this incident, adhering to strict self-discipline,\" he posted on Weibo, China's Twitter-like social media platform. Making disrespectful references to China's leaders in public is considered a taboo in China, even today. And Bi's comment was directed at the man regarded by many as the country's founding father -- despite his controversial reputation. The 75-second video clip, seemingly filmed on the cellphone of another dinner guest, was uploaded on Monday. Since then, it has been removed from video-sharing sites inside China, although it was still accessible on Weibo. It's unclear when the incident occurred, or what the relationsip was between the camera person and Bi. CCTV said it would investigate. \"As a CCTV presenter, Bi Fujian's speech in the online video has led to grave social consequences,\" the network said in a statement posted on its Weibo account. CCTV did not respond to a CNN request for comment. Fondly known as \"Grandpa Bi,\" the 56-year-old TV personality was born and grew up in the Mao era. The song Bi riffed on was part of a \"red\" Peking opera that was first performed in the late 1950s. It was popularized during the Cultural Revolution of the 1960s and 1970s -- which was launched by Mao -- when China was torn apart by violence and social unrest. The video quickly divided China's online community. Critics said Bi, as an influential public figure, deserved a harsh punishment. But others rushed to his defense, arguing that Bi was simply enjoying himself in a private setting and was set up by whoever uploaded the clip. The video also emerged just a day before the new head of CCTV started his job, leading some to wonder if it were a case of \"a new broom sweeps clean.\" Mao still divides opinion in China. His giant portrait hangs on Beijing's Tiananmen Gate, and thousands flock to see his embalmed body at his mausoleum in Tiananmen Square in the heart of the Chinese capital. But despite this reverence, Mao's is a deeply flawed legacy. Many remember him as a brutal dictator who inspired fear, paranoia and famine, and whose actions resulted in tens of millions of deaths. CNN's Shen Lu contributed to this report.\n",
      "--------------------\n",
      "\n",
      "--- Sample 3 ---\n",
      "summarize English: Andy Murray has shown nerves of steel on the tennis court, but the British No 1 appeared slightly overwhelmed by the occasion during his wedding rehearsal on Friday. The former Wimbledon champion is marrying his long-term girlfriend Kim Sears in his hometown of Dunblane on Saturday and visited the cathedral to run through the service with friends, family and his fiancee. The pair have been together for 10 years after meeting at the 2005 US Open. They got back together after a brief split in 2010 before announcing their engagement last year. Andy Murray looked understandably nervous as he arrived for his wedding rehearsal . Murray arrives at Dunblane Cathedral to run through the wedding service ahead of his big day . Fiancee Kim Sears appeared to be in a rush to get the rehearsal out of the way . Murray and Sears leave the cathedral together ahead of their last night of being single . CLICK HERE to see a history of the pair's relationship through the years . Murray will forego a traditional honeymoon to fly to Barcelona after his nuptials to take a look at prospective new assistant coach Jonas Bjorkman - an appointment that has taken on greater significance since his coach Amelie Mauresmo announced she is pregnant. It is understood that the 27 year-old Scot has known about Mauresmo' s pregnancy for a number of weeks, but he was anyway planning on linking up with Bjorkman – the former world singles No 4 and doubles No 1 – prior to that. As Sportsmail reported as far back as nearly a year ago, the Swede was on Murray's shortlist to replace Ivan Lendl before he settled on the former Wimbledon women's champion as part of a groundbreaking move. Murray looked typically stony faced before waiting to get entry into the cathedral (right) The 27-year-old manages to crack a smile as he and Sears move into the cathedral . The pair will be married in Murray's hometown Dunblane Cathedral on Saturday April 11 . Local businesses were also getting ready to join in the celebrations of their returning hero . Homemade bunting lines the streets of Dunblane ahead of the special ocassion . Even a local hairdressers has its windows decorated with messages of love . Murray has a golden postbox in his hometown after winning gold at the 2012 London Olympics .\n",
      "--------------------\n",
      "\n",
      "--- Sample 4 ---\n",
      "summarize Hindi: Five-month-old Elijah is 'the happiest baby ever', according to his mother. He loves everything that moves, especially seeing people and exploring new places. But doctors say he probably won't live to see his second birthday because of a fatal genetic illness. His heartbroken mum Jessica and dad Andrew McCrae have pledged to do everything they can to make sure the little boy from Penrith, in Sydney's west, lives a full life, compiling a 30-item bucket list for their son to complete before he passes away. From a road trip to Queensland to watching the sunrise and set with him, Jessica told Daily Mail Australia the family are planning to show Elijah as much of the world as they can in his final days. Little Elijah suffers the fatal genetic disease Type 1 Spinal Muscular Atrophy (SMA). Born strong, he's now 'very floppy', and doctors say he will probably not survive his second birthday . ‘His sweet face will always know how to bring a smile to my face, even long after he has left us,' his mum Jessica McCrae said . First item on the bucket list: LIttle Ellijah got to visit the Sydney Royal Easter Show at the weekend and meet all of the animals . Fun times: Dad Andrew cradles his little boy . 'Just because he can't move his body doesn't mean he can't enjoy life any other way, like the rest of us can,' his mum told Daily Mail Australia. Little Elijah suffers from Type 1 Spinal Muscular Atrophy (SMA). He gets weaker as he grows older. Born strong, he's now 'very floppy'. His parents were devastated when he was diagnosed. They are refusing to wallow in their sadness. 'You can sit there and be depressed about it, or say, \"he's here for two years, we've got to make the most of it,' Jessica said. The parents were motivated to start the list - which includes visiting Sydney's aquarium and Opera House - by their love for their son, and heartbreak at seeing other happy families having fun with their grown children. 'I thought, \"let's make a bucket list. 'I've got my own - like travelling the world - but (Elijah) won't ever be able to do things like that. 'So why don't we do things we would do with a child when we're older?' The family have already ticked one item off the line-up. They visited the Sydney Royal Easter Show at the weekend. It's the Easter Show! Dad Andrew and mum Jessica are pictured with their baby boy out the front of all the rides at the Easter Show last weekend . Look at those goats! Little Elijah is pictured with some of his furry friends. His parents are hoping to take him to Sydney Wildlife World and Featherdale Wildlife Park as well . 'We have to enjoy it now. We don't get to see him enjoy these things when he's like 10, we have to do it now' Little Elijah was fascinated by all the 'shiny stores' and loved looking at the cakes, his mum said. And in the coming weeks, they are hoping to head out on a road trip to Queensland, stopping along the way at various sights, including Byron Bay, the theme parks on the Gold Coast and The Entrance, on the Central Coast of New South Wales. 'The hardest thing is seeing other families, especially when you go out and see these families having fun,' Jessica said. 'We have to enjoy it now. We don't get to see him enjoy these things when he's like 10, we have to do it now.' 'The hardest thing is seeing other families, especially when you go out and see these families having fun,' Jessica said . Elijah was born very strong but has lost a lot of muscle because of his condition. ''It breaks you, to see your baby get so weak,' his mum said . The family have made a fundraising page in the hope they can raise the money to complete the list together. But the item they want the most is to celebrate Elijah's first birthday with a big party. 'I want him to celebrate his 21st birthday,' she wrote on Facebook, 'but I'm being realistic'. There is hope that Elijah may survive longer than the doctors have predicted. Elijah has been approved to go into a clinical trial which will hopefully prolong the little boy’s life. 1. Trip to New Zealand . 2. The Gold Coast . 3. Experience Movie World . 4. See Sea World . 5. Visit Australia Zoo . 6. Go to Wollongong.. 7. An adventure to Forster . 8. See Palm Beach . 9. Take a peek at The Entrance . 10. Go to Coffs Harbour . 11. Swing by Byron Bay . 12. Attend the Sydney Aquarium . 13. See the creatures at Sydney Wildlife Park . 14. Pat the animals at Featherdale Wildlife Park . 15. Paddle at Manly/Bondi Beach . 16. See the Opera House . 17. Visit Darling Harbour . 18. Be dazzled by the Vivid Light Festival . 19. Take him on a ferry . 20. Watch the sunset and sunrise . 21. Visit The Three Sisters/Blue Mountains . 22. Go to Mt Tomah ('a taste of Scotland') 23. Attend the Bundanoon Highland Gathering . 24. Have a family photoshoot . 25. Celebrate his 6th months on Earth with a party . 26. Dedicate him to God . 26. Host a 1st birthday party . 27. Go to the Easter Show (done) 28. Head to a museum . 29. Visit the look-a-likes at Madame Tussauds . Of two groups in the trial, one will receive a drug, while the other must undergo an invasive ‘sham surgery’. ‘Praying hard he will get the drug and not the sham,’ his mother wrote on Facebook. The treatment Elijah will receive will be randomly selected by a computer. No one, bar a few selected people, will know as his mother is not allowed to accompany her baby during the procedure. ‘Words cannot describe the anxiety, the emotions I am feeling. The first wish listed on Elijah's bucket list is to visit New Zealand with his parents (above, Paparoa National Park on New Zealand's South Island) Number three on Elijah's bucket list is to see Queensland's iconic family theme park Sea World . Visiting Australia Zoo is number five on Elijah's bucket list, where one of the highlights is visiting the koalas . 'It's all hitting me harder than ever now. Especially because if he gets the drug it might prolong his life. ‘If not, then we will lose him soon. It kind of feels like it is his \"d-day\". ''It breaks you, to see your baby get so weak.' But she knows Elijah will always be with them. Jessica wrote on Facebook: '(His) sweet face will always know how to bring a smile to my face, even long after he has left us.' Seeing the Opera House is number 16 on Elijah's bucket list, a landmark made famous by it's white sails .\n",
      "--------------------\n",
      "\n",
      "--- Sample 5 ---\n",
      "summarize Hindi: Ben Hagon's (pictured) high-powered Mercedes sports car crashed into Tara McIntyre leaving her with life-changing injuries . A driver  destroyed a young woman's life when he smashed into her car after taking a lethal cocktail of drink and prescription drugs has been jailed. Tara McIntyre, 24, was left virtually wheelchair-bound after Ben Hagon's high-powered Mercedes sports car crashed into her at up to 75mph, a court heard. She had just popped out to the shops in her small KA when she was hit by the drunk-driver and suffered the life-changing injuries which included a fractured spine and pelvis. The court heard that Hagon, who already had a conviction for drink-driving in 2003, had taken prescription tranquilisers and was more than twice the legal alcohol limit when got behind the wheel. Judge David Turner branded Hagon's driving 'madness' - and accused him of changing Miss McIntyre's life for the worse for ever. Sentencing him to two years and eight months in prison, Judge Turner said: 'Miss McIntyre went on what ought to have been a simple shopping trip and has been left with injuries which are literally life changing. 'She will be dependent on others for help for the rest of her life. The medical consequences have been utterly traumatic.' 'This personal catastrophe has been brought about fully by those moments of madness in your driving. 'I accept you feel considerable remorse. This is frankly as bad a piece of driving in combination one can imagine.' Hagon was overtaking a line of cars on the A131 at High Garrett, near Braintree, Essex (pictured) when he ploughed into the side of Miss McIntyre's Ford Ka . Chelmsford Crown Court heard that Hagon was overtaking a line of cars on the A131 at High Garrett, near Braintree, Essex, when he ploughed into the side of Miss McIntyre's Ford Ka as she waited to turn right. One eye-witness estimated he was hitting 75mph in the 40mph stretch of road in his 54-plate Mercedes SLK. But others said his speed was closer to 50-60mph before the horrifying smash  at about 2.15pm on February 2 last year. Police crash investigators found that Hagon had made no attempt to brake. Miss McIntyre was rushed to Addenbrooke's Hospital, Cambridge, by air ambulance suffering head injuries, a fractured spine and pelvis, and a cut to her artery. She spent the next three months in hospital, including some time in an induced coma, and a further two months in a special rehabilitation facility. Miss McIntyre was rushed to Addenbrooke's Hospital after the crash with terrible head injuries, a fractured spine and pelvis . The formerly independent young woman now needs help with basic day-to-day tasks and has been told she will spend most of her life in a wheelchair as she is only able to walk just 60 yards with a stick. She was also left partially sighted and had to learn to speak again after brain damage caused by the crash. Miss McIntyre's boyfriend, who was sitting beside her in the front seat, amazingly escaped with minor injuries, as did Hagon. Jane Oldfield, prosecuting, said Hagon had been banned for drink-driving in October 2003. She also said Miss McIntyre's devastated parents were now having to move house as their home could not be adapted sufficiently to care for their seriously-disabled daughter. Hagon, of Halstead, Essex, had been out drinking the night before and downed Champagne, three pints of beer and a jug of cocktail, the court heard. A blood test calculated he had 161 milligrammes of alcohol in 100 millilitres of blood- double the legal limit. Hagon admitted drink-driving and causing serious injury by dangerous driving and was jailed for two years eight months at Chelmsford Crown Court (pictured) He had also swallowed prescription tranquillisers as part of treatment he's having for anxiety. The court heard he had blatantly ignored advice not to drive while taking the powerful drugs after being warned that their effects would be worsened when combined with alcohol. Rakesh Sharma, defending, said Hagon was now seeking help for alcohol addiction and did not intend to ever drive again. Mr Sharma said: 'He thinks about the victim every day and can't believe what he has done.' Hagon admitted drink-driving and causing serious injury by dangerous driving and was jailed for two years and eight months. He was also banned from driving for five years. He will have take an extended re-test if he does want to drive again. No award was made for compensation or costs due to the prison term, but Hagon must pay a victim surcharge.\n",
      "--------------------\n",
      "\n",
      "--- Sample 6 ---\n",
      "summarize Hindi: Utrecht has twelve players on the pitch at one stage during Sunday's 1-1 draw with Ajax at the Stadion Galgenwaard. No, the home side weren't cheating but instead Utrecht winger Edouard Duplan had no idea he had been substituted midway through the first-half. The referee blew on 24 minutes amid confusion Utrecht had an extra man on the pitch and marched over to Duplan. Edouard Duplan is spoken to by the referee as Utrecht appear to have twelve players on the pitch against Ajax . The referee points at substitute Tommy Oar on the field of the play and Duplan appears to realise his mistake . Duplan makes his way off the pitch and hops over the advertising hoarding to the Utrecht bench on Sunday . Duplan sees the funny side of mishap and laughs with the bench during Utrecht's 1-1 draw with Ajax . The Frenchman protested but suddenly seemed to realise substitute Tommy Oar was now on the pitch and soon made his departure. With a hop over the advertising hoardings, Duplan saw the funny side and attempted to explain the mix-up to his team-mates on the bench. Having briefly had twelve players on the pitch during the first half, Utrecht ended the game with 10, following Ramon Leeuwin's sending off in the 87th minute. Leeuwin didn't have the best of afternoon's after putting the ball into his own net to give Ajax the lead before Gevero Markiet equalised for Utrecht with five minutes to go. Australia international Tommy Oar replaced  team-mate Duplan in the 24th minute of the Eredivisie clash .\n",
      "--------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "import pandas as pd\n",
    "\n",
    "# Load data using pandas and remove any empty rows\n",
    "df = pd.read_csv('../Dataset/final_cleaned_dataset_CNN.csv').dropna().reset_index(drop=True)\n",
    "raw_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# Define the task prefixes for the model\n",
    "PREFIX_ENG = \"summarize English: \"\n",
    "PREFIX_HIN = \"summarize Hindi: \"\n",
    "\n",
    "def format_dataset(batch):\n",
    "    \"\"\"Transforms a batch of data for the two summarization tasks.\"\"\"\n",
    "    inputs, targets = [], []\n",
    "    \n",
    "    # Create alternating examples for English and Hindi summarization\n",
    "    for article, eng_summary, hin_summary in zip(\n",
    "        batch['raw_news_article'],\n",
    "        batch['english_summary'],\n",
    "        batch['hindi_summary']\n",
    "    ):\n",
    "        if isinstance(article, str):\n",
    "            # English Task\n",
    "            inputs.append(PREFIX_ENG + article)\n",
    "            targets.append(eng_summary)\n",
    "            \n",
    "            # Hindi Task\n",
    "            inputs.append(PREFIX_HIN + article)\n",
    "            targets.append(hin_summary)\n",
    "            \n",
    "    return {'inputs': inputs, 'targets': targets}\n",
    "\n",
    "# Apply the formatting and create train/test splits\n",
    "processed_dataset = raw_dataset.map(\n",
    "    format_dataset,\n",
    "    batched=True,\n",
    "    remove_columns=raw_dataset.column_names\n",
    ").flatten()\n",
    "\n",
    "train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "\n",
    "final_datasets = DatasetDict({\n",
    "    'train': train_test_split['train'],\n",
    "    'test': train_test_split['test']\n",
    "})\n",
    "\n",
    "# --- NEW AND IMPROVED VERIFICATION ---\n",
    "# Loop through the first 6 samples of the shuffled training set to see the mix of tasks.\n",
    "print(\"--- Verifying the first 6 samples of the SHUFFLED training data ---\")\n",
    "for i in range(6):\n",
    "    print(f\"\\n--- Sample {i+1} ---\")\n",
    "    print(final_datasets['train'][i]['inputs'])\n",
    "    print(\"--------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a87c73a",
   "metadata": {},
   "source": [
    "Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a00123a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Map:   0%|          | 0/8854 [00:00<?, ? examples/s]c:\\Users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 8854/8854 [00:32<00:00, 270.83 examples/s]\n",
      "Map: 100%|██████████| 984/984 [00:03<00:00, 276.40 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import MT5Tokenizer\n",
    "\n",
    "MODEL_CHECKPOINT = \"google/mt5-base\"\n",
    "tokenizer = MT5Tokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "\n",
    "MAX_INPUT_LENGTH = 1024\n",
    "MAX_TARGET_LENGTH = 256\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Converts text inputs and targets into token IDs.\"\"\"\n",
    "    model_inputs = tokenizer(\n",
    "        examples['inputs'], \n",
    "        max_length=MAX_INPUT_LENGTH, \n",
    "        truncation=True, \n",
    "    )\n",
    "    \n",
    "    # Ensures labels are tokenized correctly for seq2seq tasks\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(\n",
    "            examples['targets'], \n",
    "            max_length=MAX_TARGET_LENGTH, \n",
    "            truncation=True,\n",
    "        )\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized_datasets = final_datasets.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aae3266",
   "metadata": {},
   "source": [
    "Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc17cb51",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "  1%|          | 50/6642 [01:16<2:44:49,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 18.8898, 'grad_norm': 25843.544921875, 'learning_rate': 5e-06, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 100/6642 [02:53<5:24:53,  2.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 17.3691, 'grad_norm': 24710.8671875, 'learning_rate': 1e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 150/6642 [04:29<2:45:05,  1.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 15.1896, 'grad_norm': 1253.855712890625, 'learning_rate': 1.5e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 200/6642 [05:47<2:38:21,  1.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 11.4705, 'grad_norm': 1275.7470703125, 'learning_rate': 2e-05, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 250/6642 [06:58<2:38:23,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 7.257, 'grad_norm': 19.44481086730957, 'learning_rate': 2.5e-05, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 300/6642 [07:58<2:12:24,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.3326, 'grad_norm': 11.529706954956055, 'learning_rate': 3e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 350/6642 [09:01<1:42:27,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.6727, 'grad_norm': 3.133930206298828, 'learning_rate': 3.5e-05, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 400/6642 [10:18<2:50:58,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.3826, 'grad_norm': 3.2587127685546875, 'learning_rate': 4e-05, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 450/6642 [11:22<2:01:21,  1.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.4258, 'grad_norm': 2.94587779045105, 'learning_rate': 4.5e-05, 'epoch': 0.2}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 500/6642 [12:22<2:09:01,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.2653, 'grad_norm': 2.916131019592285, 'learning_rate': 5e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 550/6642 [13:28<1:49:31,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1574, 'grad_norm': 2.276667833328247, 'learning_rate': 4.959296646043634e-05, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 600/6642 [14:29<1:44:05,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.1425, 'grad_norm': 3.2969565391540527, 'learning_rate': 4.9185932920872686e-05, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 650/6642 [15:53<2:37:25,  1.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0699, 'grad_norm': 2.586760997772217, 'learning_rate': 4.8778899381309024e-05, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 700/6642 [17:06<1:37:12,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.0012, 'grad_norm': 2.9604198932647705, 'learning_rate': 4.837186584174536e-05, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 750/6642 [17:56<1:43:36,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9563, 'grad_norm': 2.398463487625122, 'learning_rate': 4.79648323021817e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 800/6642 [18:55<2:05:23,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.9003, 'grad_norm': 2.998619794845581, 'learning_rate': 4.7557798762618045e-05, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 850/6642 [19:57<2:11:00,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.909, 'grad_norm': 2.534437894821167, 'learning_rate': 4.7150765223054384e-05, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▎        | 900/6642 [20:47<1:21:09,  1.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.96, 'grad_norm': 2.140286445617676, 'learning_rate': 4.674373168349072e-05, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 950/6642 [21:40<1:24:51,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.8684, 'grad_norm': 2.402892589569092, 'learning_rate': 4.633669814392706e-05, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 1000/6642 [22:33<1:50:25,  1.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.877, 'grad_norm': 2.3655755519866943, 'learning_rate': 4.5929664604363405e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 1050/6642 [23:35<2:05:17,  1.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7866, 'grad_norm': 3.0120809078216553, 'learning_rate': 4.552263106479974e-05, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1100/6642 [24:38<1:59:18,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.799, 'grad_norm': 2.3288731575012207, 'learning_rate': 4.511559752523608e-05, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 1150/6642 [25:45<1:56:08,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.761, 'grad_norm': 2.0763607025146484, 'learning_rate': 4.470856398567242e-05, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 1200/6642 [26:41<2:15:02,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.787, 'grad_norm': 2.4499988555908203, 'learning_rate': 4.4301530446108765e-05, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 1250/6642 [27:35<1:22:43,  1.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7463, 'grad_norm': 2.4600257873535156, 'learning_rate': 4.38944969065451e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 1300/6642 [28:34<1:38:50,  1.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7063, 'grad_norm': 2.149730682373047, 'learning_rate': 4.348746336698144e-05, 'epoch': 0.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1350/6642 [29:34<1:45:07,  1.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7094, 'grad_norm': 2.884317636489868, 'learning_rate': 4.308042982741778e-05, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 1400/6642 [30:25<1:17:47,  1.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.707, 'grad_norm': 2.2151288986206055, 'learning_rate': 4.2673396287854124e-05, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 1450/6642 [31:24<2:26:38,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6963, 'grad_norm': 2.1696739196777344, 'learning_rate': 4.226636274829046e-05, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 1500/6642 [32:34<1:37:04,  1.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.7001, 'grad_norm': 1.9839057922363281, 'learning_rate': 4.18593292087268e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 1550/6642 [33:36<2:00:39,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6753, 'grad_norm': 2.3683249950408936, 'learning_rate': 4.1452295669163146e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 1600/6642 [34:48<1:34:02,  1.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6208, 'grad_norm': 2.190981388092041, 'learning_rate': 4.1045262129599484e-05, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 1650/6642 [35:41<1:39:47,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6683, 'grad_norm': 2.2084054946899414, 'learning_rate': 4.063822859003582e-05, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 1700/6642 [37:04<2:15:07,  1.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6425, 'grad_norm': 2.8109753131866455, 'learning_rate': 4.023119505047216e-05, 'epoch': 0.77}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▋       | 1750/6642 [38:15<2:01:18,  1.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6371, 'grad_norm': 2.0437192916870117, 'learning_rate': 3.9824161510908506e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 1800/6642 [39:34<1:33:57,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.61, 'grad_norm': 2.209165096282959, 'learning_rate': 3.9417127971344844e-05, 'epoch': 0.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 1850/6642 [40:32<1:41:25,  1.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.6236, 'grad_norm': 2.6539177894592285, 'learning_rate': 3.901009443178118e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 1900/6642 [41:26<1:58:56,  1.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5873, 'grad_norm': 2.196930408477783, 'learning_rate': 3.860306089221752e-05, 'epoch': 0.86}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▉       | 1950/6642 [42:20<1:11:22,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.575, 'grad_norm': 2.296056032180786, 'learning_rate': 3.8196027352653865e-05, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 2000/6642 [43:22<1:29:03,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5951, 'grad_norm': 2.392451524734497, 'learning_rate': 3.7788993813090204e-05, 'epoch': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 2050/6642 [44:13<1:01:55,  1.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5612, 'grad_norm': 2.19561505317688, 'learning_rate': 3.738196027352654e-05, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 2100/6642 [45:48<2:25:21,  1.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5424, 'grad_norm': 2.6735122203826904, 'learning_rate': 3.697492673396288e-05, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 2150/6642 [47:21<1:31:32,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5664, 'grad_norm': 2.106947898864746, 'learning_rate': 3.6567893194399225e-05, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2200/6642 [49:05<2:28:31,  2.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5255, 'grad_norm': 2.2351150512695312, 'learning_rate': 3.616085965483556e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 2214/6642 [49:28<1:53:43,  1.54s/it]c:\\Users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n",
      "                                                     \n",
      " 33%|███▎      | 2214/6642 [53:05<1:53:43,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9671177864074707, 'eval_rouge1': 10.9929, 'eval_rouge2': 4.2532, 'eval_rougeL': 9.6798, 'eval_rougeLsum': 11.0107, 'eval_runtime': 217.3446, 'eval_samples_per_second': 4.527, 'eval_steps_per_second': 1.132, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 2250/6642 [55:44<1:20:17,  1.10s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5857, 'grad_norm': 2.328514337539673, 'learning_rate': 3.57538261152719e-05, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 2300/6642 [56:57<1:38:11,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.4735, 'grad_norm': 2.3959734439849854, 'learning_rate': 3.534679257570824e-05, 'epoch': 1.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 2350/6642 [57:59<1:47:00,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5136, 'grad_norm': 2.202885866165161, 'learning_rate': 3.4939759036144585e-05, 'epoch': 1.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 2400/6642 [59:09<1:14:42,  1.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.5112, 'grad_norm': 2.123568296432495, 'learning_rate': 3.453272549658092e-05, 'epoch': 1.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 2403/6642 [59:12<1:12:46,  1.03s/it]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 60\u001b[0m\n\u001b[0;32m     49\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m     50\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     51\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m     57\u001b[0m )\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/final_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages\\transformers\\trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages\\transformers\\trainer.py:2216\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2213\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[0;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2222\u001b[0m ):\n\u001b[0;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages\\transformers\\trainer.py:3241\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   3238\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs)\n\u001b[0;32m   3240\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[1;32m-> 3241\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mn_gpu \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3244\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mmean()  \u001b[38;5;66;03m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages\\torch\\cuda\\memory.py:192\u001b[0m, in \u001b[0;36mempty_cache\u001b[1;34m()\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m`nvidia-smi`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;124;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[1;32m--> 192\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    MT5ForConditionalGeneration, \n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "model = MT5ForConditionalGeneration.from_pretrained(MODEL_CHECKPOINT)\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "MODEL_NAME = \"mt5-base-cnn-summarizer-en-hi\"\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=MODEL_NAME,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, # <--- THIS IS THE FIX\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge2\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Calculates ROUGE scores for evaluation.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    decoded_preds = [\"\\n\".join(pred.strip().split()) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(label.strip().split()) for label in decoded_labels]\n",
    "    \n",
    "    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    \n",
    "    return {k: round(v * 100, 4) for k, v in result.items()}\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "trainer.save_model(f\"{MODEL_NAME}/final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "76112239",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b404c5bbde424bdf9a5201cafbadfea5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6642 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 18.8898, 'grad_norm': 25843.544921875, 'learning_rate': 5e-06, 'epoch': 0.02}\n",
      "{'loss': 17.3691, 'grad_norm': 24710.8671875, 'learning_rate': 1e-05, 'epoch': 0.05}\n",
      "{'loss': 15.1896, 'grad_norm': 1253.855712890625, 'learning_rate': 1.5e-05, 'epoch': 0.07}\n",
      "{'loss': 11.4705, 'grad_norm': 1275.7470703125, 'learning_rate': 2e-05, 'epoch': 0.09}\n",
      "{'loss': 7.257, 'grad_norm': 19.44481086730957, 'learning_rate': 2.5e-05, 'epoch': 0.11}\n",
      "{'loss': 4.3326, 'grad_norm': 11.529706954956055, 'learning_rate': 3e-05, 'epoch': 0.14}\n",
      "{'loss': 3.6727, 'grad_norm': 3.133930206298828, 'learning_rate': 3.5e-05, 'epoch': 0.16}\n",
      "{'loss': 3.3826, 'grad_norm': 3.2587127685546875, 'learning_rate': 4e-05, 'epoch': 0.18}\n",
      "{'loss': 3.4258, 'grad_norm': 2.94587779045105, 'learning_rate': 4.5e-05, 'epoch': 0.2}\n",
      "{'loss': 3.2653, 'grad_norm': 2.916131019592285, 'learning_rate': 5e-05, 'epoch': 0.23}\n",
      "{'loss': 3.1574, 'grad_norm': 2.276667833328247, 'learning_rate': 4.959296646043634e-05, 'epoch': 0.25}\n",
      "{'loss': 3.1425, 'grad_norm': 3.2969565391540527, 'learning_rate': 4.9185932920872686e-05, 'epoch': 0.27}\n",
      "{'loss': 3.0699, 'grad_norm': 2.586760997772217, 'learning_rate': 4.8778899381309024e-05, 'epoch': 0.29}\n",
      "{'loss': 3.0012, 'grad_norm': 2.9604198932647705, 'learning_rate': 4.837186584174536e-05, 'epoch': 0.32}\n",
      "{'loss': 2.9563, 'grad_norm': 2.398463487625122, 'learning_rate': 4.79648323021817e-05, 'epoch': 0.34}\n",
      "{'loss': 2.9003, 'grad_norm': 2.998619794845581, 'learning_rate': 4.7557798762618045e-05, 'epoch': 0.36}\n",
      "{'loss': 2.909, 'grad_norm': 2.534437894821167, 'learning_rate': 4.7150765223054384e-05, 'epoch': 0.38}\n",
      "{'loss': 2.96, 'grad_norm': 2.140286445617676, 'learning_rate': 4.674373168349072e-05, 'epoch': 0.41}\n",
      "{'loss': 2.8684, 'grad_norm': 2.402892589569092, 'learning_rate': 4.633669814392706e-05, 'epoch': 0.43}\n",
      "{'loss': 2.877, 'grad_norm': 2.3655755519866943, 'learning_rate': 4.5929664604363405e-05, 'epoch': 0.45}\n",
      "{'loss': 2.7866, 'grad_norm': 3.0120809078216553, 'learning_rate': 4.552263106479974e-05, 'epoch': 0.47}\n",
      "{'loss': 2.799, 'grad_norm': 2.3288731575012207, 'learning_rate': 4.511559752523608e-05, 'epoch': 0.5}\n",
      "{'loss': 2.761, 'grad_norm': 2.0763607025146484, 'learning_rate': 4.470856398567242e-05, 'epoch': 0.52}\n",
      "{'loss': 2.787, 'grad_norm': 2.4499988555908203, 'learning_rate': 4.4301530446108765e-05, 'epoch': 0.54}\n",
      "{'loss': 2.7463, 'grad_norm': 2.4600257873535156, 'learning_rate': 4.38944969065451e-05, 'epoch': 0.56}\n",
      "{'loss': 2.7063, 'grad_norm': 2.149730682373047, 'learning_rate': 4.348746336698144e-05, 'epoch': 0.59}\n",
      "{'loss': 2.7094, 'grad_norm': 2.884317636489868, 'learning_rate': 4.308042982741778e-05, 'epoch': 0.61}\n",
      "{'loss': 2.707, 'grad_norm': 2.2151288986206055, 'learning_rate': 4.2673396287854124e-05, 'epoch': 0.63}\n",
      "{'loss': 2.6963, 'grad_norm': 2.1696739196777344, 'learning_rate': 4.226636274829046e-05, 'epoch': 0.65}\n",
      "{'loss': 2.7001, 'grad_norm': 1.9839057922363281, 'learning_rate': 4.18593292087268e-05, 'epoch': 0.68}\n",
      "{'loss': 2.6753, 'grad_norm': 2.3683249950408936, 'learning_rate': 4.1452295669163146e-05, 'epoch': 0.7}\n",
      "{'loss': 2.6208, 'grad_norm': 2.190981388092041, 'learning_rate': 4.1045262129599484e-05, 'epoch': 0.72}\n",
      "{'loss': 2.6683, 'grad_norm': 2.2084054946899414, 'learning_rate': 4.063822859003582e-05, 'epoch': 0.75}\n",
      "{'loss': 2.6425, 'grad_norm': 2.8109753131866455, 'learning_rate': 4.023119505047216e-05, 'epoch': 0.77}\n",
      "{'loss': 2.6371, 'grad_norm': 2.0437192916870117, 'learning_rate': 3.9824161510908506e-05, 'epoch': 0.79}\n",
      "{'loss': 2.61, 'grad_norm': 2.209165096282959, 'learning_rate': 3.9417127971344844e-05, 'epoch': 0.81}\n",
      "{'loss': 2.6236, 'grad_norm': 2.6539177894592285, 'learning_rate': 3.901009443178118e-05, 'epoch': 0.84}\n",
      "{'loss': 2.5873, 'grad_norm': 2.196930408477783, 'learning_rate': 3.860306089221752e-05, 'epoch': 0.86}\n",
      "{'loss': 2.575, 'grad_norm': 2.296056032180786, 'learning_rate': 3.8196027352653865e-05, 'epoch': 0.88}\n",
      "{'loss': 2.5951, 'grad_norm': 2.392451524734497, 'learning_rate': 3.7788993813090204e-05, 'epoch': 0.9}\n",
      "{'loss': 2.5612, 'grad_norm': 2.19561505317688, 'learning_rate': 3.738196027352654e-05, 'epoch': 0.93}\n",
      "{'loss': 2.5424, 'grad_norm': 2.6735122203826904, 'learning_rate': 3.697492673396288e-05, 'epoch': 0.95}\n",
      "{'loss': 2.5664, 'grad_norm': 2.106947898864746, 'learning_rate': 3.6567893194399225e-05, 'epoch': 0.97}\n",
      "{'loss': 2.5255, 'grad_norm': 2.2351150512695312, 'learning_rate': 3.616085965483556e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a373a6434a2d428087551adc15afc5f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/246 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 07:33:32,718 [INFO] - Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.9671177864074707, 'eval_rouge1': 11.0001, 'eval_rouge2': 4.2691, 'eval_rougeL': 9.6964, 'eval_rougeLsum': 10.9729, 'eval_runtime': 211.7028, 'eval_samples_per_second': 4.648, 'eval_steps_per_second': 1.162, 'epoch': 1.0}\n",
      "{'loss': 2.5857, 'grad_norm': 2.328514337539673, 'learning_rate': 3.57538261152719e-05, 'epoch': 1.02}\n",
      "{'loss': 2.4735, 'grad_norm': 2.3959734439849854, 'learning_rate': 3.534679257570824e-05, 'epoch': 1.04}\n",
      "{'loss': 2.5136, 'grad_norm': 2.202885866165161, 'learning_rate': 3.4939759036144585e-05, 'epoch': 1.06}\n",
      "{'loss': 2.5112, 'grad_norm': 2.123568296432495, 'learning_rate': 3.453272549658092e-05, 'epoch': 1.08}\n",
      "{'loss': 2.4621, 'grad_norm': 2.071408271789551, 'learning_rate': 3.412569195701726e-05, 'epoch': 1.11}\n",
      "{'loss': 2.4829, 'grad_norm': 1.851739525794983, 'learning_rate': 3.37186584174536e-05, 'epoch': 1.13}\n",
      "{'loss': 2.4779, 'grad_norm': 2.233159065246582, 'learning_rate': 3.331162487788994e-05, 'epoch': 1.15}\n",
      "{'loss': 2.4767, 'grad_norm': 2.1406004428863525, 'learning_rate': 3.290459133832628e-05, 'epoch': 1.17}\n",
      "{'loss': 2.4986, 'grad_norm': 2.0391101837158203, 'learning_rate': 3.249755779876262e-05, 'epoch': 1.2}\n",
      "{'loss': 2.4718, 'grad_norm': 2.272228240966797, 'learning_rate': 3.209052425919896e-05, 'epoch': 1.22}\n",
      "{'loss': 2.4791, 'grad_norm': 2.4773452281951904, 'learning_rate': 3.16834907196353e-05, 'epoch': 1.24}\n",
      "{'loss': 2.4839, 'grad_norm': 2.3240935802459717, 'learning_rate': 3.1276457180071636e-05, 'epoch': 1.26}\n",
      "{'loss': 2.4922, 'grad_norm': 2.343090295791626, 'learning_rate': 3.0869423640507974e-05, 'epoch': 1.29}\n",
      "{'loss': 2.46, 'grad_norm': 2.3500118255615234, 'learning_rate': 3.0462390100944322e-05, 'epoch': 1.31}\n",
      "{'loss': 2.4549, 'grad_norm': 2.4027912616729736, 'learning_rate': 3.005535656138066e-05, 'epoch': 1.33}\n",
      "{'loss': 2.3833, 'grad_norm': 2.2222204208374023, 'learning_rate': 2.9648323021817e-05, 'epoch': 1.36}\n",
      "{'loss': 2.4444, 'grad_norm': 2.89462947845459, 'learning_rate': 2.9241289482253337e-05, 'epoch': 1.38}\n",
      "{'loss': 2.4434, 'grad_norm': 2.0932962894439697, 'learning_rate': 2.8834255942689682e-05, 'epoch': 1.4}\n",
      "{'loss': 2.4231, 'grad_norm': 2.2439112663269043, 'learning_rate': 2.842722240312602e-05, 'epoch': 1.42}\n",
      "{'loss': 2.413, 'grad_norm': 2.211310625076294, 'learning_rate': 2.802018886356236e-05, 'epoch': 1.45}\n",
      "{'loss': 2.3843, 'grad_norm': 2.6021902561187744, 'learning_rate': 2.7613155323998697e-05, 'epoch': 1.47}\n",
      "{'loss': 2.409, 'grad_norm': 2.6927690505981445, 'learning_rate': 2.7206121784435042e-05, 'epoch': 1.49}\n",
      "{'loss': 2.4084, 'grad_norm': 2.357555866241455, 'learning_rate': 2.679908824487138e-05, 'epoch': 1.51}\n",
      "{'loss': 2.381, 'grad_norm': 2.321312427520752, 'learning_rate': 2.6392054705307718e-05, 'epoch': 1.54}\n",
      "{'loss': 2.3776, 'grad_norm': 2.317113161087036, 'learning_rate': 2.598502116574406e-05, 'epoch': 1.56}\n",
      "{'loss': 2.4128, 'grad_norm': 2.9134864807128906, 'learning_rate': 2.5577987626180398e-05, 'epoch': 1.58}\n",
      "{'loss': 2.4025, 'grad_norm': 2.422832489013672, 'learning_rate': 2.517095408661674e-05, 'epoch': 1.6}\n",
      "{'loss': 2.3955, 'grad_norm': 2.417001247406006, 'learning_rate': 2.4763920547053078e-05, 'epoch': 1.63}\n",
      "{'loss': 2.3747, 'grad_norm': 2.398787260055542, 'learning_rate': 2.435688700748942e-05, 'epoch': 1.65}\n",
      "{'loss': 2.395, 'grad_norm': 2.0513436794281006, 'learning_rate': 2.3949853467925758e-05, 'epoch': 1.67}\n",
      "{'loss': 2.4453, 'grad_norm': 2.0952892303466797, 'learning_rate': 2.3542819928362096e-05, 'epoch': 1.69}\n",
      "{'loss': 2.4022, 'grad_norm': 2.0036370754241943, 'learning_rate': 2.3135786388798438e-05, 'epoch': 1.72}\n",
      "{'loss': 2.4027, 'grad_norm': 2.1861188411712646, 'learning_rate': 2.2728752849234776e-05, 'epoch': 1.74}\n",
      "{'loss': 2.3968, 'grad_norm': 2.1474661827087402, 'learning_rate': 2.2321719309671117e-05, 'epoch': 1.76}\n",
      "{'loss': 2.3598, 'grad_norm': 2.0651872158050537, 'learning_rate': 2.1914685770107456e-05, 'epoch': 1.78}\n",
      "{'loss': 2.3375, 'grad_norm': 2.3957748413085938, 'learning_rate': 2.1507652230543797e-05, 'epoch': 1.81}\n",
      "{'loss': 2.3777, 'grad_norm': 2.6203742027282715, 'learning_rate': 2.1100618690980136e-05, 'epoch': 1.83}\n",
      "{'loss': 2.3472, 'grad_norm': 2.0378875732421875, 'learning_rate': 2.0693585151416477e-05, 'epoch': 1.85}\n",
      "{'loss': 2.3554, 'grad_norm': 2.2028231620788574, 'learning_rate': 2.0286551611852815e-05, 'epoch': 1.87}\n",
      "{'loss': 2.3837, 'grad_norm': 2.0698325634002686, 'learning_rate': 1.9879518072289157e-05, 'epoch': 1.9}\n",
      "{'loss': 2.3409, 'grad_norm': 2.291039228439331, 'learning_rate': 1.9472484532725495e-05, 'epoch': 1.92}\n",
      "{'loss': 2.4185, 'grad_norm': 2.2808704376220703, 'learning_rate': 1.9065450993161837e-05, 'epoch': 1.94}\n",
      "{'loss': 2.3665, 'grad_norm': 1.9720441102981567, 'learning_rate': 1.865841745359818e-05, 'epoch': 1.96}\n",
      "{'loss': 2.3696, 'grad_norm': 2.2738401889801025, 'learning_rate': 1.8251383914034517e-05, 'epoch': 1.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "815a677c538f4b7da327d6e2af69d871",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/246 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 08:32:07,485 [INFO] - Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8547613620758057, 'eval_rouge1': 10.9926, 'eval_rouge2': 4.1325, 'eval_rougeL': 9.5914, 'eval_rougeLsum': 10.9835, 'eval_runtime': 204.6945, 'eval_samples_per_second': 4.807, 'eval_steps_per_second': 1.202, 'epoch': 2.0}\n",
      "{'loss': 2.3587, 'grad_norm': 2.2020087242126465, 'learning_rate': 1.784435037447086e-05, 'epoch': 2.01}\n",
      "{'loss': 2.3383, 'grad_norm': 2.1042938232421875, 'learning_rate': 1.7437316834907197e-05, 'epoch': 2.03}\n",
      "{'loss': 2.321, 'grad_norm': 2.182854413986206, 'learning_rate': 1.7030283295343538e-05, 'epoch': 2.06}\n",
      "{'loss': 2.319, 'grad_norm': 2.20234751701355, 'learning_rate': 1.6623249755779876e-05, 'epoch': 2.08}\n",
      "{'loss': 2.2927, 'grad_norm': 2.4622364044189453, 'learning_rate': 1.6216216216216218e-05, 'epoch': 2.1}\n",
      "{'loss': 2.3131, 'grad_norm': 2.276627540588379, 'learning_rate': 1.5809182676652556e-05, 'epoch': 2.12}\n",
      "{'loss': 2.3021, 'grad_norm': 2.378438711166382, 'learning_rate': 1.5402149137088898e-05, 'epoch': 2.15}\n",
      "{'loss': 2.2758, 'grad_norm': 2.5027997493743896, 'learning_rate': 1.4995115597525236e-05, 'epoch': 2.17}\n",
      "{'loss': 2.3107, 'grad_norm': 2.0754849910736084, 'learning_rate': 1.4588082057961578e-05, 'epoch': 2.19}\n",
      "{'loss': 2.2993, 'grad_norm': 2.120500326156616, 'learning_rate': 1.4181048518397916e-05, 'epoch': 2.21}\n",
      "{'loss': 2.2857, 'grad_norm': 2.482067584991455, 'learning_rate': 1.3774014978834258e-05, 'epoch': 2.24}\n",
      "{'loss': 2.3138, 'grad_norm': 2.156250476837158, 'learning_rate': 1.3366981439270596e-05, 'epoch': 2.26}\n",
      "{'loss': 2.3109, 'grad_norm': 2.0112152099609375, 'learning_rate': 1.2959947899706938e-05, 'epoch': 2.28}\n",
      "{'loss': 2.2992, 'grad_norm': 2.2126593589782715, 'learning_rate': 1.2552914360143276e-05, 'epoch': 2.3}\n",
      "{'loss': 2.3023, 'grad_norm': 2.410353660583496, 'learning_rate': 1.2145880820579616e-05, 'epoch': 2.33}\n",
      "{'loss': 2.3116, 'grad_norm': 2.292889356613159, 'learning_rate': 1.1738847281015956e-05, 'epoch': 2.35}\n",
      "{'loss': 2.3066, 'grad_norm': 2.173419713973999, 'learning_rate': 1.1331813741452296e-05, 'epoch': 2.37}\n",
      "{'loss': 2.2733, 'grad_norm': 1.7938703298568726, 'learning_rate': 1.0924780201888635e-05, 'epoch': 2.39}\n",
      "{'loss': 2.3268, 'grad_norm': 1.9090203046798706, 'learning_rate': 1.0517746662324975e-05, 'epoch': 2.42}\n",
      "{'loss': 2.2804, 'grad_norm': 2.188750982284546, 'learning_rate': 1.0110713122761315e-05, 'epoch': 2.44}\n",
      "{'loss': 2.3619, 'grad_norm': 2.15006160736084, 'learning_rate': 9.703679583197655e-06, 'epoch': 2.46}\n",
      "{'loss': 2.2661, 'grad_norm': 1.995252251625061, 'learning_rate': 9.296646043633995e-06, 'epoch': 2.48}\n",
      "{'loss': 2.2746, 'grad_norm': 2.334820032119751, 'learning_rate': 8.889612504070335e-06, 'epoch': 2.51}\n",
      "{'loss': 2.2854, 'grad_norm': 2.454831600189209, 'learning_rate': 8.482578964506675e-06, 'epoch': 2.53}\n",
      "{'loss': 2.2894, 'grad_norm': 2.0796775817871094, 'learning_rate': 8.075545424943015e-06, 'epoch': 2.55}\n",
      "{'loss': 2.2562, 'grad_norm': 2.168222665786743, 'learning_rate': 7.668511885379355e-06, 'epoch': 2.57}\n",
      "{'loss': 2.2848, 'grad_norm': 2.1270856857299805, 'learning_rate': 7.2614783458156965e-06, 'epoch': 2.6}\n",
      "{'loss': 2.2817, 'grad_norm': 2.0160129070281982, 'learning_rate': 6.8544448062520364e-06, 'epoch': 2.62}\n",
      "{'loss': 2.269, 'grad_norm': 2.382535934448242, 'learning_rate': 6.447411266688376e-06, 'epoch': 2.64}\n",
      "{'loss': 2.3014, 'grad_norm': 1.9333324432373047, 'learning_rate': 6.0403777271247154e-06, 'epoch': 2.66}\n",
      "{'loss': 2.28, 'grad_norm': 2.0555520057678223, 'learning_rate': 5.633344187561055e-06, 'epoch': 2.69}\n",
      "{'loss': 2.2575, 'grad_norm': 2.009002685546875, 'learning_rate': 5.226310647997395e-06, 'epoch': 2.71}\n",
      "{'loss': 2.3183, 'grad_norm': 2.5866494178771973, 'learning_rate': 4.819277108433735e-06, 'epoch': 2.73}\n",
      "{'loss': 2.294, 'grad_norm': 2.092362642288208, 'learning_rate': 4.412243568870075e-06, 'epoch': 2.76}\n",
      "{'loss': 2.3125, 'grad_norm': 3.028928756713867, 'learning_rate': 4.005210029306415e-06, 'epoch': 2.78}\n",
      "{'loss': 2.2985, 'grad_norm': 2.1373884677886963, 'learning_rate': 3.598176489742755e-06, 'epoch': 2.8}\n",
      "{'loss': 2.2903, 'grad_norm': 2.0309066772460938, 'learning_rate': 3.191142950179095e-06, 'epoch': 2.82}\n",
      "{'loss': 2.3365, 'grad_norm': 1.976462721824646, 'learning_rate': 2.784109410615435e-06, 'epoch': 2.85}\n",
      "{'loss': 2.3225, 'grad_norm': 1.8123267889022827, 'learning_rate': 2.3770758710517748e-06, 'epoch': 2.87}\n",
      "{'loss': 2.2832, 'grad_norm': 2.4565532207489014, 'learning_rate': 1.9700423314881147e-06, 'epoch': 2.89}\n",
      "{'loss': 2.2979, 'grad_norm': 2.3088431358337402, 'learning_rate': 1.5630087919244546e-06, 'epoch': 2.91}\n",
      "{'loss': 2.322, 'grad_norm': 2.4589955806732178, 'learning_rate': 1.1559752523607946e-06, 'epoch': 2.94}\n",
      "{'loss': 2.3179, 'grad_norm': 2.014497756958008, 'learning_rate': 7.489417127971345e-07, 'epoch': 2.96}\n",
      "{'loss': 2.2765, 'grad_norm': 2.1943283081054688, 'learning_rate': 3.419081732334745e-07, 'epoch': 2.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages\\transformers\\generation\\utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e544c23424704e03ae7f7563e3467dc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/246 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 09:16:36,932 [INFO] - Using default tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.8242502212524414, 'eval_rouge1': 11.8785, 'eval_rouge2': 4.6153, 'eval_rougeL': 10.457, 'eval_rougeLsum': 11.9025, 'eval_runtime': 204.8678, 'eval_samples_per_second': 4.803, 'eval_steps_per_second': 1.201, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 9039.8248, 'train_samples_per_second': 2.938, 'train_steps_per_second': 0.735, 'train_loss': 2.9490119522862033, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    MT5ForConditionalGeneration, \n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "model = MT5ForConditionalGeneration.from_pretrained(MODEL_CHECKPOINT)\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "BATCH_SIZE = 4\n",
    "MODEL_NAME = \"mt5-base-cnn-summarizer-en-hi-v4\"\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=MODEL_NAME,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,\n",
    "    fp16=False, # <--- THIS IS THE FIX\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"rouge2\",\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Calculates ROUGE scores for evaluation.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    decoded_preds = [\"\\n\".join(pred.strip().split()) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(label.strip().split()) for label in decoded_labels]\n",
    "    \n",
    "    result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    \n",
    "    return {k: round(v * 100, 4) for k, v in result.items()}\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "trainer.save_model(f\"{MODEL_NAME}/final_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8574be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4919/4919 [00:00<00:00, 21255.17 examples/s]\n",
      "c:\\Users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 8854/8854 [00:33<00:00, 263.35 examples/s]\n",
      "Map: 100%|██████████| 984/984 [00:03<00:00, 272.22 examples/s]\n",
      "                                                     \n",
      " 36%|███▌      | 2403/6642 [1:32:22<1:12:46,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 18.8898, 'grad_norm': 25843.544921875, 'learning_rate': 5e-06, 'epoch': 0.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 36%|███▌      | 2403/6642 [1:59:13<1:12:46,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 17.3691, 'grad_norm': 24710.8671875, 'learning_rate': 1e-05, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       \n",
      " 36%|███▌      | 2403/6642 [2:30:24<1:12:46,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 15.1896, 'grad_norm': 1253.855712890625, 'learning_rate': 1.5e-05, 'epoch': 0.07}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datetime import datetime\n",
    "from datasets import DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    MT5ForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    MT5Tokenizer,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "# Configure logger to save to a unique file and print to console\n",
    "log_filename = f\"training_log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logging.info(\"--- Starting New Training Run ---\")\n",
    "\n",
    "try:\n",
    "    logging.info(\"Step 2: Starting data loading and preparation.\")\n",
    "    df = pd.read_csv('../Dataset/final_cleaned_dataset_CNN.csv').dropna().reset_index(drop=True)\n",
    "    raw_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "    PREFIX_ENG = \"summarize English: \"\n",
    "    PREFIX_HIN = \"summarize Hindi: \"\n",
    "\n",
    "    def format_dataset(batch):\n",
    "        inputs, targets = [], []\n",
    "        for article, eng_summary, hin_summary in zip(\n",
    "            batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']\n",
    "        ):\n",
    "            if isinstance(article, str):\n",
    "                inputs.append(PREFIX_ENG + article)\n",
    "                targets.append(eng_summary)\n",
    "                inputs.append(PREFIX_HIN + article)\n",
    "                targets.append(hin_summary)\n",
    "        return {'inputs': inputs, 'targets': targets}\n",
    "\n",
    "    processed_dataset = raw_dataset.map(\n",
    "        format_dataset, batched=True, remove_columns=raw_dataset.column_names\n",
    "    ).flatten()\n",
    "\n",
    "    train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    final_datasets = DatasetDict({\n",
    "        'train': train_test_split['train'], 'test': train_test_split['test']\n",
    "    })\n",
    "    logging.info(f\"Data prepared successfully. Training samples: {len(final_datasets['train'])}, Test samples: {len(final_datasets['test'])}\")\n",
    "\n",
    "    logging.info(\"Step 3: Starting tokenization.\")\n",
    "    MODEL_CHECKPOINT = \"google/mt5-base\"\n",
    "    tokenizer = MT5Tokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "    MAX_INPUT_LENGTH, MAX_TARGET_LENGTH = 1024, 256\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        model_inputs = tokenizer(examples['inputs'], max_length=MAX_INPUT_LENGTH, truncation=True)\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(examples['targets'], max_length=MAX_TARGET_LENGTH, truncation=True)\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    tokenized_datasets = final_datasets.map(tokenize_function, batched=True)\n",
    "    logging.info(\"Tokenization complete.\")\n",
    "\n",
    "    logging.info(\"Step 4: Initializing Trainer and preparing for model training.\")\n",
    "    model = MT5ForConditionalGeneration.from_pretrained(MODEL_CHECKPOINT)\n",
    "    rouge_metric = evaluate.load(\"rouge\")\n",
    "    BATCH_SIZE = 4\n",
    "    MODEL_NAME = \"mt5-base-cnn-summarizer-en-hi\"\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=MODEL_NAME,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f\"{MODEL_NAME}/logs\",\n",
    "        logging_steps=50,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        predict_with_generate=True,\n",
    "        fp16=False,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"rouge2\",\n",
    "        generation_max_length=256\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        decoded_preds = [\"\\n\".join(pred.strip().split()) for pred in decoded_preds]\n",
    "        decoded_labels = [\"\\n\".join(label.strip().split()) for label in decoded_labels]\n",
    "        result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "        return {k: round(v * 100, 4) for k, v in result.items()}\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    logging.info(\"Starting model training...\")\n",
    "    trainer.train()\n",
    "    logging.info(\"Training finished successfully.\")\n",
    "    trainer.save_model(f\"{MODEL_NAME}/final_model\")\n",
    "    logging.info(f\"Model saved to {MODEL_NAME}/final_model\")\n",
    "\n",
    "    logging.info(\"Step 5: Performing inference with the trained model.\")\n",
    "    model_path = f\"{MODEL_NAME}/final_model\"\n",
    "    summarizer = pipeline(\"summarization\", model=model_path, tokenizer=model_path)\n",
    "    \n",
    "    article_text = \"\"\"\n",
    "    The Indian Space Research Organisation (ISRO) successfully launched its ambitious Mars Orbiter Mission, also known as Mangalyaan, making India the first nation to succeed on its maiden attempt to reach Mars. The low-cost mission, which cost only $74 million, was designed to study the Martian atmosphere and surface. The spacecraft orbited Mars for several years, sending back valuable data and images, far exceeding its expected mission life. The success of Mangalyaan was a major milestone for India's space program, demonstrating its capability to execute complex interplanetary missions and cementing its position as a major player in space exploration.\n",
    "    \"\"\"\n",
    "\n",
    "    english_summary = summarizer(PREFIX_ENG + article_text, max_length=100)\n",
    "    logging.info(f\"--- English Summary ---\\n{english_summary[0]['summary_text']}\")\n",
    "\n",
    "    hindi_summary = summarizer(PREFIX_HIN + article_text, max_length=120)\n",
    "    logging.info(f\"--- Hindi Summary ---\\n{hindi_summary[0]['summary_text']}\")\n",
    "\n",
    "    logging.info(\"--- Run Completed Successfully ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An unexpected error occurred during the run: {e}\", exc_info=True)\n",
    "    logging.error(\"--- Run Failed ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d339de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 05:18:51,366 [INFO] - --- Starting New Training Run with GPU ---\n",
      "2025-09-22 05:18:51,366 [INFO] - Step 2: Starting data loading and preparation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8172412108d44bf28500d24c0125f5ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4919 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 05:18:52,218 [INFO] - Data prepared successfully. Samples: 8854 train, 984 test.\n",
      "2025-09-22 05:18:52,218 [INFO] - Step 3: Starting tokenization.\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f8e7263dc0e43d2be5318bf071e7ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8854 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1f84b837c684ec087ec4e1014c9fa2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 05:19:31,647 [INFO] - Tokenization complete.\n",
      "2025-09-22 05:19:31,647 [INFO] - Step 4: Initializing Trainer.\n",
      "2025-09-22 05:19:45,913 [INFO] - Starting model training on GPU...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6ce2c12f3c7462caedf16a570109c0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6642 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 0.02}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 0.05}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 0.07}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 0.09}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 0.11}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 0.14}\n",
      "{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 0.0, 'epoch': 0.16}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 123\u001b[0m\n\u001b[0;32m    112\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m    113\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m    114\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    119\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics,\n\u001b[0;32m    120\u001b[0m )\n\u001b[0;32m    122\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting model training on GPU...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 123\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining finished successfully.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    125\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/final_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages\\transformers\\trainer.py:1885\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1883\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1884\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1885\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1886\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1889\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages\\transformers\\trainer.py:2218\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2215\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m   2216\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[1;32m-> 2218\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2219\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2220\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2221\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2222\u001b[0m ):\n\u001b[0;32m   2223\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2224\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n\u001b[0;32m   2225\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datetime import datetime\n",
    "from datasets import DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    MT5ForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    MT5Tokenizer,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "# Configure logger\n",
    "log_filename = f\"training_log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logging.info(\"--- Starting New Training Run with GPU ---\")\n",
    "\n",
    "try:\n",
    "    logging.info(\"Step 2: Starting data loading and preparation.\")\n",
    "    df = pd.read_csv('../Dataset/final_cleaned_dataset_CNN.csv', engine='python', on_bad_lines='skip')\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    raw_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "    PREFIX_ENG = \"summarize English: \"\n",
    "    PREFIX_HIN = \"summarize Hindi: \"\n",
    "\n",
    "    def format_dataset(batch):\n",
    "        inputs, targets = [], []\n",
    "        for article, eng_summary, hin_summary in zip(\n",
    "            batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']\n",
    "        ):\n",
    "            if isinstance(article, str):\n",
    "                inputs.append(PREFIX_ENG + article)\n",
    "                targets.append(eng_summary)\n",
    "                inputs.append(PREFIX_HIN + article)\n",
    "                targets.append(hin_summary)\n",
    "        return {'inputs': inputs, 'targets': targets}\n",
    "\n",
    "    processed_dataset = raw_dataset.map(\n",
    "        format_dataset, batched=True, remove_columns=raw_dataset.column_names\n",
    "    ).flatten()\n",
    "\n",
    "    train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    final_datasets = DatasetDict({\n",
    "        'train': train_test_split['train'], 'test': train_test_split['test']\n",
    "    })\n",
    "    logging.info(f\"Data prepared successfully. Samples: {len(final_datasets['train'])} train, {len(final_datasets['test'])} test.\")\n",
    "\n",
    "    logging.info(\"Step 3: Starting tokenization.\")\n",
    "    MODEL_CHECKPOINT = \"google/mt5-base\"\n",
    "    tokenizer = MT5Tokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "    MAX_INPUT_LENGTH, MAX_TARGET_LENGTH = 1024, 256\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        model_inputs = tokenizer(examples['inputs'], max_length=MAX_INPUT_LENGTH, truncation=True)\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(examples['targets'], max_length=MAX_TARGET_LENGTH, truncation=True)\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    tokenized_datasets = final_datasets.map(tokenize_function, batched=True)\n",
    "    logging.info(\"Tokenization complete.\")\n",
    "\n",
    "    logging.info(\"Step 4: Initializing Trainer.\")\n",
    "    model = MT5ForConditionalGeneration.from_pretrained(MODEL_CHECKPOINT)\n",
    "    rouge_metric = evaluate.load(\"rouge\")\n",
    "    BATCH_SIZE = 4\n",
    "    MODEL_NAME = \"mt5-base-cnn-summarizer-en-hi\"\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=MODEL_NAME,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f\"{MODEL_NAME}/logs\",\n",
    "        logging_steps=50,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        predict_with_generate=True,\n",
    "        fp16=True,  # Re-enabled for RTX A5000 performance\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"rouge2\",\n",
    "        generation_max_length=256\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        decoded_preds = [\"\\n\".join(pred.strip().split()) for pred in decoded_preds]\n",
    "        decoded_labels = [\"\\n\".join(label.strip().split()) for label in decoded_labels]\n",
    "        result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "        return {k: round(v * 100, 4) for k, v in result.items}\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    logging.info(\"Starting model training on GPU...\")\n",
    "    trainer.train()\n",
    "    logging.info(\"Training finished successfully.\")\n",
    "    trainer.save_model(f\"{MODEL_NAME}/final_model\")\n",
    "    logging.info(f\"Model saved to {MODEL_NAME}/final_model\")\n",
    "\n",
    "    logging.info(\"--- Run Completed Successfully ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An unexpected error occurred: {e}\", exc_info=True)\n",
    "    logging.error(\"--- Run Failed ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec65485",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 13:56:36,347 [INFO] - --- Starting New Training Run with GPU ---\n",
      "2025-09-22 13:56:36,347 [INFO] - Starting data loading and preparation.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7177811628d4b159864768c2d51b6d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4919 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 13:56:37,191 [INFO] - Data prepared. Samples: 8854 train, 984 test.\n",
      "2025-09-22 13:56:37,191 [INFO] - Starting tokenization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feca67f1230b441d8590698b97ffe5cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/8854 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3946: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e5dac647f7b4a6c8f3276394b385260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/984 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-22 13:57:16,484 [INFO] - Tokenization complete.\n",
      "2025-09-22 13:57:16,484 [INFO] - Initializing Trainer.\n",
      "c:\\Users\\admin\\anaconda3\\envs\\news_summarizer\\lib\\site-packages\\transformers\\training_args.py:1474: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "2025-09-22 13:57:27,517 [INFO] - Starting model training on GPU...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d57aa18690e46aa82e40305198c73d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6642 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 18.8898, 'grad_norm': 25843.544921875, 'learning_rate': 5e-06, 'epoch': 0.02}\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import evaluate\n",
    "from datetime import datetime\n",
    "from datasets import DatasetDict, Dataset\n",
    "from transformers import (\n",
    "    MT5ForConditionalGeneration,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    MT5Tokenizer,\n",
    "    pipeline\n",
    ")\n",
    "\n",
    "# Configure a logger to save progress and errors to a timestamped file\n",
    "log_filename = f\"training_log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(log_filename),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "logging.info(\"--- Starting New Training Run with GPU ---\")\n",
    "\n",
    "try:\n",
    "    logging.info(\"Starting data loading and preparation.\")\n",
    "    # Robustly read the CSV, skipping any malformed lines\n",
    "    df = pd.read_csv('../Dataset/final_cleaned_dataset_CNN.csv', engine='python', on_bad_lines='skip')\n",
    "    df.dropna(inplace=True)\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    raw_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "    PREFIX_ENG = \"summarize English: \"\n",
    "    PREFIX_HIN = \"summarize Hindi: \"\n",
    "\n",
    "    def format_dataset(batch):\n",
    "        inputs, targets = [], []\n",
    "        for article, eng_summary, hin_summary in zip(\n",
    "            batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']\n",
    "        ):\n",
    "            if isinstance(article, str):\n",
    "                inputs.append(PREFIX_ENG + article)\n",
    "                targets.append(eng_summary)\n",
    "                inputs.append(PREFIX_HIN + article)\n",
    "                targets.append(hin_summary)\n",
    "        return {'inputs': inputs, 'targets': targets}\n",
    "\n",
    "    processed_dataset = raw_dataset.map(\n",
    "        format_dataset, batched=True, remove_columns=raw_dataset.column_names\n",
    "    ).flatten()\n",
    "\n",
    "    train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    final_datasets = DatasetDict({\n",
    "        'train': train_test_split['train'], 'test': train_test_split['test']\n",
    "    })\n",
    "    logging.info(f\"Data prepared. Samples: {len(final_datasets['train'])} train, {len(final_datasets['test'])} test.\")\n",
    "\n",
    "    logging.info(\"Starting tokenization.\")\n",
    "    MODEL_CHECKPOINT = \"google/mt5-base\"\n",
    "    tokenizer = MT5Tokenizer.from_pretrained(MODEL_CHECKPOINT)\n",
    "    MAX_INPUT_LENGTH, MAX_TARGET_LENGTH = 1024, 256\n",
    "\n",
    "    def tokenize_function(examples):\n",
    "        model_inputs = tokenizer(examples['inputs'], max_length=MAX_INPUT_LENGTH, truncation=True)\n",
    "        with tokenizer.as_target_tokenizer():\n",
    "            labels = tokenizer(examples['targets'], max_length=MAX_TARGET_LENGTH, truncation=True)\n",
    "        model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "        return model_inputs\n",
    "\n",
    "    tokenized_datasets = final_datasets.map(tokenize_function, batched=True)\n",
    "    logging.info(\"Tokenization complete.\")\n",
    "\n",
    "    logging.info(\"Initializing Trainer.\")\n",
    "    model = MT5ForConditionalGeneration.from_pretrained(MODEL_CHECKPOINT)\n",
    "    rouge_metric = evaluate.load(\"rouge\")\n",
    "    BATCH_SIZE = 4\n",
    "    MODEL_NAME = \"mt5-base-cnn-summarizer-en-hi_v3\"\n",
    "\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=MODEL_NAME,\n",
    "        num_train_epochs=3,\n",
    "        per_device_train_batch_size=BATCH_SIZE,\n",
    "        per_device_eval_batch_size=BATCH_SIZE,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        logging_dir=f\"{MODEL_NAME}/logs\",\n",
    "        logging_steps=50,\n",
    "        evaluation_strategy=\"epoch\", # Use \"eval_strategy\" in newer library versions\n",
    "        save_strategy=\"epoch\",\n",
    "        predict_with_generate=True,\n",
    "        fp16=False, # Set to False for maximum stability\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"rouge2\",\n",
    "        generation_max_length=256,\n",
    "        adam_epsilon=1e-8 # Added for optimizer stability\n",
    "    )\n",
    "\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "    def compute_metrics(eval_pred):\n",
    "        predictions, labels = eval_pred\n",
    "        # Clean up potentially invalid token IDs before decoding\n",
    "        predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "        decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "        \n",
    "        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        decoded_preds = [\"\\n\".join(pred.strip().split()) for pred in decoded_preds]\n",
    "        decoded_labels = [\"\\n\".join(label.strip().split()) for label in decoded_labels]\n",
    "        result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "        return {k: round(v * 100, 4) for k, v in result.items()}\n",
    "\n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    logging.info(\"Starting model training on GPU...\")\n",
    "    trainer.train()\n",
    "    logging.info(\"Training finished successfully.\")\n",
    "    trainer.save_model(f\"{MODEL_NAME}/final_model\")\n",
    "    logging.info(f\"Model saved to {MODEL_NAME}/final_model\")\n",
    "\n",
    "    logging.info(\"Performing inference with the trained model.\")\n",
    "    model_path = f\"{MODEL_NAME}/final_model\"\n",
    "    summarizer = pipeline(\"summarization\", model=model_path, tokenizer=model_path)\n",
    "    \n",
    "    article_text = \"\"\"\n",
    "    The Indian Space Research Organisation (ISRO) successfully launched its ambitious Mars Orbiter Mission, also known as Mangalyaan, making India the first nation to succeed on its maiden attempt to reach Mars. The low-cost mission, which cost only $74 million, was designed to study the Martian atmosphere and surface. The spacecraft orbited Mars for several years, sending back valuable data and images, far exceeding its expected mission life. The success of Mangalyaan was a major milestone for India's space program, demonstrating its capability to execute complex interplanetary missions and cementing its position as a major player in space exploration.\n",
    "    \"\"\"\n",
    "\n",
    "    english_summary = summarizer(PREFIX_ENG + article_text, max_length=100)\n",
    "    logging.info(f\"--- English Summary ---\\n{english_summary[0]['summary_text']}\")\n",
    "\n",
    "    hindi_summary = summarizer(PREFIX_HIN + article_text, max_length=120)\n",
    "    logging.info(f\"--- Hindi Summary ---\\n{hindi_summary[0]['summary_text']}\")\n",
    "\n",
    "    logging.info(\"--- Run Completed Successfully ---\")\n",
    "\n",
    "except Exception as e:\n",
    "    logging.error(f\"An unexpected error occurred: {e}\", exc_info=True)\n",
    "    logging.error(\"--- Run Failed ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b928120",
   "metadata": {},
   "source": [
    "Inference of the MOdel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79a35d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- English Summary ---\n",
      "Bihar PM Modi threatened Congress-RJD for Abusing His Late Mother, asserting that someone who disowned his sister and stabbed his own brother in the back cannot be expected to respect any woman. He asserted that Bihar would \"give a fitting reply\" to Tejashwi, who stood on stage and encouraged party workers to hurl abuses. Bihar deputy CM Samrat Choudhar\n",
      "\n",
      "--- Hindi Summary ---\n",
      "मैनेजर PM मोदी ने कांग्रेस-RJD के अध्यक्ष टीजाशवी पीरादा याददा के विरोध में पार्टी के अध्यक्ष तेजाशवी पीरादा याददा के विरोध में विरोध प्रदर्शन किया, जिसमें वह अपने पिता और अपनी बेटी को गालियों से संबोधित करने के लिए विरोध प्रदर्शन कर रहे थे। यह घटना Bihar के मुख्य मंत्री रमेश गंदेदी ने बाद में बाद में\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_path = f\"{MODEL_NAME}/final_model\"\n",
    "summarizer = pipeline(\"summarization\", model=model_path, tokenizer=model_path)\n",
    "\n",
    "article_text = \"\"\"\n",
    "‘Insult To Every Mother’: PM Modi Tears Into Congress-RJD For Abusing His Late Mother\n",
    "\"It is well-known that someone who has disowned his own sister and stabbed his own brother in the back cannot be expected to respect any woman. If he had education and values, he would never have resorted to such cheap tactics just before the Devi Paksha,\" he continued.He went on to declare that the women in poll-bound Bihar would \"give a fitting reply to Tejashwi, who stood on the stage and got his own workers to insult the late mother of Prime Minister ji.\"\"The people of Bihar are hurt and outraged by the insult to Prime Minister Modi ji’s late mother,\" he said.The slugfest over alleged abuses flared up again after Bihar deputy CM Samrat Choudhary shared a video of Tejashwi's rally accusing RJD leader of encouraging party workers to hurl abuses. This came only weeks after a similar row during Rahul Gandhi’s Darbhanga rally.\"Tejashwi Yadav once again had Modi's deceased mother abused. He has once again shattered Bihar's culture. The more RJD workers hurled abuses at the rally, the more Tejashwi Yadav encouraged them. The mothers and sisters of Bihar will surely take account of this hooligan mentality and abuse,\" he said.RJD defended itself saying that the video was \"fabricated\". “Leader of opposition Tejashwi Prasad Yadav ji is delivering a speech today at Mahua during the Bihar Adhikar Yatra. The speech is available on my Facebook page — you can listen to it. In it, no worker or any person has abused or used foul language for the Hon’ble Prime Minister,” party leader Mukesh Raushan said.\"\"\"\n",
    "\n",
    "# Generate English Summary using the English prefix\n",
    "english_summary = summarizer(PREFIX_ENG + article_text, max_length=100)\n",
    "print(\"--- English Summary ---\")\n",
    "print(english_summary[0]['summary_text'])\n",
    "\n",
    "# Generate Hindi Summary using the Hindi prefix\n",
    "hindi_summary = summarizer(PREFIX_HIN + article_text, max_length=120)\n",
    "print(\"\\n--- Hindi Summary ---\")\n",
    "print(hindi_summary[0]['summary_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news_summarizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
