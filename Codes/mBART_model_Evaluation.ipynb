{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8e804243",
   "metadata": {},
   "source": [
    "Bulk Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bcb4c641",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 02:28:31,001 [INFO] - --- Starting Bulk Evaluation with Custom Inference Loop ---\n",
      "2025-10-08 02:28:31,001 [INFO] - Loading model from: mbart-large-50-cnn-summarizer-v14/final_model\n",
      "2025-10-08 02:28:33,938 [INFO] - Loading evaluation data from: ../Dataset/filtered_articles_CNN.csv\n",
      "2025-10-08 02:28:34,118 [WARNING] - Using a subset of 200 examples for quick evaluation.\n",
      "2025-10-08 02:28:34,118 [INFO] - Evaluation data prepared with 200 examples.\n",
      "2025-10-08 02:28:34,118 [INFO] - Generating summaries for the dataset...\n",
      "Summarizing Batches:   0%|          | 0/13 [10:42<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 146\u001b[0m\n\u001b[0;32m    143\u001b[0m         logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn unexpected error occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    145\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 146\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 103\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     94\u001b[0m eng_summary_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m     95\u001b[0m     inputs\u001b[38;5;241m.\u001b[39minput_ids, num_beams\u001b[38;5;241m=\u001b[39mNUM_BEAMS, max_length\u001b[38;5;241m=\u001b[39mmax_len, min_length\u001b[38;5;241m=\u001b[39mmin_len,\n\u001b[0;32m     96\u001b[0m     length_penalty\u001b[38;5;241m=\u001b[39mLENGTH_PENALTY, no_repeat_ngram_size\u001b[38;5;241m=\u001b[39mNO_REPEAT_NGRAM_SIZE,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     99\u001b[0m     forced_bos_token_id\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mlang_code_to_id[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_XX\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    100\u001b[0m )\n\u001b[0;32m    101\u001b[0m batch_eng_summaries \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(eng_summary_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 103\u001b[0m hin_summary_ids \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    104\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_BEAMS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_len\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmin_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    105\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlength_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mLENGTH_PENALTY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mno_repeat_ngram_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNO_REPEAT_NGRAM_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    106\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrepetition_penalty\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mREPETITION_PENALTY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDO_SAMPLE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mEARLY_STOPPING\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOP_K\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTOP_P\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTEMPERATURE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    108\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforced_bos_token_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlang_code_to_id\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhi_IN\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    109\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m batch_hin_summaries \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(hin_summary_ids, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    112\u001b[0m generated_eng_summaries\u001b[38;5;241m.\u001b[39mextend(batch_eng_summaries)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\generation\\utils.py:2564\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2561\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[0;32m   2563\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[1;32m-> 2564\u001b[0m result \u001b[38;5;241m=\u001b[39m decoding_method(\n\u001b[0;32m   2565\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2566\u001b[0m     input_ids,\n\u001b[0;32m   2567\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2568\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2569\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2570\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_mode_kwargs,\n\u001b[0;32m   2571\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2572\u001b[0m )\n\u001b[0;32m   2574\u001b[0m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[0;32m   2575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2576\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mreturn_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result\u001b[38;5;241m.\u001b[39mpast_key_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_legacy_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2579\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\generation\\utils.py:3265\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   3262\u001b[0m flat_running_sequences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flatten_beam_dim(running_sequences[:, :, :cur_len])\n\u001b[0;32m   3263\u001b[0m model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(flat_running_sequences, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m-> 3265\u001b[0m model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_inputs, return_dict\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   3267\u001b[0m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[0;32m   3268\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_model_kwargs_for_generation(\n\u001b[0;32m   3269\u001b[0m     model_outputs,\n\u001b[0;32m   3270\u001b[0m     model_kwargs,\n\u001b[0;32m   3271\u001b[0m     is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[0;32m   3272\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\models\\mbart\\modeling_mbart.py:1439\u001b[0m, in \u001b[0;36mMBartForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1436\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1437\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id)\n\u001b[1;32m-> 1439\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1440\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1450\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1451\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1452\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1453\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1456\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1457\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\n\u001b[0;32m   1459\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\models\\mbart\\modeling_mbart.py:1267\u001b[0m, in \u001b[0;36mMBartModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1261\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1262\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1263\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1264\u001b[0m     )\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1267\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1276\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1278\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1281\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\models\\mbart\\modeling_mbart.py:1116\u001b[0m, in \u001b[0;36mMBartDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dropout_probability \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayerdrop:\n\u001b[0;32m   1114\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 1116\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[0;32m   1120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1124\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1128\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\models\\mbart\\modeling_mbart.py:451\u001b[0m, in \u001b[0;36mMBartDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_values, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[0;32m    449\u001b[0m residual \u001b[38;5;241m=\u001b[39m hidden_states\n\u001b[0;32m    450\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_layer_norm(hidden_states)\n\u001b[1;32m--> 451\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactivation_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mactivation_dropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    453\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(hidden_states)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\activations.py:85\u001b[0m, in \u001b[0;36mGELUActivation.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m---> 85\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import unicodedata\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_PATH = \"mbart-large-50-cnn-summarizer-v14/final_model\"\n",
    "EVAL_DATA_PATH = \"../Dataset/filtered_articles_CNN.csv\" \n",
    "BATCH_SIZE = 16 \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#Set to a number for a quick evaluation on a subset, or None for the full dataset ---\n",
    "NUM_EVAL_SAMPLES = 200\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"bulk_evaluation_log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] - %(message)s\", handlers=[logging.FileHandler(log_filename), logging.StreamHandler()])\n",
    "\n",
    "def sanitize_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    return text.replace('\"\"', '\"').strip()\n",
    "\n",
    "def normalize_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    return ' '.join(unicodedata.normalize('NFKC', text).split())\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        logging.info(\"--- Starting Bulk Evaluation with Custom Inference Loop ---\")\n",
    "        \n",
    "        # 1. --- Load Model and Tokenizer ---\n",
    "        logging.info(f\"Loading model from: {MODEL_PATH}\")\n",
    "        model = MBartForConditionalGeneration.from_pretrained(MODEL_PATH).to(DEVICE)\n",
    "        tokenizer = MBart50TokenizerFast.from_pretrained(MODEL_PATH)\n",
    "        model.eval() \n",
    "        \n",
    "        # 2. --- Load and Prepare the Evaluation Dataset ---\n",
    "        logging.info(f\"Loading evaluation data from: {EVAL_DATA_PATH}\")\n",
    "        df_eval = pd.read_csv(EVAL_DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        df_eval.dropna(subset=['raw_news_article', 'english_summary', 'hindi_summary'], inplace=True)\n",
    "        \n",
    "        for col in ['raw_news_article', 'english_summary', 'hindi_summary']:\n",
    "            df_eval[col] = df_eval[col].apply(sanitize_text).apply(normalize_text)\n",
    "        \n",
    "        eval_dataset = Dataset.from_pandas(df_eval)\n",
    "\n",
    "        # --- NEW: Logic to select a subset for evaluation ---\n",
    "        if NUM_EVAL_SAMPLES and NUM_EVAL_SAMPLES < len(eval_dataset):\n",
    "            logging.warning(f\"Using a subset of {NUM_EVAL_SAMPLES} examples for quick evaluation.\")\n",
    "            eval_dataset = eval_dataset.select(range(NUM_EVAL_SAMPLES))\n",
    "\n",
    "        logging.info(f\"Evaluation data prepared with {len(eval_dataset)} examples.\")\n",
    "\n",
    "        data_loader = DataLoader(eval_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "        # 3. --- Run Custom Inference Loop ---\n",
    "        generated_eng_summaries = []\n",
    "        generated_hin_summaries = []\n",
    "        reference_eng_summaries = []\n",
    "        reference_hin_summaries = []\n",
    "\n",
    "        logging.info(\"Generating summaries for the dataset...\")\n",
    "        for batch in tqdm(data_loader, desc=\"Summarizing Batches\"):\n",
    "            articles = batch['raw_news_article']\n",
    "            \n",
    "            tokenizer.src_lang = \"en_XX\"\n",
    "            inputs = tokenizer(articles, return_tensors=\"pt\", max_length=1024, truncation=True, padding=True).to(DEVICE)\n",
    "            \n",
    "            NUM_BEAMS = 10\n",
    "            NO_REPEAT_NGRAM_SIZE = 3\n",
    "            REPETITION_PENALTY = 3.0\n",
    "            LENGTH_PENALTY = 1.0\n",
    "            DO_SAMPLE = True\n",
    "            EARLY_STOPPING = True\n",
    "            TOP_K = 50\n",
    "            TOP_P = 0.95\n",
    "            TEMPERATURE = 0.8\n",
    "            \n",
    "            input_word_count = np.mean([len(art.split()) for art in articles])\n",
    "            ratio = 0.25 \n",
    "            min_len = max(40, int(input_word_count * (ratio - 0.1))) \n",
    "            max_len = min(256, int(input_word_count * (ratio + 0.1)))\n",
    "\n",
    "            eng_summary_ids = model.generate(\n",
    "                inputs.input_ids, num_beams=NUM_BEAMS, max_length=max_len, min_length=min_len,\n",
    "                length_penalty=LENGTH_PENALTY, no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE,\n",
    "                repetition_penalty=REPETITION_PENALTY, do_sample=DO_SAMPLE, early_stopping=EARLY_STOPPING,\n",
    "                top_k=TOP_K, top_p=TOP_P, temperature=TEMPERATURE,\n",
    "                forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"]\n",
    "            )\n",
    "            batch_eng_summaries = tokenizer.batch_decode(eng_summary_ids, skip_special_tokens=True)\n",
    "            \n",
    "            hin_summary_ids = model.generate(\n",
    "                inputs.input_ids, num_beams=NUM_BEAMS, max_length=max_len, min_length=min_len,\n",
    "                length_penalty=LENGTH_PENALTY, no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE,\n",
    "                repetition_penalty=REPETITION_PENALTY, do_sample=DO_SAMPLE, early_stopping=EARLY_STOPPING,\n",
    "                top_k=TOP_K, top_p=TOP_P, temperature=TEMPERATURE,\n",
    "                forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"]\n",
    "            )\n",
    "            batch_hin_summaries = tokenizer.batch_decode(hin_summary_ids, skip_special_tokens=True)\n",
    "            \n",
    "            generated_eng_summaries.extend(batch_eng_summaries)\n",
    "            generated_hin_summaries.extend(batch_hin_summaries)\n",
    "            reference_eng_summaries.extend(batch['english_summary'])\n",
    "            reference_hin_summaries.extend(batch['hindi_summary'])\n",
    "\n",
    "        # 4. --- Calculate and Print Final Metrics ---\n",
    "        logging.info(\"Summarization complete. Calculating final metrics...\")\n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "        bleurt_metric = evaluate.load(\"bleurt\", \"bleurt-20\")\n",
    "\n",
    "        all_generated_summaries = generated_eng_summaries + generated_hin_summaries\n",
    "        all_reference_summaries = reference_eng_summaries + reference_hin_summaries\n",
    "\n",
    "        rouge_results = rouge_metric.compute(predictions=all_generated_summaries, references=all_reference_summaries)\n",
    "        bleurt_results = bleurt_metric.compute(predictions=all_generated_summaries, references=all_reference_summaries)\n",
    "\n",
    "        final_metrics = {\n",
    "            \"rouge1\": rouge_results[\"rouge1\"] * 100,\n",
    "            \"rouge2\": rouge_results[\"rouge2\"] * 100,\n",
    "            \"rougeL\": rouge_results[\"rougeL\"] * 100,\n",
    "            \"bleurt_f1\": np.mean(bleurt_results[\"scores\"]) * 100\n",
    "        }\n",
    "        \n",
    "        logging.info(\"\\n\" + \"=\"*80)\n",
    "        logging.info(\"--- FINAL BULK EVALUATION METRICS ---\".center(80))\n",
    "        logging.info(\"=\"*80)\n",
    "        for key, value in final_metrics.items():\n",
    "            logging.info(f\"  - {key}: {value:.4f}\")\n",
    "        logging.info(\"=\"*80)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a3e6db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0276345",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import unicodedata\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast\n",
    ")\n",
    "from tqdm import tqdm\n",
    "# Import the bleurt library directly for GPU-acceleration ---\n",
    "try:\n",
    "    from bleurt import score as bleurt_scorer\n",
    "    BLEURT_INSTALLED = True\n",
    "except ImportError:\n",
    "    BLEURT_INSTALLED = False\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_PATH = \"mbart-large-50-cnn-summarizer-v14/final_model\"\n",
    "EVAL_DATA_PATH = \"../Dataset/filtered_articles_CNN.csv\" \n",
    "BATCH_SIZE = 16 \n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUM_EVAL_SAMPLES = 500\n",
    "\n",
    "# --- NEW: Control BLEURT on the GPU ---\n",
    "USE_GPU_FOR_BLEURT = True\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"bulk_evaluation_log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] - %(message)s\", handlers=[logging.FileHandler(log_filename), logging.StreamHandler()])\n",
    "\n",
    "def sanitize_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    return text.replace('\"\"', '\"').strip()\n",
    "\n",
    "def normalize_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    return ' '.join(unicodedata.normalize('NFKC', text).split())\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        logging.info(\"--- Starting Bulk Evaluation with Custom Inference Loop ---\")\n",
    "        \n",
    "        # 1. --- Load Model and Tokenizer ---\n",
    "        logging.info(f\"Loading model from: {MODEL_PATH}\")\n",
    "        model = MBartForConditionalGeneration.from_pretrained(MODEL_PATH).to(DEVICE)\n",
    "        tokenizer = MBart50TokenizerFast.from_pretrained(MODEL_PATH)\n",
    "        model.eval() \n",
    "        \n",
    "        # 2. --- Load and Prepare the Evaluation Dataset ---\n",
    "        logging.info(f\"Loading evaluation data from: {EVAL_DATA_PATH}\")\n",
    "        df_eval = pd.read_csv(EVAL_DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        df_eval.dropna(subset=['raw_news_article', 'english_summary', 'hindi_summary'], inplace=True)\n",
    "        \n",
    "        for col in ['raw_news_article', 'english_summary', 'hindi_summary']:\n",
    "            df_eval[col] = df_eval[col].apply(sanitize_text).apply(normalize_text)\n",
    "        \n",
    "        eval_dataset = Dataset.from_pandas(df_eval)\n",
    "\n",
    "        if NUM_EVAL_SAMPLES and NUM_EVAL_SAMPLES < len(eval_dataset):\n",
    "            logging.warning(f\"Using a subset of {NUM_EVAL_SAMPLES} examples for quick evaluation.\")\n",
    "            eval_dataset = eval_dataset.select(range(NUM_EVAL_SAMPLES))\n",
    "\n",
    "        logging.info(f\"Evaluation data prepared with {len(eval_dataset)} examples.\")\n",
    "\n",
    "        data_loader = DataLoader(eval_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "        # 3. --- Run Custom Inference Loop ---\n",
    "        generated_eng_summaries, generated_hin_summaries = [], []\n",
    "        reference_eng_summaries, reference_hin_summaries = [], []\n",
    "\n",
    "        logging.info(\"Generating summaries for the dataset...\")\n",
    "        for batch in tqdm(data_loader, desc=\"Summarizing Batches\"):\n",
    "            articles = batch['raw_news_article']\n",
    "            \n",
    "            tokenizer.src_lang = \"en_XX\"\n",
    "            inputs = tokenizer(articles, return_tensors=\"pt\", max_length=1024, truncation=True, padding=True).to(DEVICE)\n",
    "            \n",
    "            NUM_BEAMS, NO_REPEAT_NGRAM_SIZE, REPETITION_PENALTY = 10, 3, 3.0\n",
    "            LENGTH_PENALTY, DO_SAMPLE, EARLY_STOPPING = 1.0, True, True\n",
    "            TOP_K, TOP_P, TEMPERATURE = 50, 0.95, 0.8\n",
    "            \n",
    "            input_word_count = np.mean([len(art.split()) for art in articles])\n",
    "            ratio = 0.25 \n",
    "            min_len, max_len = max(40, int(input_word_count * (ratio - 0.1))), min(256, int(input_word_count * (ratio + 0.1)))\n",
    "\n",
    "            eng_summary_ids = model.generate(\n",
    "                inputs.input_ids, num_beams=NUM_BEAMS, max_length=max_len, min_length=min_len, length_penalty=LENGTH_PENALTY,\n",
    "                no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE, repetition_penalty=REPETITION_PENALTY, do_sample=DO_SAMPLE,\n",
    "                early_stopping=EARLY_STOPPING, top_k=TOP_K, top_p=TOP_P, temperature=TEMPERATURE,\n",
    "                forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"]\n",
    "            )\n",
    "            batch_eng_summaries = tokenizer.batch_decode(eng_summary_ids, skip_special_tokens=True)\n",
    "            \n",
    "            hin_summary_ids = model.generate(\n",
    "                inputs.input_ids, num_beams=NUM_BEAMS, max_length=max_len, min_length=min_len, length_penalty=LENGTH_PENALTY,\n",
    "                no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE, repetition_penalty=REPETITION_PENALTY, do_sample=DO_SAMPLE,\n",
    "                early_stopping=EARLY_STOPPING, top_k=TOP_K, top_p=TOP_P, temperature=TEMPERATURE,\n",
    "                forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"]\n",
    "            )\n",
    "            batch_hin_summaries = tokenizer.batch_decode(hin_summary_ids, skip_special_tokens=True)\n",
    "            \n",
    "            generated_eng_summaries.extend(batch_eng_summaries)\n",
    "            generated_hin_summaries.extend(batch_hin_summaries)\n",
    "            reference_eng_summaries.extend(batch['english_summary'])\n",
    "            reference_hin_summaries.extend(batch['hindi_summary'])\n",
    "\n",
    "        # 4. --- Calculate and Print Final Metrics ---\n",
    "        logging.info(\"Summarization complete. Calculating final metrics...\")\n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "        \n",
    "        all_generated = generated_eng_summaries + generated_hin_summaries\n",
    "        all_references = reference_eng_summaries + reference_hin_summaries\n",
    "\n",
    "        logging.info(\"Calculating ROUGE scores...\")\n",
    "        rouge_results = rouge_metric.compute(predictions=all_generated, references=all_references)\n",
    "        final_metrics = { \"rouge1\": rouge_results[\"rouge1\"] * 100, \"rouge2\": rouge_results[\"rouge2\"] * 100, \"rougeL\": rouge_results[\"rougeL\"] * 100 }\n",
    "\n",
    "        if USE_GPU_FOR_BLEURT:\n",
    "            if BLEURT_INSTALLED:\n",
    "                logging.info(\"Calculating BLEURT scores on GPU (this may still take some time)...\")\n",
    "                bleurt_checkpoint = \"bleurt-20\"\n",
    "                scorer = bleurt_scorer.BleurtScorer(bleurt_checkpoint)\n",
    "                bleurt_scores = scorer.score(references=all_references, candidates=all_generated)\n",
    "                final_metrics[\"bleurt_f1\"] = np.mean(bleurt_scores) * 100\n",
    "            else:\n",
    "                logging.error(\"The 'bleurt' library is not installed. Skipping GPU-based BLEURT calculation. Please run 'pip install bleurt'.\")\n",
    "        else:\n",
    "            logging.info(\"Calculating BLEURT scores on CPU (this will be very slow)...\")\n",
    "            bleurt_metric_cpu = evaluate.load(\"bleurt\", \"bleurt-20\")\n",
    "            bleurt_results = bleurt_metric_cpu.compute(predictions=all_generated, references=all_references)\n",
    "            final_metrics[\"bleurt_f1\"] = np.mean(bleurt_results[\"scores\"]) * 100\n",
    "        \n",
    "        logging.info(\"\\n\" + \"=\"*80)\n",
    "        logging.info(\"--- FINAL BULK EVALUATION METRICS ---\".center(80))\n",
    "        logging.info(\"=\"*80)\n",
    "        for key, value in final_metrics.items():\n",
    "            logging.info(f\"  - {key}: {value:.4f}\")\n",
    "        logging.info(\"=\"*80)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b65ce5c",
   "metadata": {},
   "source": [
    "Individual Interactive Sumaarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bd871cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model and tokenizer from: mbart-large-50-cnn-summarizer-v14/final_model...\n",
      "\n",
      "================================================================================\n",
      "                              --- Model Ready ---                               \n",
      "               You can now generate summaries in the cells below.               \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import textwrap\n",
    "import logging\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_PATH = \"mbart-large-50-cnn-summarizer-v14/final_model\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Global variables to hold the loaded model and tokenizer ---\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    Loads the fine-tuned mBART model and tokenizer into memory.\n",
    "    This function is called automatically at the end of this cell.\n",
    "    \"\"\"\n",
    "    global model, tokenizer\n",
    "    \n",
    "    if model is not None and tokenizer is not None:\n",
    "        print(\"Model and tokenizer are already loaded.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    try:\n",
    "        print(f\"Loading model and tokenizer from: {MODEL_PATH}...\")\n",
    "        model = MBartForConditionalGeneration.from_pretrained(MODEL_PATH).to(DEVICE)\n",
    "        tokenizer = MBart50TokenizerFast.from_pretrained(MODEL_PATH)\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"--- Model Ready ---\".center(80))\n",
    "        print(\"You can now generate summaries in the cells below.\".center(80))\n",
    "        print(\"=\"*80)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        model, tokenizer = None, None\n",
    "\n",
    "def generate_summary(article_text):\n",
    "    \"\"\"\n",
    "    Takes a news article string and prints high-quality, abstractive summaries.\n",
    "    Uses a robust length strategy and finely-tuned generation parameters.\n",
    "    \"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        print(\"Model not loaded. Please run the setup cell (containing `load_model()`) first.\")\n",
    "        return\n",
    "\n",
    "    # --- DEFINITIVE: Final, Robust Length Calculation ---\n",
    "    input_word_count = len(article_text.split())\n",
    "    # Give the model a generous window to work with, which prevents cut-off sentences.\n",
    "    min_len = 30\n",
    "    max_len = 510 # A generous max length. `early_stopping` will ensure it stops naturally.\n",
    "\n",
    "    # --- DEFINITIVE: Finely-Tuned Beam Search for Factual, Complete Summaries ---\n",
    "    gen_kwargs = {\n",
    "        \"num_beams\": 12,\n",
    "        \"length_penalty\": 2.0,            # Strongly encourages the model to generate complete sentences.\n",
    "        \"repetition_penalty\": 2.5,        # Strongly discourages repeating words or phrases.\n",
    "        \"no_repeat_ngram_size\": 3,        # Prevents repeating sequences of 3 words.\n",
    "        \"do_sample\": False,               # CRITICAL: Disables sampling to prioritize factual accuracy.\n",
    "        \"early_stopping\": True,           # Finishes generation when all beams have reached the end token.\n",
    "        \"min_length\": min_len,\n",
    "        \"max_length\": max_len,\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SOURCE ARTICLE (truncated):\")\n",
    "    print(\"=\"*80)\n",
    "    print(textwrap.fill(article_text[:1000] + (\"...\" if len(article_text) > 1000 else \"\"), width=80))\n",
    "    print(f\"\\n(Article length: {input_word_count} words. Target summary length: {min_len}-{max_len} tokens)\")\n",
    "\n",
    "    # Tokenize the article\n",
    "    tokenizer.src_lang = \"en_XX\"\n",
    "    inputs = tokenizer(article_text, return_tensors=\"pt\", max_length=1024, truncation=True).to(DEVICE)\n",
    "\n",
    "    # --- Generate English Summary ---\n",
    "    eng_summary_ids = model.generate(\n",
    "        inputs.input_ids,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"],\n",
    "        **gen_kwargs\n",
    "    )\n",
    "    english_summary = tokenizer.decode(eng_summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATED ENGLISH SUMMARY:\")\n",
    "    print(\"=\"*80)\n",
    "    print(textwrap.fill(english_summary, width=80))\n",
    "\n",
    "    # --- Generate Hindi Summary ---\n",
    "    hin_summary_ids = model.generate(\n",
    "        inputs.input_ids,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"],\n",
    "        **gen_kwargs\n",
    "    )\n",
    "    hindi_summary = tokenizer.decode(hin_summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATED HINDI SUMMARY:\")\n",
    "    print(\"=\"*80)\n",
    "    print(textwrap.fill(hindi_summary, width=90))\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# --- Automatically load the model when this cell is run ---\n",
    "load_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0b2e6d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE (truncated):\n",
      "================================================================================\n",
      " India secured a decisive victory over Australia in the final match of the T20\n",
      "series, winning by a margin of 35 runs in Bengaluru. Batting first, India posted\n",
      "a competitive total of 198 for 4, thanks to a powerful half-century from captain\n",
      "Suryakumar Yadav, who scored 78 off just 45 balls. In response, Australia's\n",
      "chase faltered early as they lost key wickets to India's fast bowlers.\n",
      "\n",
      "(Article length: 66 words. Target summary length: 30-200 tokens)\n",
      "\n",
      "================================================================================\n",
      "GENERATED ENGLISH SUMMARY:\n",
      "================================================================================\n",
      "The Indian team secured a decisive victory over Australia in the final match of\n",
      "the T20 series, winning by a margin of 35 runs in Bengaluru. Batting first,\n",
      "India posted a competitive total of 198 for 4, thanks to a powerful half-century\n",
      "from captain Suryakumar Yadav. However, Australia's chase faltered early as they\n",
      "lost key wickets to India's fast bowlers.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY:\n",
      "================================================================================\n",
      "    T20         \n",
      " ,  35        ,   4  \n",
      " 198          , \n",
      " 45   78    ,       \n",
      "       \n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "article_to_test = \"\"\"\n",
    "India secured a decisive victory over Australia in the final match of the T20 series, winning by a margin of 35 runs in Bengaluru. Batting first, India posted a competitive total of 198 for 4, thanks to a powerful half-century from captain Suryakumar Yadav, who scored 78 off just 45 balls. In response, Australia's chase faltered early as they lost key wickets to India's fast bowlers.\n",
    "\"\"\"\n",
    "\n",
    "generate_summary(article_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e774a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from mbart-large-50-cnn-summarizer-v14/final_model...\n",
      "--- Model Ready ---\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 1/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "India secured a decisive victory over Australia in the final match of the T20\n",
      "series, winning by a margin of 35 runs in Bengaluru. Batting first, India posted\n",
      "a competitive total of 198 for 4, thanks to a powerful half-century from captain\n",
      "Suryakumar Yadav, who scored 78 off just 45 balls. In response, Australia's\n",
      "chase faltered early as they lost key wickets to India's fast bowlers.\n",
      "\n",
      "(Article length: 66 words. Target summary length: 30-200 tokens)\n",
      "\n",
      "------------------------------ ENGLISH SUMMARY ------------------------------\n",
      "The Indian team secured a decisive victory over Australia in the final match of\n",
      "the T20 series, winning by a margin of 35 runs in Bengaluru. Batting first,\n",
      "India posted a competitive total of 198 for 4, thanks to a powerful half-century\n",
      "from captain Suryakumar Yadav. However, Australia's chase faltered early as they\n",
      "lost key wickets to India's fast bowlers.\n",
      "\n",
      "------------------------------ HINDI SUMMARY --------------------------------\n",
      "    T20         \n",
      " ,  35        ,   4  \n",
      " 198          , \n",
      " 45   78    ,       \n",
      "       \n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 2/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "A major tech firm today unveiled its latest flagship smartphone, featuring a\n",
      "revolutionary new camera system with 'periscope zoom' technology. The device,\n",
      "which also boasts a foldable OLED display and 5G connectivity, aims to redefine\n",
      "the premium mobile market. Analysts are optimistic, noting that the innovative\n",
      "camera could be a key differentiator in a crowded field. However, concerns\n",
      "remain about the device's high price point, which exceeds $1,500, potentially\n",
      "limiting its mass-market appeal despite the advanced features.\n",
      "\n",
      "(Article length: 77 words. Target summary length: 30-200 tokens)\n",
      "\n",
      "------------------------------ ENGLISH SUMMARY ------------------------------\n",
      "A major tech firm has unveiled its latest flagship smartphone, featuring a\n",
      "revolutionary new camera system with 'periscope zoom' technology. The device\n",
      "also features a foldable OLED display and 5G connectivity, aiming to redefine\n",
      "the premium mobile market. Analysts are optimistic that the innovative camera\n",
      "could be a key differentiator in a crowded field. However, concerns persist\n",
      "about its high $1,500 price point, potentially limiting its mass-market appeal\n",
      "despite advanced features.\n",
      "\n",
      "------------------------------ HINDI SUMMARY --------------------------------\n",
      "           , \n",
      "''         premium  \n",
      "               5\n",
      "            \n",
      "           \n",
      ",   ,  $1,500   ,      \n",
      "      \n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 3/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "NASA's Artemis program achieved a major milestone this week as the Orion\n",
      "spacecraft successfully completed its uncrewed flyby of the Moon and is now on\n",
      "its return trajectory to Earth. The mission, Artemis I, is a critical test of\n",
      "the agency's deep space exploration systems, including the powerful Space Launch\n",
      "System (SLS) rocket and the Orion crew capsule. During its journey, Orion\n",
      "traveled farther from Earth than any human-rated spacecraft has ever gone\n",
      "before, capturing stunning high-resolution images of the lunar surface and Earth\n",
      "from a distance. The spacecraft's heat shield will face its most extreme test\n",
      "during re-entry, when it will endure temperatures of nearly 5,000 degrees\n",
      "Fahrenheit while traveling at over 24,000 miles per hour. A successful\n",
      "splashdown in the Pacific Ocean will pave the way for Artemis II, the program's\n",
      "first crewed mission, which will send astronauts on a similar lunar flyby,\n",
      "further cementing humanity's path back to the Moon and, eventually, to Mars.\n",
      "\n",
      "(Article length: 157 words. Target summary length: 30-200 tokens)\n",
      "\n",
      "------------------------------ ENGLISH SUMMARY ------------------------------\n",
      "A major milestone for NASA's Artemis program was achieved by the Orion\n",
      "spacecraft, which successfully completed its uncrewed flyby of the Moon and is\n",
      "now on its return trajectory to Earth. The mission, Artemis I, is a critical\n",
      "test of the agency's deep space exploration systems, including the powerful\n",
      "Space Launch System (SLS) rocket and the Orion crew capsule. During its journey,\n",
      "Orion traveled farther than any human-rated spacecraft before, capturing\n",
      "stunning high-resolution images of the lunar surface and Earth from a distance.\n",
      "It will endure temperatures of nearly 5,000 degrees Fahrenheit during re-entry,\n",
      "traveling over 24,000 miles per hour. A successful splashdown in the Pacific\n",
      "Ocean will lead to Artemis II, the program's first crewed mission that will send\n",
      "astronauts on a similar lunar flyby, further cementing humanity's path back to\n",
      "\n",
      "------------------------------ HINDI SUMMARY --------------------------------\n",
      "           ,  \n",
      "              \n",
      "             ,\n",
      "    ()       ,  \n",
      "       ,       \n",
      " -    ,  24,000      \n",
      " 5,000          , \n",
      "          ,  \n",
      "II         \n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 4/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "Global stock markets experienced a volatile week as central banks around the\n",
      "world signaled a more aggressive stance on combating inflation. The US Federal\n",
      "Reserve hinted at larger-than-expected interest rate hikes, causing a sell-off\n",
      "in technology stocks and growth-oriented sectors. Meanwhile, the European\n",
      "Central Bank is facing pressure to act as energy prices continue to soar across\n",
      "the continent, impacting both consumer spending and industrial production.\n",
      "Investors are now closely watching upcoming inflation data and corporate\n",
      "earnings reports for signs of a potential economic slowdown. Experts suggest a\n",
      "period of uncertainty is likely to continue as markets digest the new reality of\n",
      "tighter monetary policy.\n",
      "\n",
      "(Article length: 105 words. Target summary length: 30-200 tokens)\n",
      "\n",
      "------------------------------ ENGLISH SUMMARY ------------------------------\n",
      "The global stock markets experienced a volatile week as central banks adopted a\n",
      "more aggressive stance on combating inflation. The US Federal Reserve hinted at\n",
      "larger-than-expected interest rate hikes, causing a sell-off in technology\n",
      "stocks and growth-oriented sectors. Meanwhile, the European Central Bank is\n",
      "facing pressure to act as energy prices continue to soar across the continent,\n",
      "impacting both consumer spending and industrial production. Investors are now\n",
      "closely monitoring future inflation data and corporate earnings for signs of a\n",
      "potential economic slowdown. Experts suggest a period of uncertainty persists as\n",
      "markets digest the new reality of tighter monetary policy.\n",
      "\n",
      "------------------------------ HINDI SUMMARY --------------------------------\n",
      "     -   ,     \n",
      "              \n",
      "    -       \n",
      ",      ,      \n",
      "             \n",
      "   ,            \n",
      "       ,    \n",
      "       \n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 5/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "Diplomats from several nations met in Geneva to resume peace talks aimed at\n",
      "resolving a long-standing regional conflict. The negotiations, which had been\n",
      "stalled for months, were restarted following a recent de-escalation of\n",
      "hostilities. Observers are cautiously optimistic, but acknowledge that\n",
      "significant political hurdles remain. The primary goal of the current round of\n",
      "talks is to establish a lasting ceasefire and facilitate the delivery of\n",
      "humanitarian aid to affected civilian populations.\n",
      "\n",
      "(Article length: 71 words. Target summary length: 30-200 tokens)\n",
      "\n",
      "------------------------------ ENGLISH SUMMARY ------------------------------\n",
      "A series of diplomats from several nations met in Geneva to resume peace talks\n",
      "aimed at resolving a long-standing regional conflict. The negotiations, which\n",
      "had been stalled for months, were restarted following a recent de-escalation of\n",
      "hostilities. While cautiously optimistic, observers acknowledge significant\n",
      "political hurdles remain. The primary goal of the current round of talks is to\n",
      "establish a lasting ceasefire and facilitate humanitarian aid delivery to\n",
      "affected civilian populations.\n",
      "\n",
      "------------------------------ HINDI SUMMARY --------------------------------\n",
      "               ,\n",
      "               \n",
      "            \n",
      "            ,\n",
      "         \n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 6/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "Researchers have published a landmark study in the journal 'Nature' detailing a\n",
      "new gene-editing technique that shows promise in correcting genetic defects\n",
      "responsible for certain inherited diseases. The method, which uses a modified\n",
      "version of the CRISPR-Cas9 system, demonstrated a significantly higher precision\n",
      "and lower rate of off-target mutations in lab experiments compared to existing\n",
      "technologies. The study focused on a specific mutation linked to cystic\n",
      "fibrosis, and the results in human cell cultures were highly encouraging. While\n",
      "the research is still in its early stages and human trials are years away, the\n",
      "scientific community is hailing it as a potential breakthrough. The technique's\n",
      "improved safety profile could overcome some of the major hurdles that have\n",
      "slowed the clinical application of gene therapy. However, the scientists\n",
      "involved urge caution, emphasizing that extensive further research is required\n",
      "to validate the findings and ensure the long-term safety and efficacy of this\n",
      "new approach before it can be considered for patient treatment.\n",
      "\n",
      "(Article length: 159 words. Target summary length: 30-200 tokens)\n",
      "\n",
      "------------------------------ ENGLISH SUMMARY ------------------------------\n",
      "The journal 'Nature' has published a landmark study detailing a new gene-editing\n",
      "technique that demonstrates promise in correcting genetic defects linked to\n",
      "certain inherited diseases. Using a modified version of the CRISPR-Cas9 system,\n",
      "the method demonstrated significantly higher precision and lower off-target\n",
      "mutation rates in lab experiments compared to existing technologies. Focusing on\n",
      "a specific cystic fibrosis mutation, the results in human cell cultures were\n",
      "highly encouraging. While the research is still in its early stages and human\n",
      "trials are years away, the scientific community views it as a potential\n",
      "breakthrough, potentially overcoming major hurdles hindering clinical\n",
      "application of gene therapy. However, scientists caution that extensive further\n",
      "research is required to validate these findings and ensure long-term safety and\n",
      "efficacy before approval for patient treatment.\n",
      "\n",
      "------------------------------ HINDI SUMMARY --------------------------------\n",
      "  ''         , \n",
      "  -         CRISPR-Cas9  \n",
      "      ,    (cystic fibrosis)\n",
      "               \n",
      "            -\n",
      "            \n",
      "              ,\n",
      "       ,    \n",
      "        , \n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 7/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "A new international report has found that the rate of deforestation in the\n",
      "Amazon rainforest accelerated by nearly 20% in the last year, reaching its\n",
      "highest level in over a decade. The report, which uses satellite data,\n",
      "attributes the surge to increased illegal logging, agricultural expansion, and\n",
      "mining activities. Environmental groups are calling for urgent government\n",
      "intervention and stronger enforcement of existing protection laws. The Amazon is\n",
      "a critical global ecosystem, playing a vital role in regulating the planet's\n",
      "climate by absorbing vast amounts of carbon dioxide. Scientists warn that\n",
      "continued deforestation could push the rainforest towards a tipping point, where\n",
      "it would transition into a drier, savanna-like state, with devastating\n",
      "consequences for global biodiversity and climate patterns.\n",
      "\n",
      "(Article length: 118 words. Target summary length: 30-200 tokens)\n",
      "\n",
      "------------------------------ ENGLISH SUMMARY ------------------------------\n",
      "A new international report indicates that the rate of deforestation in the\n",
      "Amazon rainforest has accelerated by nearly 20% in the last year, reaching its\n",
      "highest level in over a decade. The report attributes this surge to increased\n",
      "illegal logging, agricultural expansion, and mining activities. Environmental\n",
      "groups are urging government intervention and stronger enforcement of existing\n",
      "protection laws as the Amazon is a critical global ecosystem, crucial for\n",
      "regulating global climate by absorbing vast amounts of carbon dioxide.\n",
      "Scientists warn that continued deforestration could lead to a \"dryr, savanna-\n",
      "like state\" transition, potentially impacting global biodiversity and climate\n",
      "patterns.\n",
      "\n",
      "------------------------------ HINDI SUMMARY --------------------------------\n",
      "               \n",
      "20%    ,              \n",
      "     ,   ,    \n",
      "             \n",
      "            \n",
      "           \n",
      "             \n",
      "          \n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 8/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "The highly anticipated sequel to a blockbuster science fiction film has\n",
      "officially begun production, with the studio releasing the first on-set photo.\n",
      "The image features the return of the original cast members alongside several new\n",
      "additions. The director has promised that the sequel will expand the universe in\n",
      "exciting ways while honoring the spirit of the first film. The movie is\n",
      "currently slated for a summer release next year.\n",
      "\n",
      "(Article length: 69 words. Target summary length: 30-200 tokens)\n",
      "\n",
      "------------------------------ ENGLISH SUMMARY ------------------------------\n",
      "A highly anticipated sequel to a blockbuster science fiction film has officially\n",
      "begun production, with the studio releasing its first on-set photo. This image\n",
      "features the return of the original cast members alongside several new\n",
      "additions. The director has promised that the sequel will expand the universe in\n",
      "exciting ways while honoring the spirit of the first film. The movie is slated\n",
      "for a summer release next year.\n",
      "\n",
      "------------------------------ HINDI SUMMARY --------------------------------\n",
      "           \n",
      "    ,   -         \n",
      "               \n",
      "               \n",
      "        \n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 9/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "A legacy automaker has announced a massive $50 billion investment into its\n",
      "electric vehicle (EV) division, signaling a dramatic acceleration of its\n",
      "transition away from internal combustion engines. The company plans to launch 15\n",
      "new all-electric models over the next three years, including sedans, SUVs, and a\n",
      "pickup truck. A key part of the strategy involves building several new\n",
      "\"gigafactories\" for battery production in North America and Europe to secure its\n",
      "supply chain. This move is seen by industry experts as a direct response to the\n",
      "growing dominance of EV-native companies and increasing regulatory pressure to\n",
      "reduce emissions.\n",
      "\n",
      "(Article length: 98 words. Target summary length: 30-200 tokens)\n",
      "\n",
      "------------------------------ ENGLISH SUMMARY ------------------------------\n",
      "A legacy automaker has announced a significant $50 billion investment in its\n",
      "electric vehicle (EV) division, signaling a dramatic acceleration of its\n",
      "transition away from internal combustion engines. The company plans to launch 15\n",
      "new all-electric models over the next three years, including sedans, SUVs, and a\n",
      "pickup truck. This strategy involves building several new \"gigafactories\" for\n",
      "battery production in North America and Europe to secure its supply chain.\n",
      "Industry experts view this move as a direct response to the growing dominance of\n",
      "EV-native companies and increasing regulatory pressure to reduce emissions.\n",
      "\n",
      "------------------------------ HINDI SUMMARY --------------------------------\n",
      "       ()    $50  \n",
      "  ,      15  -  \n",
      " ,  ,          \n",
      "        \"\"    \n",
      " ,  -         \n",
      "             \n",
      " EV-           \n",
      "    \n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 10/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "A new trade agreement between two major economic blocs was signed this week,\n",
      "aiming to reduce tariffs and streamline regulations across dozens of industries.\n",
      "The pact, which covers everything from agricultural goods to digital services,\n",
      "is the culmination of nearly five years of intense negotiations. Proponents\n",
      "argue that the deal will boost economic growth, lower consumer prices, and\n",
      "create hundreds of thousands of new jobs by fostering closer integration and\n",
      "simplifying cross-border commerce. However, the agreement has also faced\n",
      "criticism from labor unions and environmental groups, who argue that it lacks\n",
      "sufficient protections for workers' rights and fails to implement strong\n",
      "environmental standards. They warn that the deal could lead to a \"race to the\n",
      "bottom\" as companies relocate to regions with lower wages and weaker\n",
      "regulations. The signatory governments have defended the pact, stating that it\n",
      "includes robust chapters on labor and the environment and will be subject to\n",
      "regular reviews to ensure compliance and address any emerging issues.\n",
      "\n",
      "(Article length: 160 words. Target summary length: 30-200 tokens)\n",
      "\n",
      "------------------------------ ENGLISH SUMMARY ------------------------------\n",
      "A new trade agreement between two major economic blocs aims to reduce tariffs\n",
      "and streamline regulations across dozens of industries, covering everything from\n",
      "agricultural goods to digital services. Over five years of intense negotiations,\n",
      "the pact aims to boost economic growth, lower consumer prices, and create\n",
      "hundreds of thousands of new jobs by fostering closer integration and\n",
      "simplifying cross-border commerce. However, it faces criticism from labor unions\n",
      "and environmental groups, who argue it lacks adequate protections for workers'\n",
      "rights and fails to implement strong environmental standards, potentially\n",
      "leading to a \"race to the bottom\" as companies relocate to regions with lower\n",
      "wages and weaker regulations. The signatory governments have defended the\n",
      "agreement, stating that it includes robust chapters on labor and the environment\n",
      "and requires regular compliance reviews to address emerging issues.\n",
      "\n",
      "------------------------------ HINDI SUMMARY --------------------------------\n",
      "             , \n",
      "              \n",
      "        ,      \n",
      "             ,\n",
      "            , \n",
      "             \n",
      "             \n",
      ",  \"  \"         \n",
      "              \n",
      "         \n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import numpy as np\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_PATH = \"mbart-large-50-cnn-summarizer-v14/final_model\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Loads the model and tokenizer.\"\"\"\n",
    "    global model, tokenizer\n",
    "    if model and tokenizer:\n",
    "        print(\"Model already loaded.\")\n",
    "        return\n",
    "    try:\n",
    "        print(f\"Loading model from {MODEL_PATH}...\")\n",
    "        model = MBartForConditionalGeneration.from_pretrained(MODEL_PATH).to(DEVICE)\n",
    "        tokenizer = MBart50TokenizerFast.from_pretrained(MODEL_PATH)\n",
    "        print(\"--- Model Ready ---\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        model, tokenizer = None, None\n",
    "\n",
    "def generate_summary(article_text):\n",
    "    \"\"\"\n",
    "    Generates and prints high-quality, abstractive summaries using the final optimized parameters.\n",
    "    \"\"\"\n",
    "    if not model or not tokenizer:\n",
    "        print(\"Model not loaded. Please run load_model() first.\")\n",
    "        return\n",
    "\n",
    "    input_word_count = len(article_text.split())\n",
    "    \n",
    "    # --- DEFINITIVE: Final, Robust Length Calculation ---\n",
    "    # Give the model a generous window to work with, which prevents cut-off sentences.\n",
    "    min_len = 30\n",
    "    max_len = 500 # A generous max length. `early_stopping` will ensure it stops naturally.\n",
    "\n",
    "    # --- DEFINITIVE: Finely-Tuned Beam Search for Factual, Complete Summaries ---\n",
    "    gen_kwargs = {\n",
    "        \"num_beams\": 12,\n",
    "        \"length_penalty\": 2.0,            # Strongly encourages the model to generate complete sentences.\n",
    "        \"repetition_penalty\": 2.5,        # Strongly discourages repeating words or phrases.\n",
    "        \"no_repeat_ngram_size\": 3,        # Prevents repeating sequences of 3 words.\n",
    "        \"do_sample\": False,               # CRITICAL: Disables sampling to prioritize factual accuracy.\n",
    "        \"early_stopping\": True,           # Finishes generation when all beams have reached the end token.\n",
    "        \"min_length\": min_len,\n",
    "        \"max_length\": max_len,\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SOURCE ARTICLE:\")\n",
    "    print(textwrap.fill(article_text, width=80))\n",
    "    print(f\"\\n(Article length: {input_word_count} words. Target summary length: {min_len}-{max_len} tokens)\")\n",
    "\n",
    "    tokenizer.src_lang = \"en_XX\"\n",
    "    inputs = tokenizer(article_text, return_tensors=\"pt\", max_length=1024, truncation=True).to(DEVICE)\n",
    "\n",
    "    eng_summary_ids = model.generate(inputs.input_ids, forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"], **gen_kwargs)\n",
    "    english_summary = tokenizer.decode(eng_summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    hin_summary_ids = model.generate(inputs.input_ids, forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"], **gen_kwargs)\n",
    "    hindi_summary = tokenizer.decode(hin_summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*30 + \" ENGLISH SUMMARY \" + \"-\"*30)\n",
    "    print(textwrap.fill(english_summary, width=100))\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*30 + \" HINDI SUMMARY \" + \"-\"*32)\n",
    "    print(textwrap.fill(hindi_summary, width=100))\n",
    "    print(\"=\"*80)\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Defines a list of diverse news articles and generates summaries for each.\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. --- Load the model once at the beginning ---\n",
    "    load_model()\n",
    "\n",
    "    # 2. --- Define a list of articles to test ---\n",
    "    articles_to_test = [\n",
    "        # --- Short Article (Sports) ---\n",
    "        \"\"\"India secured a decisive victory over Australia in the final match of the T20 series, winning by a margin of 35 runs in Bengaluru. Batting first, India posted a competitive total of 198 for 4, thanks to a powerful half-century from captain Suryakumar Yadav, who scored 78 off just 45 balls. In response, Australia's chase faltered early as they lost key wickets to India's fast bowlers.\"\"\",\n",
    "        \n",
    "        # --- Medium Article (Technology) ---\n",
    "        \"\"\"A major tech firm today unveiled its latest flagship smartphone, featuring a revolutionary new camera system with 'periscope zoom' technology. The device, which also boasts a foldable OLED display and 5G connectivity, aims to redefine the premium mobile market. Analysts are optimistic, noting that the innovative camera could be a key differentiator in a crowded field. However, concerns remain about the device's high price point, which exceeds $1,500, potentially limiting its mass-market appeal despite the advanced features.\"\"\",\n",
    "        \n",
    "        # --- Long Article (Science/Space) ---\n",
    "        \"\"\"NASA's Artemis program achieved a major milestone this week as the Orion spacecraft successfully completed its uncrewed flyby of the Moon and is now on its return trajectory to Earth. The mission, Artemis I, is a critical test of the agency's deep space exploration systems, including the powerful Space Launch System (SLS) rocket and the Orion crew capsule. During its journey, Orion traveled farther from Earth than any human-rated spacecraft has ever gone before, capturing stunning high-resolution images of the lunar surface and Earth from a distance. The spacecraft's heat shield will face its most extreme test during re-entry, when it will endure temperatures of nearly 5,000 degrees Fahrenheit while traveling at over 24,000 miles per hour. A successful splashdown in the Pacific Ocean will pave the way for Artemis II, the program's first crewed mission, which will send astronauts on a similar lunar flyby, further cementing humanity's path back to the Moon and, eventually, to Mars.\"\"\",\n",
    "        \n",
    "        # --- Medium Article (Business/Finance) ---\n",
    "        \"\"\"Global stock markets experienced a volatile week as central banks around the world signaled a more aggressive stance on combating inflation. The US Federal Reserve hinted at larger-than-expected interest rate hikes, causing a sell-off in technology stocks and growth-oriented sectors. Meanwhile, the European Central Bank is facing pressure to act as energy prices continue to soar across the continent, impacting both consumer spending and industrial production. Investors are now closely watching upcoming inflation data and corporate earnings reports for signs of a potential economic slowdown. Experts suggest a period of uncertainty is likely to continue as markets digest the new reality of tighter monetary policy.\"\"\",\n",
    "\n",
    "        # --- Short Article (World News) ---\n",
    "        \"\"\"Diplomats from several nations met in Geneva to resume peace talks aimed at resolving a long-standing regional conflict. The negotiations, which had been stalled for months, were restarted following a recent de-escalation of hostilities. Observers are cautiously optimistic, but acknowledge that significant political hurdles remain. The primary goal of the current round of talks is to establish a lasting ceasefire and facilitate the delivery of humanitarian aid to affected civilian populations.\"\"\",\n",
    "        \n",
    "        # --- Long Article (Health/Science) ---\n",
    "        \"\"\"Researchers have published a landmark study in the journal 'Nature' detailing a new gene-editing technique that shows promise in correcting genetic defects responsible for certain inherited diseases. The method, which uses a modified version of the CRISPR-Cas9 system, demonstrated a significantly higher precision and lower rate of off-target mutations in lab experiments compared to existing technologies. The study focused on a specific mutation linked to cystic fibrosis, and the results in human cell cultures were highly encouraging. While the research is still in its early stages and human trials are years away, the scientific community is hailing it as a potential breakthrough. The technique's improved safety profile could overcome some of the major hurdles that have slowed the clinical application of gene therapy. However, the scientists involved urge caution, emphasizing that extensive further research is required to validate the findings and ensure the long-term safety and efficacy of this new approach before it can be considered for patient treatment.\"\"\",\n",
    "\n",
    "        # --- Medium Article (Environment) ---\n",
    "        \"\"\"A new international report has found that the rate of deforestation in the Amazon rainforest accelerated by nearly 20% in the last year, reaching its highest level in over a decade. The report, which uses satellite data, attributes the surge to increased illegal logging, agricultural expansion, and mining activities. Environmental groups are calling for urgent government intervention and stronger enforcement of existing protection laws. The Amazon is a critical global ecosystem, playing a vital role in regulating the planet's climate by absorbing vast amounts of carbon dioxide. Scientists warn that continued deforestation could push the rainforest towards a tipping point, where it would transition into a drier, savanna-like state, with devastating consequences for global biodiversity and climate patterns.\"\"\",\n",
    "\n",
    "        # --- Short Article (Entertainment) ---\n",
    "        \"\"\"The highly anticipated sequel to a blockbuster science fiction film has officially begun production, with the studio releasing the first on-set photo. The image features the return of the original cast members alongside several new additions. The director has promised that the sequel will expand the universe in exciting ways while honoring the spirit of the first film. The movie is currently slated for a summer release next year.\"\"\",\n",
    "\n",
    "        # --- Medium Article (Automotive/Tech) ---\n",
    "        \"\"\"A legacy automaker has announced a massive $50 billion investment into its electric vehicle (EV) division, signaling a dramatic acceleration of its transition away from internal combustion engines. The company plans to launch 15 new all-electric models over the next three years, including sedans, SUVs, and a pickup truck. A key part of the strategy involves building several new \"gigafactories\" for battery production in North America and Europe to secure its supply chain. This move is seen by industry experts as a direct response to the growing dominance of EV-native companies and increasing regulatory pressure to reduce emissions.\"\"\",\n",
    "\n",
    "        # --- Long Article (International Relations) ---\n",
    "        \"\"\"A new trade agreement between two major economic blocs was signed this week, aiming to reduce tariffs and streamline regulations across dozens of industries. The pact, which covers everything from agricultural goods to digital services, is the culmination of nearly five years of intense negotiations. Proponents argue that the deal will boost economic growth, lower consumer prices, and create hundreds of thousands of new jobs by fostering closer integration and simplifying cross-border commerce. However, the agreement has also faced criticism from labor unions and environmental groups, who argue that it lacks sufficient protections for workers' rights and fails to implement strong environmental standards. They warn that the deal could lead to a \"race to the bottom\" as companies relocate to regions with lower wages and weaker regulations. The signatory governments have defended the pact, stating that it includes robust chapters on labor and the environment and will be subject to regular reviews to ensure compliance and address any emerging issues.\"\"\",\n",
    "    ]\n",
    "\n",
    "    # 3. --- Loop through the articles and generate summaries ---\n",
    "    for i, article in enumerate(articles_to_test, 1):\n",
    "        print(f\"\\n\\n--- SUMMARIZING ARTICLE {i}/{len(articles_to_test)} ---\")\n",
    "        generate_summary(article)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9a93613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE (truncated):\n",
      "================================================================================\n",
      "A landmark international treaty to combat plastic pollution has been agreed upon\n",
      "by delegates from over 170 countries at a United Nations Environment Assembly\n",
      "session held in Nairobi. Hailed as the most significant environmental pact since\n",
      "the Paris Agreement, the resolution establishes an Intergovernmental Negotiating\n",
      "Committee (INC) tasked with drafting a legally binding agreement by the end of\n",
      "2026. The future treaty aims to address the full lifecycle of plastic, from its\n",
      "production and design to its disposal and recycling. The negotiations were\n",
      "complex, with debates centering on whether the treaty should focus solely on\n",
      "plastic waste management or include caps on virgin plastic production. Major\n",
      "plastic-producing nations and fossil fuel companies had advocated for a focus on\n",
      "recycling, while a coalition of environmental groups and many developing nations\n",
      "pushed for stricter controls on production itself. The final resolution provides\n",
      "a broad mandate for the INC to consider all opti...\n",
      "\n",
      "(Article length: 150 words. Target summary length: 30-510 tokens)\n",
      "\n",
      "================================================================================\n",
      "GENERATED ENGLISH SUMMARY:\n",
      "================================================================================\n",
      "A landmark international treaty to combat plastic pollution has been agreed upon\n",
      "by delegates from over 170 countries at a United Nations Environment Assembly\n",
      "session in Nairobi. Hailed as the most significant environmental pact since the\n",
      "Paris Agreement, the resolution establishes an Intergovernmental Negotiating\n",
      "Committee (INC) tasked with drafting a legally binding agreement by the end of\n",
      "2026. The future treaty aims to address the full lifecycle of plastic, from its\n",
      "production and design to disposal and recycling. The negotiations were complex,\n",
      "with debates centering on whether the treaty should focus solely on plastic\n",
      "waste management or include caps on virgin plastic production. Major plastic-\n",
      "producing nations and fossil fuel companies had advocated for stricter controls\n",
      "on recycling, while environmental groups and developing nations pushed for more\n",
      "controls. The final resolution provides a broad mandate for the INC to consider\n",
      "all options.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY:\n",
      "================================================================================\n",
      "     170       \n",
      "              \n",
      "      2026       binding   \n",
      " ,             \n",
      "  (INC)      ,     2026    \n",
      "           ,      \n",
      "             \n",
      "            \n",
      "   ,              \n",
      ",           INC   \n",
      "       \n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "article_to_test = \"\"\"A landmark international treaty to combat plastic pollution has been agreed upon by delegates from over 170 countries at a United Nations Environment Assembly session held in Nairobi. Hailed as the most significant environmental pact since the Paris Agreement, the resolution establishes an Intergovernmental Negotiating Committee (INC) tasked with drafting a legally binding agreement by the end of 2026. The future treaty aims to address the full lifecycle of plastic, from its production and design to its disposal and recycling. The negotiations were complex, with debates centering on whether the treaty should focus solely on plastic waste management or include caps on virgin plastic production. Major plastic-producing nations and fossil fuel companies had advocated for a focus on recycling, while a coalition of environmental groups and many developing nations pushed for stricter controls on production itself. The final resolution provides a broad mandate for the INC to consider all options.\n",
    "\"\"\"\n",
    "generate_summary(article_to_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarizer_env3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
