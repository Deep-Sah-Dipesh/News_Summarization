{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5b42332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:00,424 [INFO] - Attempting to load model using safetensors to bypass torch.load vulnerability check.\n",
      "2025-10-07 20:18:03,701 [INFO] - Model loaded successfully using safetensors.\n",
      "2025-10-07 20:18:04,413 [INFO] - --- Starting Text Sanitization & Normalization ---\n",
      "2025-10-07 20:18:05,972 [INFO] - --- Text Sanitization & Normalization Finished ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9216a5a5d0b04378961d45eb049db9fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d79d7511c2a4bfd9bf8c485e0c02f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba626404057043358374835a417e5195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\bleurt-20\\downloads\\extracted\\8db8856a80394ae84b010e83ab663d4a3ccfa244ce3d0dbe00143f73e65ff123\\BLEURT-20.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,190 [INFO] - Reading checkpoint C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\bleurt-20\\downloads\\extracted\\8db8856a80394ae84b010e83ab663d4a3ccfa244ce3d0dbe00143f73e65ff123\\BLEURT-20.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,190 [INFO] - Config file found, reading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,190 [INFO] - Will load checkpoint BLEURT-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,190 [INFO] - Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:BLEURT-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,207 [INFO] - ... name:BLEURT-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,207 [INFO] - ... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,213 [INFO] - ... max_seq_length:512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,214 [INFO] - ... vocab_file:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,217 [INFO] - ... do_lower_case:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... sp_model:sent_piece\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,219 [INFO] - ... sp_model:sent_piece\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... dynamic_seq_length:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,221 [INFO] - ... dynamic_seq_length:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,224 [INFO] - Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,226 [INFO] - Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,228 [INFO] - Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load model: C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\bleurt-20\\downloads\\extracted\\8db8856a80394ae84b010e83ab663d4a3ccfa244ce3d0dbe00143f73e65ff123\\BLEURT-20\\sent_piece.model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,230 [INFO] - Will load model: C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\bleurt-20\\downloads\\extracted\\8db8856a80394ae84b010e83ab663d4a3ccfa244ce3d0dbe00143f73e65ff123\\BLEURT-20\\sent_piece.model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SentencePiece tokenizer created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,728 [INFO] - SentencePiece tokenizer created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,728 [INFO] - Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,741 [INFO] - Loading model.\n",
      "2025-10-07 20:19:01,388 [INFO] - Fingerprint not found. Saved model loading will continue.\n",
      "2025-10-07 20:19:01,388 [INFO] - path_and_singleprint metric could not be logged. Saved model loading will continue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:19:01,391 [INFO] - BLEURT initialized.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_17504\\2307351194.py:162: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "2025-10-07 20:19:05,307 [INFO] - Starting final training (v14) from scratch with mBART-LARGE...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8304' max='8304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8304/8304 2:34:51, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.085600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.616600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.491900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.400200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.296500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.149700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.123900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.061800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.024100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.050900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.986000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.974200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.995600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.963600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.958900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.993300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.945800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.988100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.989400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.944000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.942400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.922900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.919300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.953600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.924700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.881000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.910100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.834500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.888200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.860600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.820100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.885900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.854700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.811400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.854200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.805200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.805600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.825500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.803000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>1.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.755900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>7.060700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>6.126900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>5.721900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>5.763300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>5.321000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>5.066300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>4.991300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>5.138900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>5.213900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>4.898200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>5.033400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>4.046200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>2.757000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.744800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>1.730200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.729200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>1.674800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.606600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>1.642300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.624300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>1.624300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.613500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>1.622600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.594600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>1.608400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.594300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>1.585400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.585500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>1.616000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.544400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>1.594600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.543800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>1.558200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.566900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>1.549500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.558500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>1.584400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.534700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>1.538600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.538400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>1.558900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.428600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>1.410100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.420200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>1.434700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.443000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>1.422400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.441800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>1.414000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.400900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>1.421300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>1.463000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>1.445700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.423500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>1.425700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>1.428100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>1.386000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.387500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>1.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>1.429800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>1.397700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.402300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>1.400500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>1.429000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>1.402000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.393200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>1.396200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.376000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>1.423400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.413800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>1.399800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>1.413700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>1.401300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.428600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>1.383700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>1.371000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>1.387500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.404000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>1.362000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>1.423100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>1.378100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.425900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>1.382100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>1.313000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>1.289700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.291400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>1.325800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.301100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>1.296900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.292400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>1.276200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>1.274700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>1.277500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>1.298900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>1.306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>1.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>1.293600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.305100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>1.276200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>1.303600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>1.273900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>1.266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>1.316200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>1.297500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>1.300700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>1.278900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>1.315200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.301500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>1.307900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>1.302800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>1.282400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>1.286500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>1.248500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>1.314700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>1.315200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>1.291400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7950</td>\n",
       "      <td>1.282700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.296600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>1.325400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>1.304500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8150</td>\n",
       "      <td>1.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>1.312300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>1.269300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>1.266200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\modeling_utils.py:3922: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "2025-10-07 22:53:59,299 [INFO] - Training finished. All checkpoints and logs are saved.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import os\n",
    "import unicodedata\n",
    "from datetime import datetime\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    TrainerCallback\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = \"facebook/mbart-large-50\"\n",
    "NEW_MODEL_OUTPUT_DIR = \"mbart-large-50-cnn-summarizer-v14\"\n",
    "NEW_DATA_PATH = \"../Dataset/new_large_CNN_dataset.csv\"\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 4 \n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "WEIGHT_DECAY = 0.3\n",
    "NUM_BEAMS_EVAL = 6\n",
    "MAX_SUMMARY_LENGTH_EVAL = 256\n",
    "METRIC_FOR_BEST_MODEL = \"bleurt_f1\"\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"mbart_large_training_log_v14_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "    handlers=[logging.FileHandler(log_filename), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "class ZeroLossCallback(TrainerCallback):\n",
    "    \"\"\"A callback that stops training if the training loss is zero to prevent wasted resources.\"\"\"\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None and 'loss' in logs and logs['loss'] == 0.0:\n",
    "            logging.error(\"CRITICAL: Training loss is zero. This indicates a data issue. Stopping training.\")\n",
    "            control.should_training_stop = True\n",
    "\n",
    "def sanitize_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    return text.replace('\"\"', '\"').strip()\n",
    "\n",
    "def normalize_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    return ' '.join(unicodedata.normalize('NFKC', text).split())\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        tokenizer = MBart50TokenizerFast.from_pretrained(BASE_MODEL_PATH)\n",
    "        \n",
    "        # --- THE DEFINITIVE FIX: Use safetensors to bypass the security check ---\n",
    "        logging.info(\"Attempting to load model using safetensors to bypass torch.load vulnerability check.\")\n",
    "        model = MBartForConditionalGeneration.from_pretrained(BASE_MODEL_PATH, use_safetensors=True)\n",
    "        logging.info(\"Model loaded successfully using safetensors.\")\n",
    "        # --------------------------------------------------------------------\n",
    "\n",
    "        df_new = pd.read_csv(NEW_DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        df_new.dropna(subset=['raw_news_article', 'english_summary', 'hindi_summary'], inplace=True)\n",
    "        \n",
    "        logging.info(\"--- Starting Text Sanitization & Normalization ---\")\n",
    "        for col in ['raw_news_article', 'english_summary', 'hindi_summary']:\n",
    "            df_new[col] = df_new[col].apply(sanitize_text).apply(normalize_text)\n",
    "        logging.info(\"--- Text Sanitization & Normalization Finished ---\")\n",
    "        \n",
    "        raw_dataset = Dataset.from_pandas(df_new)\n",
    "\n",
    "        def format_dataset_mbart(batch):\n",
    "            inputs, targets, langs = [], [], []\n",
    "            for article, eng_summary, hin_summary in zip(\n",
    "                batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']\n",
    "            ):\n",
    "                if isinstance(article, str) and article:\n",
    "                    inputs.append(article)\n",
    "                    targets.append(eng_summary)\n",
    "                    langs.append(\"en_XX\")\n",
    "                    inputs.append(article)\n",
    "                    targets.append(hin_summary)\n",
    "                    langs.append(\"hi_IN\")\n",
    "            return {'article': inputs, 'summary': targets, 'target_lang': langs}\n",
    "\n",
    "        processed_dataset = raw_dataset.map(\n",
    "            format_dataset_mbart, batched=True, remove_columns=raw_dataset.column_names\n",
    "        )\n",
    "        \n",
    "        train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        final_datasets = DatasetDict({\n",
    "            'train': train_test_split['train'],\n",
    "            'test': train_test_split['test']\n",
    "        })\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            tokenizer.src_lang = \"en_XX\"\n",
    "            model_inputs = tokenizer(examples['article'], max_length=1024, truncation=True)\n",
    "            \n",
    "            labels_batch = []\n",
    "            for i in range(len(examples['summary'])):\n",
    "                tokenizer.tgt_lang = examples['target_lang'][i]\n",
    "                labels = tokenizer(\n",
    "                    text_target=examples['summary'][i], \n",
    "                    max_length=MAX_SUMMARY_LENGTH_EVAL, \n",
    "                    truncation=True\n",
    "                )\n",
    "                labels_batch.append(labels['input_ids'])\n",
    "            \n",
    "            model_inputs[\"labels\"] = labels_batch\n",
    "            return model_inputs\n",
    "\n",
    "        tokenized_datasets = final_datasets.map(tokenize_function, batched=True, remove_columns=['article', 'summary', 'target_lang'])\n",
    "        \n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "        bleurt_metric = evaluate.load(\"bleurt\", \"bleurt-20\")\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "            bleurt_result = bleurt_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "            \n",
    "            result = {\n",
    "                \"rouge1\": rouge_result[\"rouge1\"], \"rouge2\": rouge_result[\"rouge2\"],\n",
    "                \"rougeL\": rouge_result[\"rougeL\"], \"bleurt_f1\": np.mean(bleurt_result[\"scores\"])\n",
    "            }\n",
    "            return {k: round(v * 100, 4) for k, v in result.items()}\n",
    "\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=NEW_MODEL_OUTPUT_DIR,\n",
    "            num_train_epochs=NUM_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            logging_dir=f\"{NEW_MODEL_OUTPUT_DIR}/logs\",\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=50,\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=NUM_EPOCHS,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            load_best_model_at_end=False,\n",
    "            report_to=\"tensorboard\",\n",
    "            generation_max_length=MAX_SUMMARY_LENGTH_EVAL,\n",
    "            generation_num_beams=NUM_BEAMS_EVAL,\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[ZeroLossCallback()]\n",
    "        )\n",
    "\n",
    "        logging.info(\"Starting final training (v14) from scratch with mBART-LARGE...\")\n",
    "        trainer.train()\n",
    "        logging.info(\"Training finished. All checkpoints and logs are saved.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during the main process: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6f813c",
   "metadata": {},
   "source": [
    "Selecting best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98b46ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:20:41,148 [INFO] - Attempting to find best model in: mbart-large-50-cnn-summarizer-v14\n",
      "2025-10-07 23:20:41,163 [INFO] - Contents of 'mbart-large-50-cnn-summarizer-v14': ['checkpoint-2076', 'checkpoint-4152', 'checkpoint-6228', 'checkpoint-8304', 'logs']\n",
      "2025-10-07 23:20:41,164 [WARNING] - Could not find any evaluation metric logs.\n",
      "2025-10-07 23:20:41,164 [WARNING] - Defaulting to the LAST saved checkpoint as the best model.\n",
      "2025-10-07 23:20:41,164 [INFO] - Identified last checkpoint: mbart-large-50-cnn-summarizer-v14\\checkpoint-8304\n",
      "2025-10-07 23:20:41,164 [INFO] - --- Model Identified for Saving ---\n",
      "2025-10-07 23:20:41,164 [INFO] - Checkpoint: mbart-large-50-cnn-summarizer-v14\\checkpoint-8304\n",
      "2025-10-07 23:22:01,049 [INFO] - Successfully copied best model to: mbart-large-50-cnn-summarizer-v14\\final_model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Configuration: ---\n",
    "OUTPUT_DIR = \"mbart-large-50-cnn-summarizer-v14\" \n",
    "METRIC_NAME = \"bleurt_f1\"\n",
    "# -------------------------------------------------\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"select_best_model_log_v14_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "    handlers=[logging.FileHandler(log_filename), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "def find_and_save_best_checkpoint(output_dir, metric_name):\n",
    "    \"\"\"\n",
    "    Finds and saves the best model checkpoint from a training run.\n",
    "    If no evaluation metrics are found, it defaults to saving the last available checkpoint.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Attempting to find best model in: {output_dir}\")\n",
    "\n",
    "        if not os.path.isdir(output_dir):\n",
    "            logging.error(f\"FATAL: The directory '{output_dir}' does not exist.\")\n",
    "            return\n",
    "        \n",
    "        logging.info(f\"Contents of '{output_dir}': {os.listdir(output_dir)}\")\n",
    "        \n",
    "        best_metric_value = None\n",
    "        best_checkpoint_path = None\n",
    "        metric_to_check = f\"eval_{metric_name}\"\n",
    "        is_loss = 'loss' in metric_to_check.lower()\n",
    "        log_history = []\n",
    "        \n",
    "        # --- Multi-level Fallback to find logs ---\n",
    "        # 1. Check top-level state file\n",
    "        main_state_path = os.path.join(output_dir, \"trainer_state.json\")\n",
    "        if os.path.exists(main_state_path):\n",
    "            with open(main_state_path, \"r\") as f: state = json.load(f)\n",
    "            log_history = state.get(\"log_history\", [])\n",
    "\n",
    "        # 2. Check individual checkpoint state files\n",
    "        if not log_history:\n",
    "            checkpoint_dirs = [d for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")]\n",
    "            for chkpt_dir in checkpoint_dirs:\n",
    "                chkpt_state_path = os.path.join(output_dir, chkpt_dir, \"trainer_state.json\")\n",
    "                if os.path.exists(chkpt_state_path):\n",
    "                    with open(chkpt_state_path, \"r\") as f: chkpt_state = json.load(f)\n",
    "                    for log in chkpt_state.get(\"log_history\", []):\n",
    "                        if metric_to_check in log: log_history.append(log)\n",
    "        \n",
    "        if log_history:\n",
    "            logging.info(f\"Found log history. Searching for best score using metric: '{metric_to_check}'\")\n",
    "            for log in log_history:\n",
    "                if metric_to_check in log:\n",
    "                    metric_value, step = log[metric_to_check], log.get('step')\n",
    "                    if step is None: continue\n",
    "                    if best_metric_value is None or \\\n",
    "                       (not is_loss and metric_value > best_metric_value) or \\\n",
    "                       (is_loss and metric_value < best_metric_value):\n",
    "                        potential_path = os.path.join(output_dir, f\"checkpoint-{step}\")\n",
    "                        if os.path.exists(potential_path):\n",
    "                            best_metric_value, best_checkpoint_path = metric_value, potential_path\n",
    "                            logging.info(f\"New best found -> Step: {step}, {metric_to_check}: {metric_value}\")\n",
    "        else:\n",
    "            # --- FINAL FALLBACK: No logs found, use the latest checkpoint ---\n",
    "            logging.warning(\"Could not find any evaluation metric logs.\")\n",
    "            logging.warning(\"Defaulting to the LAST saved checkpoint as the best model.\")\n",
    "            checkpoint_dirs = [d for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")]\n",
    "            if checkpoint_dirs:\n",
    "                latest_step = -1\n",
    "                for chkpt_dir in checkpoint_dirs:\n",
    "                    try:\n",
    "                        step = int(chkpt_dir.split('-')[-1])\n",
    "                        if step > latest_step:\n",
    "                            latest_step = step\n",
    "                            best_checkpoint_path = os.path.join(output_dir, chkpt_dir)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                if best_checkpoint_path:\n",
    "                     logging.info(f\"Identified last checkpoint: {best_checkpoint_path}\")\n",
    "\n",
    "        if not best_checkpoint_path:\n",
    "            logging.error(\"FATAL: Could not find any valid checkpoints to save.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"--- Model Identified for Saving ---\")\n",
    "        logging.info(f\"Checkpoint: {best_checkpoint_path}\")\n",
    "        if best_metric_value is not None:\n",
    "            logging.info(f\"Metric ({metric_to_check}): {best_metric_value}\")\n",
    "\n",
    "        final_model_path = os.path.join(output_dir, \"final_model\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            shutil.rmtree(final_model_path)\n",
    "            \n",
    "        shutil.copytree(best_checkpoint_path, final_model_path)\n",
    "        logging.info(f\"Successfully copied best model to: {final_model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    find_and_save_best_checkpoint(OUTPUT_DIR, METRIC_NAME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef79801",
   "metadata": {},
   "source": [
    "Post Training Evaluation and Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de9365b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:36,192 [INFO] - --- Starting Post-Training Evaluation of Checkpoints ---\n",
      "2025-10-07 23:31:36,197 [INFO] - Loading and preparing test data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5183ca20744914bc599508d8774e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:39,400 [INFO] - Test data prepared with 1845 examples.\n",
      "2025-10-07 23:31:39,400 [INFO] - Found 4 checkpoints to evaluate: ['checkpoint-2076', 'checkpoint-4152', 'checkpoint-6228', 'checkpoint-8304']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\bleurt-20\\downloads\\extracted\\8db8856a80394ae84b010e83ab663d4a3ccfa244ce3d0dbe00143f73e65ff123\\BLEURT-20.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,081 [INFO] - Reading checkpoint C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\bleurt-20\\downloads\\extracted\\8db8856a80394ae84b010e83ab663d4a3ccfa244ce3d0dbe00143f73e65ff123\\BLEURT-20.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,083 [INFO] - Config file found, reading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,089 [INFO] - Will load checkpoint BLEURT-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,090 [INFO] - Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:BLEURT-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,092 [INFO] - ... name:BLEURT-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,093 [INFO] - ... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,096 [INFO] - ... max_seq_length:512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,098 [INFO] - ... vocab_file:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,099 [INFO] - ... do_lower_case:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... sp_model:sent_piece\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,102 [INFO] - ... sp_model:sent_piece\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... dynamic_seq_length:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,104 [INFO] - ... dynamic_seq_length:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,105 [INFO] - Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,108 [INFO] - Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,109 [INFO] - Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load model: C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\bleurt-20\\downloads\\extracted\\8db8856a80394ae84b010e83ab663d4a3ccfa244ce3d0dbe00143f73e65ff123\\BLEURT-20\\sent_piece.model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,111 [INFO] - Will load model: C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\bleurt-20\\downloads\\extracted\\8db8856a80394ae84b010e83ab663d4a3ccfa244ce3d0dbe00143f73e65ff123\\BLEURT-20\\sent_piece.model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SentencePiece tokenizer created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,601 [INFO] - SentencePiece tokenizer created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,614 [INFO] - Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,617 [INFO] - Loading model.\n",
      "2025-10-07 23:31:57,978 [INFO] - Fingerprint not found. Saved model loading will continue.\n",
      "2025-10-07 23:31:57,980 [INFO] - path_and_singleprint metric could not be logged. Saved model loading will continue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:57,986 [INFO] - BLEURT initialized.\n",
      "2025-10-07 23:31:57,986 [INFO] - \n",
      "--- Evaluating Checkpoint: mbart-large-50-cnn-summarizer-v14\\checkpoint-2076 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f017cafa497465185ffeb8098a698cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_17504\\753486160.py:125: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "2025-10-07 23:32:04,261 [INFO] - Running evaluation on checkpoint-2076...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='102' max='462' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [102/462 11:19 < 40:22, 0.15 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import unicodedata\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "OUTPUT_DIR = \"mbart-large-50-cnn-summarizer-v14\" # trained model path\n",
    "METRIC_NAME = \"bleurt_f1\" # metric to find the best model\n",
    "DATA_PATH = \"../Dataset/new_large_CNN_dataset.csv\" # Path to original dataset\n",
    "MAX_SUMMARY_LENGTH_EVAL = 256\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"evaluate_checkpoints_log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] - %(message)s\", handlers=[logging.FileHandler(log_filename), logging.StreamHandler()])\n",
    "\n",
    "# --- Data Loading and Processing Functions (from the original training script) ---\n",
    "def sanitize_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    return text.replace('\"\"', '\"').strip()\n",
    "\n",
    "def normalize_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    return ' '.join(unicodedata.normalize('NFKC', text).split())\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        logging.info(\"--- Starting Post-Training Evaluation of Checkpoints ---\")\n",
    "        \n",
    "        # 1. --- Load and Prepare the Test Dataset ---\n",
    "        logging.info(\"Loading and preparing test data...\")\n",
    "        \n",
    "        df_new = pd.read_csv(DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        df_new.dropna(subset=['raw_news_article', 'english_summary', 'hindi_summary'], inplace=True)\n",
    "        \n",
    "        for col in ['raw_news_article', 'english_summary', 'hindi_summary']:\n",
    "            df_new[col] = df_new[col].apply(sanitize_text).apply(normalize_text)\n",
    "        \n",
    "        raw_dataset = Dataset.from_pandas(df_new)\n",
    "\n",
    "        def format_dataset_mbart(batch):\n",
    "            inputs, targets, langs = [], [], []\n",
    "            for article, eng_summary, hin_summary in zip(batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']):\n",
    "                if isinstance(article, str) and article:\n",
    "                    inputs.append(article); targets.append(eng_summary); langs.append(\"en_XX\")\n",
    "                    inputs.append(article); targets.append(hin_summary); langs.append(\"hi_IN\")\n",
    "            return {'article': inputs, 'summary': targets, 'target_lang': langs}\n",
    "\n",
    "        processed_dataset = raw_dataset.map(format_dataset_mbart, batched=True, remove_columns=raw_dataset.column_names)\n",
    "        \n",
    "        train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        test_dataset_untokenized = train_test_split['test']\n",
    "        logging.info(f\"Test data prepared with {len(test_dataset_untokenized)} examples.\")\n",
    "\n",
    "        # 2. --- Find all Checkpoints ---\n",
    "        checkpoint_dirs = sorted(\n",
    "            [d for d in os.listdir(OUTPUT_DIR) if d.startswith(\"checkpoint-\")],\n",
    "            key=lambda x: int(x.split('-')[-1])\n",
    "        )\n",
    "        if not checkpoint_dirs:\n",
    "            logging.error(f\"FATAL: No 'checkpoint-*' directories found in '{OUTPUT_DIR}'.\")\n",
    "            return\n",
    "        logging.info(f\"Found {len(checkpoint_dirs)} checkpoints to evaluate: {checkpoint_dirs}\")\n",
    "        \n",
    "        all_results = []\n",
    "        best_metric_value = None\n",
    "        best_checkpoint_path = None\n",
    "        metric_to_check = f\"eval_{METRIC_NAME}\"\n",
    "        is_loss = 'loss' in metric_to_check.lower()\n",
    "        \n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "        bleurt_metric = evaluate.load(\"bleurt\", \"bleurt-20\")\n",
    "\n",
    "        def compute_metrics_wrapper(eval_pred, tokenizer):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "            bleurt_result = bleurt_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "            result = {\"rouge1\": rouge_result[\"rouge1\"], \"rouge2\": rouge_result[\"rouge2\"], \"rougeL\": rouge_result[\"rougeL\"], \"bleurt_f1\": np.mean(bleurt_result[\"scores\"])}\n",
    "            return {f\"eval_{k}\": round(v * 100, 4) for k, v in result.items()}\n",
    "\n",
    "        # 3. --- Loop Through and Evaluate Each Checkpoint ---\n",
    "        for chkpt_dir in checkpoint_dirs:\n",
    "            chkpt_path = os.path.join(OUTPUT_DIR, chkpt_dir)\n",
    "            logging.info(f\"\\n--- Evaluating Checkpoint: {chkpt_path} ---\")\n",
    "            \n",
    "            model = MBartForConditionalGeneration.from_pretrained(chkpt_path)\n",
    "            tokenizer = MBart50TokenizerFast.from_pretrained(chkpt_path)\n",
    "            \n",
    "            def tokenize_for_eval(examples):\n",
    "                tokenizer.src_lang = \"en_XX\"\n",
    "                model_inputs = tokenizer(examples['article'], max_length=1024, truncation=True)\n",
    "                labels_batch = []\n",
    "                for i in range(len(examples['summary'])):\n",
    "                    tokenizer.tgt_lang = examples['target_lang'][i]\n",
    "                    labels = tokenizer(text_target=examples['summary'][i], max_length=MAX_SUMMARY_LENGTH_EVAL, truncation=True)\n",
    "                    labels_batch.append(labels['input_ids'])\n",
    "                model_inputs[\"labels\"] = labels_batch\n",
    "                return model_inputs\n",
    "\n",
    "            tokenized_test_dataset = test_dataset_untokenized.map(tokenize_for_eval, batched=True, remove_columns=['article', 'summary', 'target_lang'])\n",
    "\n",
    "            temp_training_args = Seq2SeqTrainingArguments(\n",
    "                output_dir=os.path.join(OUTPUT_DIR, \"temp_eval\"),\n",
    "                per_device_eval_batch_size=4,\n",
    "                predict_with_generate=True,\n",
    "                fp16=torch.cuda.is_available()\n",
    "            )\n",
    "\n",
    "            trainer = Seq2SeqTrainer(\n",
    "                model=model, args=temp_training_args,\n",
    "                eval_dataset=tokenized_test_dataset, tokenizer=tokenizer,\n",
    "                data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "                compute_metrics=lambda p: compute_metrics_wrapper(p, tokenizer)\n",
    "            )\n",
    "            \n",
    "            logging.info(f\"Running evaluation on {chkpt_dir}...\")\n",
    "            eval_results = trainer.evaluate()\n",
    "            \n",
    "            logging.info(f\"--- Results for {chkpt_dir} ---\")\n",
    "            for key, value in eval_results.items():\n",
    "                logging.info(f\"  - {key}: {value:.4f}\")\n",
    "            all_results.append({'checkpoint': chkpt_dir, **eval_results})\n",
    "            \n",
    "            metric_value = eval_results.get(metric_to_check)\n",
    "            if metric_value is not None:\n",
    "                if best_metric_value is None or (not is_loss and metric_value > best_metric_value) or (is_loss and metric_value < best_metric_value):\n",
    "                    best_metric_value, best_checkpoint_path = metric_value, chkpt_path\n",
    "                    logging.info(f\"*** New best checkpoint found: {chkpt_dir} with {metric_to_check}: {metric_value:.4f} ***\")\n",
    "\n",
    "        # 4. --- Print Final Summary Table and Save the Best ---\n",
    "        if not all_results:\n",
    "            logging.error(\"No checkpoints were successfully evaluated.\")\n",
    "            return\n",
    "\n",
    "        logging.info(\"\\n\" + \"=\"*80)\n",
    "        logging.info(\"--- FINAL EVALUATION SUMMARY ---\".center(80))\n",
    "        logging.info(\"=\"*80)\n",
    "        header = f\"{'Checkpoint':<20} | {'eval_loss':<12} | {'eval_rouge1':<12} | {'eval_rouge2':<12} | {'eval_rougeL':<12} | {'eval_bleurt_f1':<15}\"\n",
    "        logging.info(header)\n",
    "        logging.info(\"-\" * len(header))\n",
    "        for result in all_results:\n",
    "            row = f\"{result['checkpoint']:<20} | {result.get('eval_loss', 'N/A'):<12.4f} | {result.get('eval_eval_rouge1', 'N/A'):<12.4f} | {result.get('eval_eval_rouge2', 'N/A'):<12.4f} | {result.get('eval_eval_rougeL', 'N/A'):<12.4f} | {result.get(metric_to_check, 'N/A'):<15.4f}\"\n",
    "            logging.info(row)\n",
    "        logging.info(\"=\"*80)\n",
    "\n",
    "        if not best_checkpoint_path:\n",
    "            logging.error(\"Could not determine the best checkpoint after evaluation.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"\\n--- Best Model Identified ---\")\n",
    "        logging.info(f\"Checkpoint: {best_checkpoint_path}\")\n",
    "        logging.info(f\"Metric ({metric_to_check}): {best_metric_value:.4f}\")\n",
    "\n",
    "        final_model_path = os.path.join(OUTPUT_DIR, \"final_model\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            shutil.rmtree(final_model_path)\n",
    "            \n",
    "        shutil.copytree(best_checkpoint_path, final_model_path)\n",
    "        logging.info(f\"Successfully copied best model to: {final_model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b6acf5",
   "metadata": {},
   "source": [
    "Bulk Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1018de89",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import unicodedata\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_PATH = \"mbart-large-50-cnn-summarizer-v14/final_model\"\n",
    "# --- path of the CSV file to evaluate ---\n",
    "EVAL_DATA_PATH = \"../Dataset/filtered_articles_CNN.csv\" \n",
    "MAX_SUMMARY_LENGTH_EVAL = 256\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"bulk_evaluation_log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] - %(message)s\", handlers=[logging.FileHandler(log_filename), logging.StreamHandler()])\n",
    "\n",
    "def sanitize_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    return text.replace('\"\"', '\"').strip()\n",
    "\n",
    "def normalize_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    return ' '.join(unicodedata.normalize('NFKC', text).split())\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        logging.info(\"--- Starting Bulk Evaluation ---\")\n",
    "        \n",
    "        # 1. --- Load Model and Tokenizer ---\n",
    "        logging.info(f\"Loading model from: {MODEL_PATH}\")\n",
    "        model = MBartForConditionalGeneration.from_pretrained(MODEL_PATH)\n",
    "        tokenizer = MBart50TokenizerFast.from_pretrained(MODEL_PATH)\n",
    "        \n",
    "        # 2. --- Load and Prepare the Evaluation Dataset ---\n",
    "        logging.info(f\"Loading evaluation data from: {EVAL_DATA_PATH}\")\n",
    "        df_eval = pd.read_csv(EVAL_DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        df_eval.dropna(subset=['raw_news_article', 'english_summary', 'hindi_summary'], inplace=True)\n",
    "        \n",
    "        for col in ['raw_news_article', 'english_summary', 'hindi_summary']:\n",
    "            df_eval[col] = df_eval[col].apply(sanitize_text).apply(normalize_text)\n",
    "        \n",
    "        raw_dataset = Dataset.from_pandas(df_eval)\n",
    "\n",
    "        def format_dataset_mbart(batch):\n",
    "            inputs, targets, langs = [], [], []\n",
    "            for article, eng_summary, hin_summary in zip(batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']):\n",
    "                if isinstance(article, str) and article:\n",
    "                    inputs.append(article); targets.append(eng_summary); langs.append(\"en_XX\")\n",
    "                    inputs.append(article); targets.append(hin_summary); langs.append(\"hi_IN\")\n",
    "            return {'article': inputs, 'summary': targets, 'target_lang': langs}\n",
    "\n",
    "        eval_dataset_untokenized = raw_dataset.map(format_dataset_mbart, batched=True, remove_columns=raw_dataset.column_names)\n",
    "\n",
    "        def tokenize_for_eval(examples):\n",
    "            tokenizer.src_lang = \"en_XX\"\n",
    "            model_inputs = tokenizer(examples['article'], max_length=1024, truncation=True)\n",
    "            labels_batch = []\n",
    "            for i in range(len(examples['summary'])):\n",
    "                tokenizer.tgt_lang = examples['target_lang'][i]\n",
    "                labels = tokenizer(text_target=examples['summary'][i], max_length=MAX_SUMMARY_LENGTH_EVAL, truncation=True)\n",
    "                labels_batch.append(labels['input_ids'])\n",
    "            model_inputs[\"labels\"] = labels_batch\n",
    "            return model_inputs\n",
    "\n",
    "        tokenized_eval_dataset = eval_dataset_untokenized.map(tokenize_for_eval, batched=True, remove_columns=['article', 'summary', 'target_lang'])\n",
    "        logging.info(f\"Evaluation data prepared with {len(tokenized_eval_dataset)} examples.\")\n",
    "\n",
    "        # 3. --- Setup Metrics and Trainer ---\n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "        bleurt_metric = evaluate.load(\"bleurt\", \"bleurt-20\")\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "            bleurt_result = bleurt_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "            result = {\"rouge1\": rouge_result[\"rouge1\"], \"rouge2\": rouge_result[\"rouge2\"], \"rougeL\": rouge_result[\"rougeL\"], \"bleurt_f1\": np.mean(bleurt_result[\"scores\"])}\n",
    "            return {f\"eval_{k}\": round(v * 100, 4) for k, v in result.items()}\n",
    "\n",
    "        temp_training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=\"temp_bulk_eval\",\n",
    "            per_device_eval_batch_size=4,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            generation_max_length=MAX_SUMMARY_LENGTH_EVAL,\n",
    "            generation_num_beams=6\n",
    "        )\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=temp_training_args,\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "            compute_metrics=compute_metrics\n",
    "        )\n",
    "\n",
    "        # 4. --- Run Prediction and Evaluation ---\n",
    "        logging.info(\"Running bulk summarization and evaluation...\")\n",
    "        results = trainer.predict(tokenized_eval_dataset)\n",
    "        \n",
    "        logging.info(\"\\n\" + \"=\"*80)\n",
    "        logging.info(\"--- FINAL EVALUATION METRICS ---\".center(80))\n",
    "        logging.info(\"=\"*80)\n",
    "        for key, value in results.metrics.items():\n",
    "            logging.info(f\"  - {key}: {value:.4f}\")\n",
    "        logging.info(\"=\"*80)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarizer_env3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
