{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5b42332",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:00,424 [INFO] - Attempting to load model using safetensors to bypass torch.load vulnerability check.\n",
      "2025-10-07 20:18:03,701 [INFO] - Model loaded successfully using safetensors.\n",
      "2025-10-07 20:18:04,413 [INFO] - --- Starting Text Sanitization & Normalization ---\n",
      "2025-10-07 20:18:05,972 [INFO] - --- Text Sanitization & Normalization Finished ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9216a5a5d0b04378961d45eb049db9fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d79d7511c2a4bfd9bf8c485e0c02f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba626404057043358374835a417e5195",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\bleurt-20\\downloads\\extracted\\8db8856a80394ae84b010e83ab663d4a3ccfa244ce3d0dbe00143f73e65ff123\\BLEURT-20.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,190 [INFO] - Reading checkpoint C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\bleurt-20\\downloads\\extracted\\8db8856a80394ae84b010e83ab663d4a3ccfa244ce3d0dbe00143f73e65ff123\\BLEURT-20.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,190 [INFO] - Config file found, reading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,190 [INFO] - Will load checkpoint BLEURT-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,190 [INFO] - Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:BLEURT-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,207 [INFO] - ... name:BLEURT-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,207 [INFO] - ... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,213 [INFO] - ... max_seq_length:512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,214 [INFO] - ... vocab_file:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,217 [INFO] - ... do_lower_case:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... sp_model:sent_piece\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,219 [INFO] - ... sp_model:sent_piece\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... dynamic_seq_length:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,221 [INFO] - ... dynamic_seq_length:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,224 [INFO] - Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,226 [INFO] - Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,228 [INFO] - Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load model: C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\bleurt-20\\downloads\\extracted\\8db8856a80394ae84b010e83ab663d4a3ccfa244ce3d0dbe00143f73e65ff123\\BLEURT-20\\sent_piece.model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,230 [INFO] - Will load model: C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\bleurt-20\\downloads\\extracted\\8db8856a80394ae84b010e83ab663d4a3ccfa244ce3d0dbe00143f73e65ff123\\BLEURT-20\\sent_piece.model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SentencePiece tokenizer created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,728 [INFO] - SentencePiece tokenizer created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,728 [INFO] - Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:18:55,741 [INFO] - Loading model.\n",
      "2025-10-07 20:19:01,388 [INFO] - Fingerprint not found. Saved model loading will continue.\n",
      "2025-10-07 20:19:01,388 [INFO] - path_and_singleprint metric could not be logged. Saved model loading will continue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 20:19:01,391 [INFO] - BLEURT initialized.\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_17504\\2307351194.py:162: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "2025-10-07 20:19:05,307 [INFO] - Starting final training (v14) from scratch with mBART-LARGE...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='8304' max='8304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [8304/8304 2:34:51, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.085600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.616600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.491900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.400200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>2.296500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>2.208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>2.149700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>2.123900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>2.061800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>2.024100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>2.050900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.986000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.974200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>1.995600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>1.963600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>1.958900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.993300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.945800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>1.988100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.989400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>1.944000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>1.942400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>1.922900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>1.919300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>1.953600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>1.924700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>1.881000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>1.910100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>1.834500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>1.888200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>1.860600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>1.820100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>1.885900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>1.854700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>1.811400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>1.854200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>1.805200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>1.805600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>1.825500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.803000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>1.825000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>1.755900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>7.060700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>6.126900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>5.721900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>5.763300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>5.321000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>5.066300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>4.991300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>5.138900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>5.213900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>4.898200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>5.033400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>4.046200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>2.757000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>1.744800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>1.730200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>1.729200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>1.674800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.606600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>1.642300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>1.624300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3150</td>\n",
       "      <td>1.624300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>1.613500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3250</td>\n",
       "      <td>1.622600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3300</td>\n",
       "      <td>1.594600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3350</td>\n",
       "      <td>1.608400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>1.594300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3450</td>\n",
       "      <td>1.585400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.585500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3550</td>\n",
       "      <td>1.616000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>1.544400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3650</td>\n",
       "      <td>1.594600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3700</td>\n",
       "      <td>1.543800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3750</td>\n",
       "      <td>1.558200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>1.566900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3850</td>\n",
       "      <td>1.549500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3900</td>\n",
       "      <td>1.558500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3950</td>\n",
       "      <td>1.584400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.534700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4050</td>\n",
       "      <td>1.538600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4100</td>\n",
       "      <td>1.538400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4150</td>\n",
       "      <td>1.558900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>1.428600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4250</td>\n",
       "      <td>1.410100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4300</td>\n",
       "      <td>1.420200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4350</td>\n",
       "      <td>1.434700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>1.443000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4450</td>\n",
       "      <td>1.422400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.441800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4550</td>\n",
       "      <td>1.414000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>1.400900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4650</td>\n",
       "      <td>1.421300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4700</td>\n",
       "      <td>1.463000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4750</td>\n",
       "      <td>1.445700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>1.423500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4850</td>\n",
       "      <td>1.425700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4900</td>\n",
       "      <td>1.428100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4950</td>\n",
       "      <td>1.386000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.387500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5050</td>\n",
       "      <td>1.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5100</td>\n",
       "      <td>1.429800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5150</td>\n",
       "      <td>1.397700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>1.402300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5250</td>\n",
       "      <td>1.400500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5300</td>\n",
       "      <td>1.429000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5350</td>\n",
       "      <td>1.402000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>1.393200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5450</td>\n",
       "      <td>1.396200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>1.376000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5550</td>\n",
       "      <td>1.423400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>1.413800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5650</td>\n",
       "      <td>1.399800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5700</td>\n",
       "      <td>1.413700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5750</td>\n",
       "      <td>1.401300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>1.428600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5850</td>\n",
       "      <td>1.383700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5900</td>\n",
       "      <td>1.371000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5950</td>\n",
       "      <td>1.387500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.404000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6050</td>\n",
       "      <td>1.362000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6100</td>\n",
       "      <td>1.423100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6150</td>\n",
       "      <td>1.378100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>1.425900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6250</td>\n",
       "      <td>1.382100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6300</td>\n",
       "      <td>1.313000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6350</td>\n",
       "      <td>1.289700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>1.291400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6450</td>\n",
       "      <td>1.325800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>1.301100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6550</td>\n",
       "      <td>1.296900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>1.292400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6650</td>\n",
       "      <td>1.276200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6700</td>\n",
       "      <td>1.274700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6750</td>\n",
       "      <td>1.277500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>1.298900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6850</td>\n",
       "      <td>1.306000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6900</td>\n",
       "      <td>1.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6950</td>\n",
       "      <td>1.293600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>1.305100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7050</td>\n",
       "      <td>1.276200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7100</td>\n",
       "      <td>1.303600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7150</td>\n",
       "      <td>1.273900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>1.266500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7250</td>\n",
       "      <td>1.316200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7300</td>\n",
       "      <td>1.297500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7350</td>\n",
       "      <td>1.300700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>1.278900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7450</td>\n",
       "      <td>1.315200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>1.301500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7550</td>\n",
       "      <td>1.307900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>1.302800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7650</td>\n",
       "      <td>1.282400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7700</td>\n",
       "      <td>1.286500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7750</td>\n",
       "      <td>1.248500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>1.314700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7850</td>\n",
       "      <td>1.315200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7900</td>\n",
       "      <td>1.291400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7950</td>\n",
       "      <td>1.282700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>1.296600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8050</td>\n",
       "      <td>1.325400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8100</td>\n",
       "      <td>1.304500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8150</td>\n",
       "      <td>1.276000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>1.312300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8250</td>\n",
       "      <td>1.269300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8300</td>\n",
       "      <td>1.266200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\modeling_utils.py:3922: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 200, 'early_stopping': True, 'num_beams': 5}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n",
      "2025-10-07 22:53:59,299 [INFO] - Training finished. All checkpoints and logs are saved.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import os\n",
    "import unicodedata\n",
    "from datetime import datetime\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer,\n",
    "    TrainerCallback\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = \"facebook/mbart-large-50\"\n",
    "NEW_MODEL_OUTPUT_DIR = \"mbart-large-50-cnn-summarizer-v14\"\n",
    "NEW_DATA_PATH = \"../Dataset/new_large_CNN_dataset.csv\"\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 4 \n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "WEIGHT_DECAY = 0.3\n",
    "NUM_BEAMS_EVAL = 6\n",
    "MAX_SUMMARY_LENGTH_EVAL = 256\n",
    "METRIC_FOR_BEST_MODEL = \"bleurt_f1\"\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"mbart_large_training_log_v14_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "    handlers=[logging.FileHandler(log_filename), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "class ZeroLossCallback(TrainerCallback):\n",
    "    \"\"\"A callback that stops training if the training loss is zero to prevent wasted resources.\"\"\"\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        if logs is not None and 'loss' in logs and logs['loss'] == 0.0:\n",
    "            logging.error(\"CRITICAL: Training loss is zero. This indicates a data issue. Stopping training.\")\n",
    "            control.should_training_stop = True\n",
    "\n",
    "def sanitize_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    return text.replace('\"\"', '\"').strip()\n",
    "\n",
    "def normalize_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    return ' '.join(unicodedata.normalize('NFKC', text).split())\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        tokenizer = MBart50TokenizerFast.from_pretrained(BASE_MODEL_PATH)\n",
    "        \n",
    "        # --- THE DEFINITIVE FIX: Use safetensors to bypass the security check ---\n",
    "        logging.info(\"Attempting to load model using safetensors to bypass torch.load vulnerability check.\")\n",
    "        model = MBartForConditionalGeneration.from_pretrained(BASE_MODEL_PATH, use_safetensors=True)\n",
    "        logging.info(\"Model loaded successfully using safetensors.\")\n",
    "        # --------------------------------------------------------------------\n",
    "\n",
    "        df_new = pd.read_csv(NEW_DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        df_new.dropna(subset=['raw_news_article', 'english_summary', 'hindi_summary'], inplace=True)\n",
    "        \n",
    "        logging.info(\"--- Starting Text Sanitization & Normalization ---\")\n",
    "        for col in ['raw_news_article', 'english_summary', 'hindi_summary']:\n",
    "            df_new[col] = df_new[col].apply(sanitize_text).apply(normalize_text)\n",
    "        logging.info(\"--- Text Sanitization & Normalization Finished ---\")\n",
    "        \n",
    "        raw_dataset = Dataset.from_pandas(df_new)\n",
    "\n",
    "        def format_dataset_mbart(batch):\n",
    "            inputs, targets, langs = [], [], []\n",
    "            for article, eng_summary, hin_summary in zip(\n",
    "                batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']\n",
    "            ):\n",
    "                if isinstance(article, str) and article:\n",
    "                    inputs.append(article)\n",
    "                    targets.append(eng_summary)\n",
    "                    langs.append(\"en_XX\")\n",
    "                    inputs.append(article)\n",
    "                    targets.append(hin_summary)\n",
    "                    langs.append(\"hi_IN\")\n",
    "            return {'article': inputs, 'summary': targets, 'target_lang': langs}\n",
    "\n",
    "        processed_dataset = raw_dataset.map(\n",
    "            format_dataset_mbart, batched=True, remove_columns=raw_dataset.column_names\n",
    "        )\n",
    "        \n",
    "        train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        final_datasets = DatasetDict({\n",
    "            'train': train_test_split['train'],\n",
    "            'test': train_test_split['test']\n",
    "        })\n",
    "        \n",
    "        def tokenize_function(examples):\n",
    "            tokenizer.src_lang = \"en_XX\"\n",
    "            model_inputs = tokenizer(examples['article'], max_length=1024, truncation=True)\n",
    "            \n",
    "            labels_batch = []\n",
    "            for i in range(len(examples['summary'])):\n",
    "                tokenizer.tgt_lang = examples['target_lang'][i]\n",
    "                labels = tokenizer(\n",
    "                    text_target=examples['summary'][i], \n",
    "                    max_length=MAX_SUMMARY_LENGTH_EVAL, \n",
    "                    truncation=True\n",
    "                )\n",
    "                labels_batch.append(labels['input_ids'])\n",
    "            \n",
    "            model_inputs[\"labels\"] = labels_batch\n",
    "            return model_inputs\n",
    "\n",
    "        tokenized_datasets = final_datasets.map(tokenize_function, batched=True, remove_columns=['article', 'summary', 'target_lang'])\n",
    "        \n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "        bleurt_metric = evaluate.load(\"bleurt\", \"bleurt-20\")\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "            bleurt_result = bleurt_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "            \n",
    "            result = {\n",
    "                \"rouge1\": rouge_result[\"rouge1\"], \"rouge2\": rouge_result[\"rouge2\"],\n",
    "                \"rougeL\": rouge_result[\"rougeL\"], \"bleurt_f1\": np.mean(bleurt_result[\"scores\"])\n",
    "            }\n",
    "            return {k: round(v * 100, 4) for k, v in result.items()}\n",
    "\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=NEW_MODEL_OUTPUT_DIR,\n",
    "            num_train_epochs=NUM_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            logging_dir=f\"{NEW_MODEL_OUTPUT_DIR}/logs\",\n",
    "            logging_strategy=\"steps\",\n",
    "            logging_steps=50,\n",
    "            save_strategy=\"epoch\",\n",
    "            save_total_limit=NUM_EPOCHS,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            load_best_model_at_end=False,\n",
    "            report_to=\"tensorboard\",\n",
    "            generation_max_length=MAX_SUMMARY_LENGTH_EVAL,\n",
    "            generation_num_beams=NUM_BEAMS_EVAL,\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "            callbacks=[ZeroLossCallback()]\n",
    "        )\n",
    "\n",
    "        logging.info(\"Starting final training (v14) from scratch with mBART-LARGE...\")\n",
    "        trainer.train()\n",
    "        logging.info(\"Training finished. All checkpoints and logs are saved.\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during the main process: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6f813c",
   "metadata": {},
   "source": [
    "Selecting best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98b46ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:20:41,148 [INFO] - Attempting to find best model in: mbart-large-50-cnn-summarizer-v14\n",
      "2025-10-07 23:20:41,163 [INFO] - Contents of 'mbart-large-50-cnn-summarizer-v14': ['checkpoint-2076', 'checkpoint-4152', 'checkpoint-6228', 'checkpoint-8304', 'logs']\n",
      "2025-10-07 23:20:41,164 [WARNING] - Could not find any evaluation metric logs.\n",
      "2025-10-07 23:20:41,164 [WARNING] - Defaulting to the LAST saved checkpoint as the best model.\n",
      "2025-10-07 23:20:41,164 [INFO] - Identified last checkpoint: mbart-large-50-cnn-summarizer-v14\\checkpoint-8304\n",
      "2025-10-07 23:20:41,164 [INFO] - --- Model Identified for Saving ---\n",
      "2025-10-07 23:20:41,164 [INFO] - Checkpoint: mbart-large-50-cnn-summarizer-v14\\checkpoint-8304\n",
      "2025-10-07 23:22:01,049 [INFO] - Successfully copied best model to: mbart-large-50-cnn-summarizer-v14\\final_model\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# --- Configuration: ---\n",
    "OUTPUT_DIR = \"mbart-large-50-cnn-summarizer-v14\" \n",
    "METRIC_NAME = \"bleurt_f1\"\n",
    "# -------------------------------------------------\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"select_best_model_log_v14_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "    handlers=[logging.FileHandler(log_filename), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "def find_and_save_best_checkpoint(output_dir, metric_name):\n",
    "    \"\"\"\n",
    "    Finds and saves the best model checkpoint from a training run.\n",
    "    If no evaluation metrics are found, it defaults to saving the last available checkpoint.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(f\"Attempting to find best model in: {output_dir}\")\n",
    "\n",
    "        if not os.path.isdir(output_dir):\n",
    "            logging.error(f\"FATAL: The directory '{output_dir}' does not exist.\")\n",
    "            return\n",
    "        \n",
    "        logging.info(f\"Contents of '{output_dir}': {os.listdir(output_dir)}\")\n",
    "        \n",
    "        best_metric_value = None\n",
    "        best_checkpoint_path = None\n",
    "        metric_to_check = f\"eval_{metric_name}\"\n",
    "        is_loss = 'loss' in metric_to_check.lower()\n",
    "        log_history = []\n",
    "        \n",
    "        # --- Multi-level Fallback to find logs ---\n",
    "        # 1. Check top-level state file\n",
    "        main_state_path = os.path.join(output_dir, \"trainer_state.json\")\n",
    "        if os.path.exists(main_state_path):\n",
    "            with open(main_state_path, \"r\") as f: state = json.load(f)\n",
    "            log_history = state.get(\"log_history\", [])\n",
    "\n",
    "        # 2. Check individual checkpoint state files\n",
    "        if not log_history:\n",
    "            checkpoint_dirs = [d for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")]\n",
    "            for chkpt_dir in checkpoint_dirs:\n",
    "                chkpt_state_path = os.path.join(output_dir, chkpt_dir, \"trainer_state.json\")\n",
    "                if os.path.exists(chkpt_state_path):\n",
    "                    with open(chkpt_state_path, \"r\") as f: chkpt_state = json.load(f)\n",
    "                    for log in chkpt_state.get(\"log_history\", []):\n",
    "                        if metric_to_check in log: log_history.append(log)\n",
    "        \n",
    "        if log_history:\n",
    "            logging.info(f\"Found log history. Searching for best score using metric: '{metric_to_check}'\")\n",
    "            for log in log_history:\n",
    "                if metric_to_check in log:\n",
    "                    metric_value, step = log[metric_to_check], log.get('step')\n",
    "                    if step is None: continue\n",
    "                    if best_metric_value is None or \\\n",
    "                       (not is_loss and metric_value > best_metric_value) or \\\n",
    "                       (is_loss and metric_value < best_metric_value):\n",
    "                        potential_path = os.path.join(output_dir, f\"checkpoint-{step}\")\n",
    "                        if os.path.exists(potential_path):\n",
    "                            best_metric_value, best_checkpoint_path = metric_value, potential_path\n",
    "                            logging.info(f\"New best found -> Step: {step}, {metric_to_check}: {metric_value}\")\n",
    "        else:\n",
    "            # --- FINAL FALLBACK: No logs found, use the latest checkpoint ---\n",
    "            logging.warning(\"Could not find any evaluation metric logs.\")\n",
    "            logging.warning(\"Defaulting to the LAST saved checkpoint as the best model.\")\n",
    "            checkpoint_dirs = [d for d in os.listdir(output_dir) if d.startswith(\"checkpoint-\")]\n",
    "            if checkpoint_dirs:\n",
    "                latest_step = -1\n",
    "                for chkpt_dir in checkpoint_dirs:\n",
    "                    try:\n",
    "                        step = int(chkpt_dir.split('-')[-1])\n",
    "                        if step > latest_step:\n",
    "                            latest_step = step\n",
    "                            best_checkpoint_path = os.path.join(output_dir, chkpt_dir)\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "                if best_checkpoint_path:\n",
    "                     logging.info(f\"Identified last checkpoint: {best_checkpoint_path}\")\n",
    "\n",
    "        if not best_checkpoint_path:\n",
    "            logging.error(\"FATAL: Could not find any valid checkpoints to save.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"--- Model Identified for Saving ---\")\n",
    "        logging.info(f\"Checkpoint: {best_checkpoint_path}\")\n",
    "        if best_metric_value is not None:\n",
    "            logging.info(f\"Metric ({metric_to_check}): {best_metric_value}\")\n",
    "\n",
    "        final_model_path = os.path.join(output_dir, \"final_model\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            shutil.rmtree(final_model_path)\n",
    "            \n",
    "        shutil.copytree(best_checkpoint_path, final_model_path)\n",
    "        logging.info(f\"Successfully copied best model to: {final_model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    find_and_save_best_checkpoint(OUTPUT_DIR, METRIC_NAME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef79801",
   "metadata": {},
   "source": [
    "Post Training Evaluation and Model Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de9365b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:36,192 [INFO] - --- Starting Post-Training Evaluation of Checkpoints ---\n",
      "2025-10-07 23:31:36,197 [INFO] - Loading and preparing test data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de5183ca20744914bc599508d8774e8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:39,400 [INFO] - Test data prepared with 1845 examples.\n",
      "2025-10-07 23:31:39,400 [INFO] - Found 4 checkpoints to evaluate: ['checkpoint-2076', 'checkpoint-4152', 'checkpoint-6228', 'checkpoint-8304']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\bleurt-20\\downloads\\extracted\\8db8856a80394ae84b010e83ab663d4a3ccfa244ce3d0dbe00143f73e65ff123\\BLEURT-20.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,081 [INFO] - Reading checkpoint C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\bleurt-20\\downloads\\extracted\\8db8856a80394ae84b010e83ab663d4a3ccfa244ce3d0dbe00143f73e65ff123\\BLEURT-20.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,083 [INFO] - Config file found, reading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,089 [INFO] - Will load checkpoint BLEURT-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,090 [INFO] - Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:BLEURT-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,092 [INFO] - ... name:BLEURT-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,093 [INFO] - ... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,096 [INFO] - ... max_seq_length:512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,098 [INFO] - ... vocab_file:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,099 [INFO] - ... do_lower_case:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... sp_model:sent_piece\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,102 [INFO] - ... sp_model:sent_piece\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... dynamic_seq_length:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,104 [INFO] - ... dynamic_seq_length:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,105 [INFO] - Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,108 [INFO] - Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,109 [INFO] - Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load model: C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\bleurt-20\\downloads\\extracted\\8db8856a80394ae84b010e83ab663d4a3ccfa244ce3d0dbe00143f73e65ff123\\BLEURT-20\\sent_piece.model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,111 [INFO] - Will load model: C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\bleurt-20\\downloads\\extracted\\8db8856a80394ae84b010e83ab663d4a3ccfa244ce3d0dbe00143f73e65ff123\\BLEURT-20\\sent_piece.model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SentencePiece tokenizer created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,601 [INFO] - SentencePiece tokenizer created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,614 [INFO] - Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:51,617 [INFO] - Loading model.\n",
      "2025-10-07 23:31:57,978 [INFO] - Fingerprint not found. Saved model loading will continue.\n",
      "2025-10-07 23:31:57,980 [INFO] - path_and_singleprint metric could not be logged. Saved model loading will continue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-07 23:31:57,986 [INFO] - BLEURT initialized.\n",
      "2025-10-07 23:31:57,986 [INFO] - \n",
      "--- Evaluating Checkpoint: mbart-large-50-cnn-summarizer-v14\\checkpoint-2076 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f017cafa497465185ffeb8098a698cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_17504\\753486160.py:125: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "2025-10-07 23:32:04,261 [INFO] - Running evaluation on checkpoint-2076...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='288' max='462' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [288/462 32:04 < 19:27, 0.15 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 181\u001b[0m\n\u001b[0;32m    178\u001b[0m         logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn unexpected error occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 181\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 133\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    125\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m    126\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, args\u001b[38;5;241m=\u001b[39mtemp_training_args,\n\u001b[0;32m    127\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_test_dataset, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m    128\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mDataCollatorForSeq2Seq(tokenizer, model\u001b[38;5;241m=\u001b[39mmodel),\n\u001b[0;32m    129\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m p: compute_metrics_wrapper(p, tokenizer)\n\u001b[0;32m    130\u001b[0m )\n\u001b[0;32m    132\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning evaluation on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchkpt_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 133\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Results for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchkpt_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m eval_results\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\trainer_seq2seq.py:191\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[1;32m--> 191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\trainer.py:4489\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4486\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   4488\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 4489\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4490\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4492\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   4493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   4494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4497\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4499\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   4500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\trainer.py:4685\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4682\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[0;32m   4684\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[1;32m-> 4685\u001b[0m losses, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4686\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4687\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   4688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4689\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\trainer_seq2seq.py:327\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.prediction_step\u001b[1;34m(self, model, inputs, prediction_loss_only, ignore_keys, **gen_kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m summon_full_params_context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    321\u001b[0m     FullyShardedDataParallel\u001b[38;5;241m.\u001b[39msummon_full_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, FullyShardedDataParallel)\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext()\n\u001b[0;32m    324\u001b[0m )\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m summon_full_params_context:\n\u001b[1;32m--> 327\u001b[0m     generated_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgen_kwargs)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# Temporary hack to ensure the generation config is not initialized for each iteration of the evaluation loop\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# TODO: remove this hack when the legacy code that initializes generation_config from a model config is\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# removed in https://github.com/huggingface/transformers/blob/98d88b23f54e5a23e741833f1e973fdf600cc2c5/src/transformers/generation/utils.py#L1183\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39m_from_model_config:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\generation\\utils.py:2564\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2561\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[0;32m   2563\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[1;32m-> 2564\u001b[0m result \u001b[38;5;241m=\u001b[39m decoding_method(\n\u001b[0;32m   2565\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2566\u001b[0m     input_ids,\n\u001b[0;32m   2567\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2568\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2569\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2570\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_mode_kwargs,\n\u001b[0;32m   2571\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2572\u001b[0m )\n\u001b[0;32m   2574\u001b[0m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[0;32m   2575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2576\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mreturn_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result\u001b[38;5;241m.\u001b[39mpast_key_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_legacy_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2579\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\generation\\utils.py:3317\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   3313\u001b[0m log_probs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mreshape(log_probs, (batch_size, num_beams \u001b[38;5;241m*\u001b[39m vocab_size))\n\u001b[0;32m   3315\u001b[0m \u001b[38;5;66;03m# c. Retrieve top-K continuations, i.e. select the next token (greedy or sampling) and then keep the best\u001b[39;00m\n\u001b[0;32m   3316\u001b[0m \u001b[38;5;66;03m# continuations among all beams based on the accumulated scores.\u001b[39;00m\n\u001b[1;32m-> 3317\u001b[0m topk_log_probs, topk_running_sequences, topk_running_beam_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_top_k_continuations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   3318\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulated_log_probs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3319\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrunning_sequences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3320\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_beam_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrunning_beam_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3321\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcur_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcur_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3322\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_prompt_len\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_prompt_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3323\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdo_sample\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3324\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbeams_to_keep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeams_to_keep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3325\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_beams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3326\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3327\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   3328\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3330\u001b[0m \u001b[38;5;66;03m# d. Check which running sequences have finished\u001b[39;00m\n\u001b[0;32m   3331\u001b[0m next_token_hits_stopping_criteria \u001b[38;5;241m=\u001b[39m stopping_criteria(\n\u001b[0;32m   3332\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flatten_beam_dim(topk_running_sequences[:, :, : cur_len \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]),  \u001b[38;5;66;03m# remove unfilled token indexes\u001b[39;00m\n\u001b[0;32m   3333\u001b[0m     all_scores,\n\u001b[0;32m   3334\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\generation\\utils.py:3028\u001b[0m, in \u001b[0;36mGenerationMixin._get_top_k_continuations\u001b[1;34m(self, accumulated_log_probs, running_sequences, running_beam_indices, cur_len, decoder_prompt_len, do_sample, beams_to_keep, num_beams, vocab_size, batch_size)\u001b[0m\n\u001b[0;32m   3025\u001b[0m topk_running_sequences[:, :, cur_len] \u001b[38;5;241m=\u001b[39m topk_ids\n\u001b[0;32m   3027\u001b[0m \u001b[38;5;66;03m# we want to store the beam indices with batch information -> real beam index = beam index % num beams\u001b[39;00m\n\u001b[1;32m-> 3028\u001b[0m batch_offset \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtopk_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m num_beams\n\u001b[0;32m   3029\u001b[0m batch_modified_indices \u001b[38;5;241m=\u001b[39m topk_current_beam_indices \u001b[38;5;241m+\u001b[39m batch_offset\n\u001b[0;32m   3030\u001b[0m topk_running_beam_indices[:, :, cur_len \u001b[38;5;241m-\u001b[39m decoder_prompt_len] \u001b[38;5;241m=\u001b[39m batch_modified_indices\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import unicodedata\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "OUTPUT_DIR = \"mbart-large-50-cnn-summarizer-v14\" # trained model path\n",
    "METRIC_NAME = \"bleurt_f1\" # metric to find the best model\n",
    "DATA_PATH = \"../Dataset/new_large_CNN_dataset.csv\" # Path to original dataset\n",
    "MAX_SUMMARY_LENGTH_EVAL = 256\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"evaluate_checkpoints_log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] - %(message)s\", handlers=[logging.FileHandler(log_filename), logging.StreamHandler()])\n",
    "\n",
    "# --- Data Loading and Processing Functions (from the original training script) ---\n",
    "def sanitize_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    return text.replace('\"\"', '\"').strip()\n",
    "\n",
    "def normalize_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    return ' '.join(unicodedata.normalize('NFKC', text).split())\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        logging.info(\"--- Starting Post-Training Evaluation of Checkpoints ---\")\n",
    "        \n",
    "        # 1. --- Load and Prepare the Test Dataset ---\n",
    "        logging.info(\"Loading and preparing test data...\")\n",
    "        \n",
    "        df_new = pd.read_csv(DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        df_new.dropna(subset=['raw_news_article', 'english_summary', 'hindi_summary'], inplace=True)\n",
    "        \n",
    "        for col in ['raw_news_article', 'english_summary', 'hindi_summary']:\n",
    "            df_new[col] = df_new[col].apply(sanitize_text).apply(normalize_text)\n",
    "        \n",
    "        raw_dataset = Dataset.from_pandas(df_new)\n",
    "\n",
    "        def format_dataset_mbart(batch):\n",
    "            inputs, targets, langs = [], [], []\n",
    "            for article, eng_summary, hin_summary in zip(batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']):\n",
    "                if isinstance(article, str) and article:\n",
    "                    inputs.append(article); targets.append(eng_summary); langs.append(\"en_XX\")\n",
    "                    inputs.append(article); targets.append(hin_summary); langs.append(\"hi_IN\")\n",
    "            return {'article': inputs, 'summary': targets, 'target_lang': langs}\n",
    "\n",
    "        processed_dataset = raw_dataset.map(format_dataset_mbart, batched=True, remove_columns=raw_dataset.column_names)\n",
    "        \n",
    "        train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        test_dataset_untokenized = train_test_split['test']\n",
    "        logging.info(f\"Test data prepared with {len(test_dataset_untokenized)} examples.\")\n",
    "\n",
    "        # 2. --- Find all Checkpoints ---\n",
    "        checkpoint_dirs = sorted(\n",
    "            [d for d in os.listdir(OUTPUT_DIR) if d.startswith(\"checkpoint-\")],\n",
    "            key=lambda x: int(x.split('-')[-1])\n",
    "        )\n",
    "        if not checkpoint_dirs:\n",
    "            logging.error(f\"FATAL: No 'checkpoint-*' directories found in '{OUTPUT_DIR}'.\")\n",
    "            return\n",
    "        logging.info(f\"Found {len(checkpoint_dirs)} checkpoints to evaluate: {checkpoint_dirs}\")\n",
    "        \n",
    "        all_results = []\n",
    "        best_metric_value = None\n",
    "        best_checkpoint_path = None\n",
    "        metric_to_check = f\"eval_{METRIC_NAME}\"\n",
    "        is_loss = 'loss' in metric_to_check.lower()\n",
    "        \n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "        bleurt_metric = evaluate.load(\"bleurt\", \"bleurt-20\")\n",
    "\n",
    "        def compute_metrics_wrapper(eval_pred, tokenizer):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "            bleurt_result = bleurt_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "            result = {\"rouge1\": rouge_result[\"rouge1\"], \"rouge2\": rouge_result[\"rouge2\"], \"rougeL\": rouge_result[\"rougeL\"], \"bleurt_f1\": np.mean(bleurt_result[\"scores\"])}\n",
    "            return {f\"eval_{k}\": round(v * 100, 4) for k, v in result.items()}\n",
    "\n",
    "        # 3. --- Loop Through and Evaluate Each Checkpoint ---\n",
    "        for chkpt_dir in checkpoint_dirs:\n",
    "            chkpt_path = os.path.join(OUTPUT_DIR, chkpt_dir)\n",
    "            logging.info(f\"\\n--- Evaluating Checkpoint: {chkpt_path} ---\")\n",
    "            \n",
    "            model = MBartForConditionalGeneration.from_pretrained(chkpt_path)\n",
    "            tokenizer = MBart50TokenizerFast.from_pretrained(chkpt_path)\n",
    "            \n",
    "            def tokenize_for_eval(examples):\n",
    "                tokenizer.src_lang = \"en_XX\"\n",
    "                model_inputs = tokenizer(examples['article'], max_length=1024, truncation=True)\n",
    "                labels_batch = []\n",
    "                for i in range(len(examples['summary'])):\n",
    "                    tokenizer.tgt_lang = examples['target_lang'][i]\n",
    "                    labels = tokenizer(text_target=examples['summary'][i], max_length=MAX_SUMMARY_LENGTH_EVAL, truncation=True)\n",
    "                    labels_batch.append(labels['input_ids'])\n",
    "                model_inputs[\"labels\"] = labels_batch\n",
    "                return model_inputs\n",
    "\n",
    "            tokenized_test_dataset = test_dataset_untokenized.map(tokenize_for_eval, batched=True, remove_columns=['article', 'summary', 'target_lang'])\n",
    "\n",
    "            temp_training_args = Seq2SeqTrainingArguments(\n",
    "                output_dir=os.path.join(OUTPUT_DIR, \"temp_eval\"),\n",
    "                per_device_eval_batch_size=4,\n",
    "                predict_with_generate=True,\n",
    "                fp16=torch.cuda.is_available()\n",
    "            )\n",
    "\n",
    "            trainer = Seq2SeqTrainer(\n",
    "                model=model, args=temp_training_args,\n",
    "                eval_dataset=tokenized_test_dataset, tokenizer=tokenizer,\n",
    "                data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "                compute_metrics=lambda p: compute_metrics_wrapper(p, tokenizer)\n",
    "            )\n",
    "            \n",
    "            logging.info(f\"Running evaluation on {chkpt_dir}...\")\n",
    "            eval_results = trainer.evaluate()\n",
    "            \n",
    "            logging.info(f\"--- Results for {chkpt_dir} ---\")\n",
    "            for key, value in eval_results.items():\n",
    "                logging.info(f\"  - {key}: {value:.4f}\")\n",
    "            all_results.append({'checkpoint': chkpt_dir, **eval_results})\n",
    "            \n",
    "            metric_value = eval_results.get(metric_to_check)\n",
    "            if metric_value is not None:\n",
    "                if best_metric_value is None or (not is_loss and metric_value > best_metric_value) or (is_loss and metric_value < best_metric_value):\n",
    "                    best_metric_value, best_checkpoint_path = metric_value, chkpt_path\n",
    "                    logging.info(f\"*** New best checkpoint found: {chkpt_dir} with {metric_to_check}: {metric_value:.4f} ***\")\n",
    "\n",
    "        # 4. --- Print Final Summary Table and Save the Best ---\n",
    "        if not all_results:\n",
    "            logging.error(\"No checkpoints were successfully evaluated.\")\n",
    "            return\n",
    "\n",
    "        logging.info(\"\\n\" + \"=\"*80)\n",
    "        logging.info(\"--- FINAL EVALUATION SUMMARY ---\".center(80))\n",
    "        logging.info(\"=\"*80)\n",
    "        header = f\"{'Checkpoint':<20} | {'eval_loss':<12} | {'eval_rouge1':<12} | {'eval_rouge2':<12} | {'eval_rougeL':<12} | {'eval_bleurt_f1':<15}\"\n",
    "        logging.info(header)\n",
    "        logging.info(\"-\" * len(header))\n",
    "        for result in all_results:\n",
    "            row = f\"{result['checkpoint']:<20} | {result.get('eval_loss', 'N/A'):<12.4f} | {result.get('eval_eval_rouge1', 'N/A'):<12.4f} | {result.get('eval_eval_rouge2', 'N/A'):<12.4f} | {result.get('eval_eval_rougeL', 'N/A'):<12.4f} | {result.get(metric_to_check, 'N/A'):<15.4f}\"\n",
    "            logging.info(row)\n",
    "        logging.info(\"=\"*80)\n",
    "\n",
    "        if not best_checkpoint_path:\n",
    "            logging.error(\"Could not determine the best checkpoint after evaluation.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"\\n--- Best Model Identified ---\")\n",
    "        logging.info(f\"Checkpoint: {best_checkpoint_path}\")\n",
    "        logging.info(f\"Metric ({metric_to_check}): {best_metric_value:.4f}\")\n",
    "\n",
    "        final_model_path = os.path.join(OUTPUT_DIR, \"final_model\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            shutil.rmtree(final_model_path)\n",
    "            \n",
    "        shutil.copytree(best_checkpoint_path, final_model_path)\n",
    "        logging.info(f\"Successfully copied best model to: {final_model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import unicodedata\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "OUTPUT_DIR = \"mbart-large-50-cnn-summarizer-v14\" # trained model path\n",
    "METRIC_NAME = \"bleurt_f1\" # metric to find the best model\n",
    "DATA_PATH = \"../Dataset/new_large_CNN_dataset.csv\" # Path to original dataset\n",
    "MAX_SUMMARY_LENGTH_EVAL = 256\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"evaluate_checkpoints_log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] - %(message)s\", handlers=[logging.FileHandler(log_filename), logging.StreamHandler()])\n",
    "\n",
    "# --- Data Loading and Processing Functions (from the original training script) ---\n",
    "def sanitize_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    return text.replace('\"\"', '\"').strip()\n",
    "\n",
    "def normalize_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    return ' '.join(unicodedata.normalize('NFKC', text).split())\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        logging.info(\"--- Starting Post-Training Evaluation of Checkpoints ---\")\n",
    "        \n",
    "        # 1. --- Load and Prepare the Test Dataset ---\n",
    "        logging.info(\"Loading and preparing test data...\")\n",
    "        \n",
    "        df_new = pd.read_csv(DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        df_new.dropna(subset=['raw_news_article', 'english_summary', 'hindi_summary'], inplace=True)\n",
    "        \n",
    "        for col in ['raw_news_article', 'english_summary', 'hindi_summary']:\n",
    "            df_new[col] = df_new[col].apply(sanitize_text).apply(normalize_text)\n",
    "        \n",
    "        raw_dataset = Dataset.from_pandas(df_new)\n",
    "\n",
    "        def format_dataset_mbart(batch):\n",
    "            inputs, targets, langs = [], [], []\n",
    "            for article, eng_summary, hin_summary in zip(batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']):\n",
    "                if isinstance(article, str) and article:\n",
    "                    inputs.append(article); targets.append(eng_summary); langs.append(\"en_XX\")\n",
    "                    inputs.append(article); targets.append(hin_summary); langs.append(\"hi_IN\")\n",
    "            return {'article': inputs, 'summary': targets, 'target_lang': langs}\n",
    "\n",
    "        processed_dataset = raw_dataset.map(format_dataset_mbart, batched=True, remove_columns=raw_dataset.column_names)\n",
    "        \n",
    "        train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        test_dataset_untokenized = train_test_split['test']\n",
    "        logging.info(f\"Test data prepared with {len(test_dataset_untokenized)} examples.\")\n",
    "\n",
    "        # 2. --- Find all Checkpoints ---\n",
    "        checkpoint_dirs = sorted(\n",
    "            [d for d in os.listdir(OUTPUT_DIR) if d.startswith(\"checkpoint-\")],\n",
    "            key=lambda x: int(x.split('-')[-1])\n",
    "        )\n",
    "        if not checkpoint_dirs:\n",
    "            logging.error(f\"FATAL: No 'checkpoint-*' directories found in '{OUTPUT_DIR}'.\")\n",
    "            return\n",
    "        logging.info(f\"Found {len(checkpoint_dirs)} checkpoints to evaluate: {checkpoint_dirs}\")\n",
    "        \n",
    "        all_results = []\n",
    "        best_metric_value = None\n",
    "        best_checkpoint_path = None\n",
    "        metric_to_check = f\"eval_{METRIC_NAME}\"\n",
    "        is_loss = 'loss' in metric_to_check.lower()\n",
    "        \n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "        bleurt_metric = evaluate.load(\"bleurt\", \"bleurt-20\")\n",
    "\n",
    "        def compute_metrics_wrapper(eval_pred, tokenizer):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "            bleurt_result = bleurt_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "            result = {\"rouge1\": rouge_result[\"rouge1\"], \"rouge2\": rouge_result[\"rouge2\"], \"rougeL\": rouge_result[\"rougeL\"], \"bleurt_f1\": np.mean(bleurt_result[\"scores\"])}\n",
    "            return {f\"eval_{k}\": round(v * 100, 4) for k, v in result.items()}\n",
    "\n",
    "        # 3. --- Loop Through and Evaluate Each Checkpoint ---\n",
    "        for chkpt_dir in checkpoint_dirs:\n",
    "            chkpt_path = os.path.join(OUTPUT_DIR, chkpt_dir)\n",
    "            logging.info(f\"\\n--- Evaluating Checkpoint: {chkpt_path} ---\")\n",
    "            \n",
    "            model = MBartForConditionalGeneration.from_pretrained(chkpt_path)\n",
    "            tokenizer = MBart50TokenizerFast.from_pretrained(chkpt_path)\n",
    "            \n",
    "            def tokenize_for_eval(examples):\n",
    "                tokenizer.src_lang = \"en_XX\"\n",
    "                model_inputs = tokenizer(examples['article'], max_length=1024, truncation=True)\n",
    "                labels_batch = []\n",
    "                for i in range(len(examples['summary'])):\n",
    "                    tokenizer.tgt_lang = examples['target_lang'][i]\n",
    "                    labels = tokenizer(text_target=examples['summary'][i], max_length=MAX_SUMMARY_LENGTH_EVAL, truncation=True)\n",
    "                    labels_batch.append(labels['input_ids'])\n",
    "                model_inputs[\"labels\"] = labels_batch\n",
    "                return model_inputs\n",
    "\n",
    "            tokenized_test_dataset = test_dataset_untokenized.map(tokenize_for_eval, batched=True, remove_columns=['article', 'summary', 'target_lang'])\n",
    "\n",
    "            temp_training_args = Seq2SeqTrainingArguments(\n",
    "                output_dir=os.path.join(OUTPUT_DIR, \"temp_eval\"),\n",
    "                per_device_eval_batch_size=4,\n",
    "                predict_with_generate=True,\n",
    "                fp16=torch.cuda.is_available()\n",
    "            )\n",
    "\n",
    "            trainer = Seq2SeqTrainer(\n",
    "                model=model, args=temp_training_args,\n",
    "                eval_dataset=tokenized_test_dataset, tokenizer=tokenizer,\n",
    "                data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "                compute_metrics=lambda p: compute_metrics_wrapper(p, tokenizer)\n",
    "            )\n",
    "            \n",
    "            logging.info(f\"Running evaluation on {chkpt_dir}...\")\n",
    "            eval_results = trainer.evaluate()\n",
    "            \n",
    "            logging.info(f\"--- Results for {chkpt_dir} ---\")\n",
    "            for key, value in eval_results.items():\n",
    "                logging.info(f\"  - {key}: {value:.4f}\")\n",
    "            all_results.append({'checkpoint': chkpt_dir, **eval_results})\n",
    "            \n",
    "            metric_value = eval_results.get(metric_to_check)\n",
    "            if metric_value is not None:\n",
    "                if best_metric_value is None or (not is_loss and metric_value > best_metric_value) or (is_loss and metric_value < best_metric_value):\n",
    "                    best_metric_value, best_checkpoint_path = metric_value, chkpt_path\n",
    "                    logging.info(f\"*** New best checkpoint found: {chkpt_dir} with {metric_to_check}: {metric_value:.4f} ***\")\n",
    "\n",
    "        # 4. --- Print Final Summary Table and Save the Best ---\n",
    "        if not all_results:\n",
    "            logging.error(\"No checkpoints were successfully evaluated.\")\n",
    "            return\n",
    "\n",
    "        logging.info(\"\\n\" + \"=\"*80)\n",
    "        logging.info(\"--- FINAL EVALUATION SUMMARY ---\".center(80))\n",
    "        logging.info(\"=\"*80)\n",
    "        header = f\"{'Checkpoint':<20} | {'eval_loss':<12} | {'eval_rouge1':<12} | {'eval_rouge2':<12} | {'eval_rougeL':<12} | {'eval_bleurt_f1':<15}\"\n",
    "        logging.info(header)\n",
    "        logging.info(\"-\" * len(header))\n",
    "        for result in all_results:\n",
    "            row = f\"{result['checkpoint']:<20} | {result.get('eval_loss', 'N/A'):<12.4f} | {result.get('eval_eval_rouge1', 'N/A'):<12.4f} | {result.get('eval_eval_rouge2', 'N/A'):<12.4f} | {result.get('eval_eval_rougeL', 'N/A'):<12.4f} | {result.get(metric_to_check, 'N/A'):<15.4f}\"\n",
    "            logging.info(row)\n",
    "        logging.info(\"=\"*80)\n",
    "\n",
    "        if not best_checkpoint_path:\n",
    "            logging.error(\"Could not determine the best checkpoint after evaluation.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"\\n--- Best Model Identified ---\")\n",
    "        logging.info(f\"Checkpoint: {best_checkpoint_path}\")\n",
    "        logging.info(f\"Metric ({metric_to_check}): {best_metric_value:.4f}\")\n",
    "\n",
    "        final_model_path = os.path.join(OUTPUT_DIR, \"final_model\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            shutil.rmtree(final_model_path)\n",
    "            \n",
    "        shutil.copytree(best_checkpoint_path, final_model_path)\n",
    "        logging.info(f\"Successfully copied best model to: {final_model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1253adfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 00:05:37,655 [INFO] - --- Starting Post-Training Evaluation of Checkpoints (Optimized) ---\n",
      "2025-10-08 00:05:37,657 [INFO] - Loading and preparing test data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f4b600408c43919de20531511dc7f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 00:05:40,845 [WARNING] - Using a subset of 500 examples for quick evaluation.\n",
      "2025-10-08 00:05:40,845 [INFO] - Test data prepared with 500 examples.\n",
      "2025-10-08 00:05:40,845 [INFO] - Found 4 checkpoints to evaluate: ['checkpoint-2076', 'checkpoint-4152', 'checkpoint-6228', 'checkpoint-8304']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\bleurt-20\\downloads\\extracted\\8db8856a80394ae84b010e83ab663d4a3ccfa244ce3d0dbe00143f73e65ff123\\BLEURT-20.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 00:05:53,257 [INFO] - Reading checkpoint C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\bleurt-20\\downloads\\extracted\\8db8856a80394ae84b010e83ab663d4a3ccfa244ce3d0dbe00143f73e65ff123\\BLEURT-20.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 00:05:53,257 [INFO] - Config file found, reading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 00:05:53,270 [INFO] - Will load checkpoint BLEURT-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 00:05:53,270 [INFO] - Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:BLEURT-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 00:05:53,273 [INFO] - ... name:BLEURT-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 00:05:53,273 [INFO] - ... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 00:05:53,279 [INFO] - ... max_seq_length:512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 00:05:53,281 [INFO] - ... vocab_file:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 00:05:53,281 [INFO] - ... do_lower_case:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... sp_model:sent_piece\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 00:05:53,281 [INFO] - ... sp_model:sent_piece\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... dynamic_seq_length:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 00:05:53,281 [INFO] - ... dynamic_seq_length:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 00:05:53,281 [INFO] - Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 00:05:53,290 [INFO] - Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 00:05:53,292 [INFO] - Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load model: C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\bleurt-20\\downloads\\extracted\\8db8856a80394ae84b010e83ab663d4a3ccfa244ce3d0dbe00143f73e65ff123\\BLEURT-20\\sent_piece.model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 00:05:53,293 [INFO] - Will load model: C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\bleurt-20\\downloads\\extracted\\8db8856a80394ae84b010e83ab663d4a3ccfa244ce3d0dbe00143f73e65ff123\\BLEURT-20\\sent_piece.model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SentencePiece tokenizer created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 00:05:53,760 [INFO] - SentencePiece tokenizer created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 00:05:53,761 [INFO] - Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 00:05:53,761 [INFO] - Loading model.\n",
      "2025-10-08 00:05:59,845 [INFO] - Fingerprint not found. Saved model loading will continue.\n",
      "2025-10-08 00:05:59,856 [INFO] - path_and_singleprint metric could not be logged. Saved model loading will continue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 00:05:59,860 [INFO] - BLEURT initialized.\n",
      "2025-10-08 00:05:59,860 [INFO] - \n",
      "--- Evaluating Checkpoint: mbart-large-50-cnn-summarizer-v14\\checkpoint-2076 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dfdd093a86f40dcad04380e7c5b6fea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_17504\\1921915600.py:133: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "2025-10-08 00:06:03,694 [INFO] - Running evaluation on checkpoint-2076...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 07:46]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 00:14:13,634 [INFO] - Using default tokenizer.\n",
      "2025-10-08 00:25:55,794 [INFO] - --- Results for checkpoint-2076 ---\n",
      "2025-10-08 00:25:55,796 [INFO] -   - eval_rouge1: 0.0409\n",
      "2025-10-08 00:25:55,797 [INFO] -   - eval_rouge2: 0.021\n",
      "2025-10-08 00:25:55,799 [INFO] -   - eval_rougeL: 0.0276\n",
      "2025-10-08 00:25:55,799 [INFO] -   - eval_bleurt_f1: 0.2583\n",
      "2025-10-08 00:25:55,800 [INFO] -   - eval_loss: 1.6859\n",
      "2025-10-08 00:25:55,801 [INFO] -   - eval_model_preparation_time: 0.0048\n",
      "2025-10-08 00:25:55,801 [INFO] -   - eval_runtime: 1192.0459\n",
      "2025-10-08 00:25:55,802 [INFO] -   - eval_samples_per_second: 0.419\n",
      "2025-10-08 00:25:55,804 [INFO] -   - eval_steps_per_second: 0.027\n",
      "2025-10-08 00:25:55,805 [INFO] - *** New best checkpoint found: checkpoint-2076 with eval_bleurt_f1: 0.2583 ***\n",
      "2025-10-08 00:25:55,806 [INFO] - \n",
      "--- Evaluating Checkpoint: mbart-large-50-cnn-summarizer-v14\\checkpoint-4152 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb8d82766a14461fba324be4d5f310be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 00:25:59,911 [INFO] - Running evaluation on checkpoint-4152...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 07:51]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 00:34:06,833 [INFO] - Using default tokenizer.\n",
      "2025-10-08 00:45:50,066 [INFO] - --- Results for checkpoint-4152 ---\n",
      "2025-10-08 00:45:50,068 [INFO] -   - eval_rouge1: 0.2696\n",
      "2025-10-08 00:45:50,068 [INFO] -   - eval_rouge2: 0.106\n",
      "2025-10-08 00:45:50,068 [INFO] -   - eval_rougeL: 0.2434\n",
      "2025-10-08 00:45:50,068 [INFO] -   - eval_bleurt_f1: 0.4036\n",
      "2025-10-08 00:45:50,072 [INFO] -   - eval_loss: 1.5638\n",
      "2025-10-08 00:45:50,072 [INFO] -   - eval_model_preparation_time: 0.0039\n",
      "2025-10-08 00:45:50,072 [INFO] -   - eval_runtime: 1189.7429\n",
      "2025-10-08 00:45:50,072 [INFO] -   - eval_samples_per_second: 0.42\n",
      "2025-10-08 00:45:50,077 [INFO] -   - eval_steps_per_second: 0.027\n",
      "2025-10-08 00:45:50,077 [INFO] - *** New best checkpoint found: checkpoint-4152 with eval_bleurt_f1: 0.4036 ***\n",
      "2025-10-08 00:45:50,080 [INFO] - \n",
      "--- Evaluating Checkpoint: mbart-large-50-cnn-summarizer-v14\\checkpoint-6228 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16b1aa61b9564b2d9377cdbf9d31423e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 00:45:54,015 [INFO] - Running evaluation on checkpoint-6228...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 4/32 00:55 < 08:39, 0.05 it/s]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 191\u001b[0m\n\u001b[0;32m    188\u001b[0m         logging\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn unexpected error occurred: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 191\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 141\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    133\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[0;32m    134\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel, args\u001b[38;5;241m=\u001b[39mtemp_training_args,\n\u001b[0;32m    135\u001b[0m     eval_dataset\u001b[38;5;241m=\u001b[39mtokenized_test_dataset, tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m    136\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mDataCollatorForSeq2Seq(tokenizer, model\u001b[38;5;241m=\u001b[39mmodel),\n\u001b[0;32m    137\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m p: compute_metrics_wrapper(p, tokenizer)\n\u001b[0;32m    138\u001b[0m )\n\u001b[0;32m    140\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRunning evaluation on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchkpt_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 141\u001b[0m eval_results \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;66;03m# Round metrics for logging\u001b[39;00m\n\u001b[0;32m    144\u001b[0m rounded_results \u001b[38;5;241m=\u001b[39m {k: \u001b[38;5;28mround\u001b[39m(v, \u001b[38;5;241m4\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m eval_results\u001b[38;5;241m.\u001b[39mitems()}\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\trainer_seq2seq.py:191\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mgather\n\u001b[0;32m    190\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gen_kwargs \u001b[38;5;241m=\u001b[39m gen_kwargs\n\u001b[1;32m--> 191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43meval_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\trainer.py:4489\u001b[0m, in \u001b[0;36mTrainer.evaluate\u001b[1;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4486\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m   4488\u001b[0m eval_loop \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprediction_loop \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39muse_legacy_prediction_loop \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluation_loop\n\u001b[1;32m-> 4489\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43meval_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4490\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdescription\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEvaluation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4492\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001b[39;49;00m\n\u001b[0;32m   4493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# self.args.prediction_loss_only\u001b[39;49;00m\n\u001b[0;32m   4494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_metrics\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   4495\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4496\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric_key_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4497\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4499\u001b[0m total_batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39meval_batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mworld_size\n\u001b[0;32m   4500\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetric_key_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_jit_compilation_time\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m output\u001b[38;5;241m.\u001b[39mmetrics:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\trainer.py:4685\u001b[0m, in \u001b[0;36mTrainer.evaluation_loop\u001b[1;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[0;32m   4682\u001b[0m         batch_size \u001b[38;5;241m=\u001b[39m observed_batch_size\n\u001b[0;32m   4684\u001b[0m \u001b[38;5;66;03m# Prediction step\u001b[39;00m\n\u001b[1;32m-> 4685\u001b[0m losses, logits, labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprediction_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction_loss_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4686\u001b[0m main_input_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain_input_name\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   4687\u001b[0m inputs_decode \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   4688\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_input(inputs[main_input_name]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m args\u001b[38;5;241m.\u001b[39minclude_for_metrics \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4689\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\trainer_seq2seq.py:327\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.prediction_step\u001b[1;34m(self, model, inputs, prediction_loss_only, ignore_keys, **gen_kwargs)\u001b[0m\n\u001b[0;32m    320\u001b[0m summon_full_params_context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    321\u001b[0m     FullyShardedDataParallel\u001b[38;5;241m.\u001b[39msummon_full_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[0;32m    322\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, FullyShardedDataParallel)\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext()\n\u001b[0;32m    324\u001b[0m )\n\u001b[0;32m    326\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m summon_full_params_context:\n\u001b[1;32m--> 327\u001b[0m     generated_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgen_kwargs)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;66;03m# Temporary hack to ensure the generation config is not initialized for each iteration of the evaluation loop\u001b[39;00m\n\u001b[0;32m    330\u001b[0m \u001b[38;5;66;03m# TODO: remove this hack when the legacy code that initializes generation_config from a model config is\u001b[39;00m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# removed in https://github.com/huggingface/transformers/blob/98d88b23f54e5a23e741833f1e973fdf600cc2c5/src/transformers/generation/utils.py#L1183\u001b[39;00m\n\u001b[0;32m    332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39m_from_model_config:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\generation\\utils.py:2564\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[1;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[0;32m   2561\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m generation_config\u001b[38;5;241m.\u001b[39muse_cache\n\u001b[0;32m   2563\u001b[0m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[1;32m-> 2564\u001b[0m result \u001b[38;5;241m=\u001b[39m decoding_method(\n\u001b[0;32m   2565\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   2566\u001b[0m     input_ids,\n\u001b[0;32m   2567\u001b[0m     logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[0;32m   2568\u001b[0m     stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[0;32m   2569\u001b[0m     generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[0;32m   2570\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgeneration_mode_kwargs,\n\u001b[0;32m   2571\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m   2572\u001b[0m )\n\u001b[0;32m   2574\u001b[0m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[0;32m   2575\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2576\u001b[0m     generation_config\u001b[38;5;241m.\u001b[39mreturn_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   2577\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpast_key_values\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   2578\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result\u001b[38;5;241m.\u001b[39mpast_key_values, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto_legacy_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   2579\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\generation\\utils.py:3260\u001b[0m, in \u001b[0;36mGenerationMixin._beam_search\u001b[1;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[0;32m   3257\u001b[0m beam_indices \u001b[38;5;241m=\u001b[39m running_beam_indices\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mclone()\n\u001b[0;32m   3259\u001b[0m \u001b[38;5;66;03m# 4. run the generation loop\u001b[39;00m\n\u001b[1;32m-> 3260\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_has_unfinished_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthis_peer_finished\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   3261\u001b[0m     \u001b[38;5;66;03m# a. Forward current tokens, obtain the logits\u001b[39;00m\n\u001b[0;32m   3262\u001b[0m     flat_running_sequences \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flatten_beam_dim(running_sequences[:, :, :cur_len])\n\u001b[0;32m   3263\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(flat_running_sequences, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\generation\\utils.py:2583\u001b[0m, in \u001b[0;36mGenerationMixin._has_unfinished_sequences\u001b[1;34m(self, this_peer_finished, synced_gpus, device)\u001b[0m\n\u001b[0;32m   2580\u001b[0m         result\u001b[38;5;241m.\u001b[39mpast_key_values \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mpast_key_values\u001b[38;5;241m.\u001b[39mto_legacy_cache()\n\u001b[0;32m   2581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m-> 2583\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_has_unfinished_sequences\u001b[39m(\u001b[38;5;28mself\u001b[39m, this_peer_finished: \u001b[38;5;28mbool\u001b[39m, synced_gpus: \u001b[38;5;28mbool\u001b[39m, device: torch\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[0;32m   2584\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   2585\u001b[0m \u001b[38;5;124;03m    Returns whether there are still unfinished sequences in the device. The existence of unfinished sequences is\u001b[39;00m\n\u001b[0;32m   2586\u001b[0m \u001b[38;5;124;03m    fed through `this_peer_finished`. ZeRO stage 3-friendly.\u001b[39;00m\n\u001b[0;32m   2587\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   2588\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m synced_gpus:\n\u001b[0;32m   2589\u001b[0m         \u001b[38;5;66;03m# Under synced_gpus the `forward` call must continue until all gpus complete their sequence.\u001b[39;00m\n\u001b[0;32m   2590\u001b[0m         \u001b[38;5;66;03m# The following logic allows an early break if all peers finished generating their sequence\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import unicodedata\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "OUTPUT_DIR = \"mbart-large-50-cnn-summarizer-v14\" \n",
    "METRIC_NAME = \"bleurt_f1\" \n",
    "DATA_PATH = \"../Dataset/new_large_CNN_dataset.csv\"\n",
    "MAX_SUMMARY_LENGTH_EVAL = 256\n",
    "# --- OPTIMIZATION: Increase batch size for faster evaluation ---\n",
    "EVAL_BATCH_SIZE = 16 \n",
    "# --- OPTIONAL: Set to a number (e.g., 500) for a quick evaluation on a subset, or None for the full dataset ---\n",
    "NUM_EVAL_SAMPLES = 500 \n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"evaluate_checkpoints_log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] - %(message)s\", handlers=[logging.FileHandler(log_filename), logging.StreamHandler()])\n",
    "\n",
    "def sanitize_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    return text.replace('\"\"', '\"').strip()\n",
    "\n",
    "def normalize_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    return ' '.join(unicodedata.normalize('NFKC', text).split())\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        logging.info(\"--- Starting Post-Training Evaluation of Checkpoints (Optimized) ---\")\n",
    "        \n",
    "        # 1. --- Load and Prepare the Test Dataset ---\n",
    "        logging.info(\"Loading and preparing test data...\")\n",
    "        \n",
    "        df_new = pd.read_csv(DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        df_new.dropna(subset=['raw_news_article', 'english_summary', 'hindi_summary'], inplace=True)\n",
    "        \n",
    "        for col in ['raw_news_article', 'english_summary', 'hindi_summary']:\n",
    "            df_new[col] = df_new[col].apply(sanitize_text).apply(normalize_text)\n",
    "        \n",
    "        raw_dataset = Dataset.from_pandas(df_new)\n",
    "\n",
    "        def format_dataset_mbart(batch):\n",
    "            inputs, targets, langs = [], [], []\n",
    "            for article, eng_summary, hin_summary in zip(batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']):\n",
    "                if isinstance(article, str) and article:\n",
    "                    inputs.append(article); targets.append(eng_summary); langs.append(\"en_XX\")\n",
    "                    inputs.append(article); targets.append(hin_summary); langs.append(\"hi_IN\")\n",
    "            return {'article': inputs, 'summary': targets, 'target_lang': langs}\n",
    "\n",
    "        processed_dataset = raw_dataset.map(format_dataset_mbart, batched=True, remove_columns=raw_dataset.column_names)\n",
    "        \n",
    "        train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        test_dataset_untokenized = train_test_split['test']\n",
    "        \n",
    "        if NUM_EVAL_SAMPLES:\n",
    "            logging.warning(f\"Using a subset of {NUM_EVAL_SAMPLES} examples for quick evaluation.\")\n",
    "            test_dataset_untokenized = test_dataset_untokenized.select(range(NUM_EVAL_SAMPLES))\n",
    "\n",
    "        logging.info(f\"Test data prepared with {len(test_dataset_untokenized)} examples.\")\n",
    "\n",
    "        # 2. --- Find all Checkpoints ---\n",
    "        checkpoint_dirs = sorted(\n",
    "            [d for d in os.listdir(OUTPUT_DIR) if d.startswith(\"checkpoint-\")],\n",
    "            key=lambda x: int(x.split('-')[-1])\n",
    "        )\n",
    "        if not checkpoint_dirs:\n",
    "            logging.error(f\"FATAL: No 'checkpoint-*' directories found in '{OUTPUT_DIR}'.\")\n",
    "            return\n",
    "        logging.info(f\"Found {len(checkpoint_dirs)} checkpoints to evaluate: {checkpoint_dirs}\")\n",
    "        \n",
    "        all_results = []\n",
    "        best_metric_value = None\n",
    "        best_checkpoint_path = None\n",
    "        metric_to_check = f\"eval_{METRIC_NAME}\"\n",
    "        is_loss = 'loss' in metric_to_check.lower()\n",
    "        \n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "        bleurt_metric = evaluate.load(\"bleurt\", \"bleurt-20\")\n",
    "\n",
    "        def compute_metrics_wrapper(eval_pred, tokenizer):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "            bleurt_result = bleurt_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "            result = {\"rouge1\": rouge_result[\"rouge1\"], \"rouge2\": rouge_result[\"rouge2\"], \"rougeL\": rouge_result[\"rougeL\"], \"bleurt_f1\": np.mean(bleurt_result[\"scores\"])}\n",
    "            return {f\"eval_{k}\": v for k, v in result.items()} # Return raw scores for averaging\n",
    "\n",
    "        # 3. --- Loop Through and Evaluate Each Checkpoint ---\n",
    "        for chkpt_dir in checkpoint_dirs:\n",
    "            chkpt_path = os.path.join(OUTPUT_DIR, chkpt_dir)\n",
    "            logging.info(f\"\\n--- Evaluating Checkpoint: {chkpt_path} ---\")\n",
    "            \n",
    "            model = MBartForConditionalGeneration.from_pretrained(chkpt_path)\n",
    "            tokenizer = MBart50TokenizerFast.from_pretrained(chkpt_path)\n",
    "            \n",
    "            def tokenize_for_eval(examples):\n",
    "                tokenizer.src_lang = \"en_XX\"\n",
    "                model_inputs = tokenizer(examples['article'], max_length=1024, truncation=True)\n",
    "                labels_batch = []\n",
    "                for i in range(len(examples['summary'])):\n",
    "                    tokenizer.tgt_lang = examples['target_lang'][i]\n",
    "                    labels = tokenizer(text_target=examples['summary'][i], max_length=MAX_SUMMARY_LENGTH_EVAL, truncation=True)\n",
    "                    labels_batch.append(labels['input_ids'])\n",
    "                model_inputs[\"labels\"] = labels_batch\n",
    "                return model_inputs\n",
    "\n",
    "            tokenized_test_dataset = test_dataset_untokenized.map(tokenize_for_eval, batched=True, remove_columns=['article', 'summary', 'target_lang'])\n",
    "\n",
    "            temp_training_args = Seq2SeqTrainingArguments(\n",
    "                output_dir=os.path.join(OUTPUT_DIR, \"temp_eval\"),\n",
    "                per_device_eval_batch_size=EVAL_BATCH_SIZE, # Using the optimized batch size\n",
    "                predict_with_generate=True,\n",
    "                fp16=torch.cuda.is_available()\n",
    "            )\n",
    "\n",
    "            trainer = Seq2SeqTrainer(\n",
    "                model=model, args=temp_training_args,\n",
    "                eval_dataset=tokenized_test_dataset, tokenizer=tokenizer,\n",
    "                data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "                compute_metrics=lambda p: compute_metrics_wrapper(p, tokenizer)\n",
    "            )\n",
    "            \n",
    "            logging.info(f\"Running evaluation on {chkpt_dir}...\")\n",
    "            eval_results = trainer.evaluate()\n",
    "            \n",
    "            # Round metrics for logging\n",
    "            rounded_results = {k: round(v, 4) for k, v in eval_results.items()}\n",
    "            logging.info(f\"--- Results for {chkpt_dir} ---\")\n",
    "            for key, value in rounded_results.items():\n",
    "                logging.info(f\"  - {key}: {value}\")\n",
    "            all_results.append({'checkpoint': chkpt_dir, **rounded_results})\n",
    "            \n",
    "            metric_value = eval_results.get(metric_to_check)\n",
    "            if metric_value is not None:\n",
    "                if best_metric_value is None or (not is_loss and metric_value > best_metric_value) or (is_loss and metric_value < best_metric_value):\n",
    "                    best_metric_value, best_checkpoint_path = metric_value, chkpt_path\n",
    "                    logging.info(f\"*** New best checkpoint found: {chkpt_dir} with {metric_to_check}: {metric_value:.4f} ***\")\n",
    "\n",
    "        # 4. --- Print Final Summary Table and Save the Best ---\n",
    "        if not all_results:\n",
    "            logging.error(\"No checkpoints were successfully evaluated.\")\n",
    "            return\n",
    "\n",
    "        logging.info(\"\\n\" + \"=\"*80)\n",
    "        logging.info(\"--- FINAL EVALUATION SUMMARY ---\".center(80))\n",
    "        logging.info(\"=\"*80)\n",
    "        header = f\"{'Checkpoint':<20} | {'eval_loss':<12} | {'eval_rouge1':<12} | {'eval_rouge2':<12} | {'eval_rougeL':<12} | {'eval_bleurt_f1':<15}\"\n",
    "        logging.info(header)\n",
    "        logging.info(\"-\" * len(header))\n",
    "        for result in all_results:\n",
    "            row = f\"{result['checkpoint']:<20} | {result.get('eval_loss', 'N/A'):<12.4f} | {result.get('eval_eval_rouge1', 'N/A'):<12.4f} | {result.get('eval_eval_rouge2', 'N/A'):<12.4f} | {result.get('eval_eval_rougeL', 'N/A'):<12.4f} | {result.get(metric_to_check, 'N/A'):<15.4f}\"\n",
    "            logging.info(row)\n",
    "        logging.info(\"=\"*80)\n",
    "\n",
    "        if not best_checkpoint_path:\n",
    "            logging.error(\"Could not determine the best checkpoint after evaluation.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"\\n--- Best Model Identified ---\")\n",
    "        logging.info(f\"Checkpoint: {best_checkpoint_path}\")\n",
    "        logging.info(f\"Metric ({metric_to_check}): {best_metric_value:.4f}\")\n",
    "\n",
    "        final_model_path = os.path.join(OUTPUT_DIR, \"final_model\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            shutil.rmtree(final_model_path)\n",
    "            \n",
    "        shutil.copytree(best_checkpoint_path, final_model_path)\n",
    "        logging.info(f\"Successfully copied best model to: {final_model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "19b8d7c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:07:05,211 [INFO] - --- Starting Post-Training Evaluation of Checkpoints (Optimized) ---\n",
      "2025-10-08 01:07:05,212 [INFO] - Loading and preparing test data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a425fbb2ae8453b94c6d70b7962da8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:07:08,338 [WARNING] - Using a subset of 500 examples for quick evaluation.\n",
      "2025-10-08 01:07:08,338 [INFO] - Test data prepared with 500 examples.\n",
      "2025-10-08 01:07:08,338 [INFO] - Found 4 checkpoints to evaluate: ['checkpoint-2076', 'checkpoint-4152', 'checkpoint-6228', 'checkpoint-8304']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\bleurt-20\\downloads\\extracted\\8db8856a80394ae84b010e83ab663d4a3ccfa244ce3d0dbe00143f73e65ff123\\BLEURT-20.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:07:20,768 [INFO] - Reading checkpoint C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\bleurt-20\\downloads\\extracted\\8db8856a80394ae84b010e83ab663d4a3ccfa244ce3d0dbe00143f73e65ff123\\BLEURT-20.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:07:20,768 [INFO] - Config file found, reading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:07:20,768 [INFO] - Will load checkpoint BLEURT-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:07:20,768 [INFO] - Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:BLEURT-20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:07:20,768 [INFO] - ... name:BLEURT-20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:07:20,768 [INFO] - ... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:07:20,768 [INFO] - ... max_seq_length:512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:07:20,768 [INFO] - ... vocab_file:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:07:20,768 [INFO] - ... do_lower_case:None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... sp_model:sent_piece\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:07:20,784 [INFO] - ... sp_model:sent_piece\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... dynamic_seq_length:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:07:20,785 [INFO] - ... dynamic_seq_length:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:07:20,787 [INFO] - Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:07:20,790 [INFO] - Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:07:20,791 [INFO] - Creating SentencePiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load model: C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\bleurt-20\\downloads\\extracted\\8db8856a80394ae84b010e83ab663d4a3ccfa244ce3d0dbe00143f73e65ff123\\BLEURT-20\\sent_piece.model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:07:20,791 [INFO] - Will load model: C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\bleurt-20\\downloads\\extracted\\8db8856a80394ae84b010e83ab663d4a3ccfa244ce3d0dbe00143f73e65ff123\\BLEURT-20\\sent_piece.model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:SentencePiece tokenizer created.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:07:21,268 [INFO] - SentencePiece tokenizer created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:07:21,271 [INFO] - Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:07:21,272 [INFO] - Loading model.\n",
      "2025-10-08 01:07:27,084 [INFO] - Fingerprint not found. Saved model loading will continue.\n",
      "2025-10-08 01:07:27,084 [INFO] - path_and_singleprint metric could not be logged. Saved model loading will continue.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:07:27,094 [INFO] - BLEURT initialized.\n",
      "2025-10-08 01:07:27,094 [INFO] - \n",
      "--- Evaluating Checkpoint: mbart-large-50-cnn-summarizer-v14\\checkpoint-2076 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4a0488697f1487082359555043d99b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_22992\\1921915600.py:133: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "2025-10-08 01:07:31,553 [INFO] - Running evaluation on checkpoint-2076...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 07:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:14:46,765 [INFO] - Using default tokenizer.\n",
      "2025-10-08 01:25:16,994 [INFO] - --- Results for checkpoint-2076 ---\n",
      "2025-10-08 01:25:16,997 [INFO] -   - eval_rouge1: 0.0409\n",
      "2025-10-08 01:25:16,998 [INFO] -   - eval_rouge2: 0.0209\n",
      "2025-10-08 01:25:16,998 [INFO] -   - eval_rougeL: 0.0275\n",
      "2025-10-08 01:25:16,998 [INFO] -   - eval_bleurt_f1: 0.2583\n",
      "2025-10-08 01:25:16,998 [INFO] -   - eval_loss: 1.6859\n",
      "2025-10-08 01:25:16,998 [INFO] -   - eval_model_preparation_time: 0.0\n",
      "2025-10-08 01:25:17,003 [INFO] -   - eval_runtime: 1064.9994\n",
      "2025-10-08 01:25:17,004 [INFO] -   - eval_samples_per_second: 0.469\n",
      "2025-10-08 01:25:17,006 [INFO] -   - eval_steps_per_second: 0.03\n",
      "2025-10-08 01:25:17,006 [INFO] - *** New best checkpoint found: checkpoint-2076 with eval_bleurt_f1: 0.2583 ***\n",
      "2025-10-08 01:25:17,006 [INFO] - \n",
      "--- Evaluating Checkpoint: mbart-large-50-cnn-summarizer-v14\\checkpoint-4152 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69d6ee1d1a147a09c8262215b9951c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:25:20,924 [INFO] - Running evaluation on checkpoint-4152...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 07:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:32:34,883 [INFO] - Using default tokenizer.\n",
      "2025-10-08 01:43:15,831 [INFO] - --- Results for checkpoint-4152 ---\n",
      "2025-10-08 01:43:15,831 [INFO] -   - eval_rouge1: 0.2682\n",
      "2025-10-08 01:43:15,831 [INFO] -   - eval_rouge2: 0.1067\n",
      "2025-10-08 01:43:15,831 [INFO] -   - eval_rougeL: 0.2432\n",
      "2025-10-08 01:43:15,831 [INFO] -   - eval_bleurt_f1: 0.4036\n",
      "2025-10-08 01:43:15,831 [INFO] -   - eval_loss: 1.5638\n",
      "2025-10-08 01:43:15,831 [INFO] -   - eval_model_preparation_time: 0.0055\n",
      "2025-10-08 01:43:15,831 [INFO] -   - eval_runtime: 1074.5817\n",
      "2025-10-08 01:43:15,839 [INFO] -   - eval_samples_per_second: 0.465\n",
      "2025-10-08 01:43:15,839 [INFO] -   - eval_steps_per_second: 0.03\n",
      "2025-10-08 01:43:15,843 [INFO] - *** New best checkpoint found: checkpoint-4152 with eval_bleurt_f1: 0.4036 ***\n",
      "2025-10-08 01:43:15,844 [INFO] - \n",
      "--- Evaluating Checkpoint: mbart-large-50-cnn-summarizer-v14\\checkpoint-6228 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bbe3ff32e3940d99e31e9964053f6f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:43:19,693 [INFO] - Running evaluation on checkpoint-6228...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 07:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:50:42,969 [INFO] - Using default tokenizer.\n",
      "2025-10-08 02:01:50,847 [INFO] - --- Results for checkpoint-6228 ---\n",
      "2025-10-08 02:01:50,847 [INFO] -   - eval_rouge1: 0.2363\n",
      "2025-10-08 02:01:50,847 [INFO] -   - eval_rouge2: 0.0982\n",
      "2025-10-08 02:01:50,863 [INFO] -   - eval_rougeL: 0.2146\n",
      "2025-10-08 02:01:50,864 [INFO] -   - eval_bleurt_f1: 0.3982\n",
      "2025-10-08 02:01:50,864 [INFO] -   - eval_loss: 1.5082\n",
      "2025-10-08 02:01:50,864 [INFO] -   - eval_model_preparation_time: 0.0136\n",
      "2025-10-08 02:01:50,864 [INFO] -   - eval_runtime: 1110.6297\n",
      "2025-10-08 02:01:50,864 [INFO] -   - eval_samples_per_second: 0.45\n",
      "2025-10-08 02:01:50,864 [INFO] -   - eval_steps_per_second: 0.029\n",
      "2025-10-08 02:01:50,864 [INFO] - \n",
      "--- Evaluating Checkpoint: mbart-large-50-cnn-summarizer-v14\\checkpoint-8304 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ddb1561a9534ae290f8a50ac3d756d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 02:01:54,763 [INFO] - Running evaluation on checkpoint-8304...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='32' max='32' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [32/32 07:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 02:09:18,003 [INFO] - Using default tokenizer.\n",
      "2025-10-08 02:20:39,800 [INFO] - --- Results for checkpoint-8304 ---\n",
      "2025-10-08 02:20:39,800 [INFO] -   - eval_rouge1: 0.249\n",
      "2025-10-08 02:20:39,800 [INFO] -   - eval_rouge2: 0.111\n",
      "2025-10-08 02:20:39,800 [INFO] -   - eval_rougeL: 0.2186\n",
      "2025-10-08 02:20:39,816 [INFO] -   - eval_bleurt_f1: 0.3965\n",
      "2025-10-08 02:20:39,817 [INFO] -   - eval_loss: 1.5014\n",
      "2025-10-08 02:20:39,818 [INFO] -   - eval_model_preparation_time: 0.0167\n",
      "2025-10-08 02:20:39,819 [INFO] -   - eval_runtime: 1124.7317\n",
      "2025-10-08 02:20:39,820 [INFO] -   - eval_samples_per_second: 0.445\n",
      "2025-10-08 02:20:39,821 [INFO] -   - eval_steps_per_second: 0.028\n",
      "2025-10-08 02:20:39,822 [INFO] - \n",
      "================================================================================\n",
      "2025-10-08 02:20:39,822 [INFO] -                         --- FINAL EVALUATION SUMMARY ---                        \n",
      "2025-10-08 02:20:39,822 [INFO] - ================================================================================\n",
      "2025-10-08 02:20:39,828 [INFO] - Checkpoint           | eval_loss    | eval_rouge1  | eval_rouge2  | eval_rougeL  | eval_bleurt_f1 \n",
      "2025-10-08 02:20:39,829 [INFO] - --------------------------------------------------------------------------------------------------\n",
      "2025-10-08 02:20:39,829 [ERROR] - An unexpected error occurred: Unknown format code 'f' for object of type 'str'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_22992\\1921915600.py\", line 168, in main\n",
      "    row = f\"{result['checkpoint']:<20} | {result.get('eval_loss', 'N/A'):<12.4f} | {result.get('eval_eval_rouge1', 'N/A'):<12.4f} | {result.get('eval_eval_rouge2', 'N/A'):<12.4f} | {result.get('eval_eval_rougeL', 'N/A'):<12.4f} | {result.get(metric_to_check, 'N/A'):<15.4f}\"\n",
      "ValueError: Unknown format code 'f' for object of type 'str'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import unicodedata\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "OUTPUT_DIR = \"mbart-large-50-cnn-summarizer-v14\" \n",
    "METRIC_NAME = \"bleurt_f1\" \n",
    "DATA_PATH = \"../Dataset/new_large_CNN_dataset.csv\"\n",
    "MAX_SUMMARY_LENGTH_EVAL = 256\n",
    "# --- OPTIMIZATION: Increase batch size for faster evaluation ---\n",
    "EVAL_BATCH_SIZE = 16 \n",
    "# --- OPTIONAL: Set to a number (e.g., 500) for a quick evaluation on a subset, or None for the full dataset ---\n",
    "NUM_EVAL_SAMPLES = 500 \n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"evaluate_checkpoints_log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] - %(message)s\", handlers=[logging.FileHandler(log_filename), logging.StreamHandler()])\n",
    "\n",
    "def sanitize_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    return text.replace('\"\"', '\"').strip()\n",
    "\n",
    "def normalize_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    return ' '.join(unicodedata.normalize('NFKC', text).split())\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        logging.info(\"--- Starting Post-Training Evaluation of Checkpoints (Optimized) ---\")\n",
    "        \n",
    "        # 1. --- Load and Prepare the Test Dataset ---\n",
    "        logging.info(\"Loading and preparing test data...\")\n",
    "        \n",
    "        df_new = pd.read_csv(DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        df_new.dropna(subset=['raw_news_article', 'english_summary', 'hindi_summary'], inplace=True)\n",
    "        \n",
    "        for col in ['raw_news_article', 'english_summary', 'hindi_summary']:\n",
    "            df_new[col] = df_new[col].apply(sanitize_text).apply(normalize_text)\n",
    "        \n",
    "        raw_dataset = Dataset.from_pandas(df_new)\n",
    "\n",
    "        def format_dataset_mbart(batch):\n",
    "            inputs, targets, langs = [], [], []\n",
    "            for article, eng_summary, hin_summary in zip(batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']):\n",
    "                if isinstance(article, str) and article:\n",
    "                    inputs.append(article); targets.append(eng_summary); langs.append(\"en_XX\")\n",
    "                    inputs.append(article); targets.append(hin_summary); langs.append(\"hi_IN\")\n",
    "            return {'article': inputs, 'summary': targets, 'target_lang': langs}\n",
    "\n",
    "        processed_dataset = raw_dataset.map(format_dataset_mbart, batched=True, remove_columns=raw_dataset.column_names)\n",
    "        \n",
    "        train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        test_dataset_untokenized = train_test_split['test']\n",
    "        \n",
    "        if NUM_EVAL_SAMPLES:\n",
    "            logging.warning(f\"Using a subset of {NUM_EVAL_SAMPLES} examples for quick evaluation.\")\n",
    "            test_dataset_untokenized = test_dataset_untokenized.select(range(NUM_EVAL_SAMPLES))\n",
    "\n",
    "        logging.info(f\"Test data prepared with {len(test_dataset_untokenized)} examples.\")\n",
    "\n",
    "        # 2. --- Find all Checkpoints ---\n",
    "        checkpoint_dirs = sorted(\n",
    "            [d for d in os.listdir(OUTPUT_DIR) if d.startswith(\"checkpoint-\")],\n",
    "            key=lambda x: int(x.split('-')[-1])\n",
    "        )\n",
    "        if not checkpoint_dirs:\n",
    "            logging.error(f\"FATAL: No 'checkpoint-*' directories found in '{OUTPUT_DIR}'.\")\n",
    "            return\n",
    "        logging.info(f\"Found {len(checkpoint_dirs)} checkpoints to evaluate: {checkpoint_dirs}\")\n",
    "        \n",
    "        all_results = []\n",
    "        best_metric_value = None\n",
    "        best_checkpoint_path = None\n",
    "        metric_to_check = f\"eval_{METRIC_NAME}\"\n",
    "        is_loss = 'loss' in metric_to_check.lower()\n",
    "        \n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "        bleurt_metric = evaluate.load(\"bleurt\", \"bleurt-20\")\n",
    "\n",
    "        def compute_metrics_wrapper(eval_pred, tokenizer):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "            bleurt_result = bleurt_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "            result = {\"rouge1\": rouge_result[\"rouge1\"], \"rouge2\": rouge_result[\"rouge2\"], \"rougeL\": rouge_result[\"rougeL\"], \"bleurt_f1\": np.mean(bleurt_result[\"scores\"])}\n",
    "            return {f\"eval_{k}\": v for k, v in result.items()} # Return raw scores for averaging\n",
    "\n",
    "        # 3. --- Loop Through and Evaluate Each Checkpoint ---\n",
    "        for chkpt_dir in checkpoint_dirs:\n",
    "            chkpt_path = os.path.join(OUTPUT_DIR, chkpt_dir)\n",
    "            logging.info(f\"\\n--- Evaluating Checkpoint: {chkpt_path} ---\")\n",
    "            \n",
    "            model = MBartForConditionalGeneration.from_pretrained(chkpt_path)\n",
    "            tokenizer = MBart50TokenizerFast.from_pretrained(chkpt_path)\n",
    "            \n",
    "            def tokenize_for_eval(examples):\n",
    "                tokenizer.src_lang = \"en_XX\"\n",
    "                model_inputs = tokenizer(examples['article'], max_length=1024, truncation=True)\n",
    "                labels_batch = []\n",
    "                for i in range(len(examples['summary'])):\n",
    "                    tokenizer.tgt_lang = examples['target_lang'][i]\n",
    "                    labels = tokenizer(text_target=examples['summary'][i], max_length=MAX_SUMMARY_LENGTH_EVAL, truncation=True)\n",
    "                    labels_batch.append(labels['input_ids'])\n",
    "                model_inputs[\"labels\"] = labels_batch\n",
    "                return model_inputs\n",
    "\n",
    "            tokenized_test_dataset = test_dataset_untokenized.map(tokenize_for_eval, batched=True, remove_columns=['article', 'summary', 'target_lang'])\n",
    "\n",
    "            temp_training_args = Seq2SeqTrainingArguments(\n",
    "                output_dir=os.path.join(OUTPUT_DIR, \"temp_eval\"),\n",
    "                per_device_eval_batch_size=EVAL_BATCH_SIZE, # Using the optimized batch size\n",
    "                predict_with_generate=True,\n",
    "                fp16=torch.cuda.is_available()\n",
    "            )\n",
    "\n",
    "            trainer = Seq2SeqTrainer(\n",
    "                model=model, args=temp_training_args,\n",
    "                eval_dataset=tokenized_test_dataset, tokenizer=tokenizer,\n",
    "                data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "                compute_metrics=lambda p: compute_metrics_wrapper(p, tokenizer)\n",
    "            )\n",
    "            \n",
    "            logging.info(f\"Running evaluation on {chkpt_dir}...\")\n",
    "            eval_results = trainer.evaluate()\n",
    "            \n",
    "            # Round metrics for logging\n",
    "            rounded_results = {k: round(v, 4) for k, v in eval_results.items()}\n",
    "            logging.info(f\"--- Results for {chkpt_dir} ---\")\n",
    "            for key, value in rounded_results.items():\n",
    "                logging.info(f\"  - {key}: {value}\")\n",
    "            all_results.append({'checkpoint': chkpt_dir, **rounded_results})\n",
    "            \n",
    "            metric_value = eval_results.get(metric_to_check)\n",
    "            if metric_value is not None:\n",
    "                if best_metric_value is None or (not is_loss and metric_value > best_metric_value) or (is_loss and metric_value < best_metric_value):\n",
    "                    best_metric_value, best_checkpoint_path = metric_value, chkpt_path\n",
    "                    logging.info(f\"*** New best checkpoint found: {chkpt_dir} with {metric_to_check}: {metric_value:.4f} ***\")\n",
    "\n",
    "        # 4. --- Print Final Summary Table and Save the Best ---\n",
    "        if not all_results:\n",
    "            logging.error(\"No checkpoints were successfully evaluated.\")\n",
    "            return\n",
    "\n",
    "        logging.info(\"\\n\" + \"=\"*80)\n",
    "        logging.info(\"--- FINAL EVALUATION SUMMARY ---\".center(80))\n",
    "        logging.info(\"=\"*80)\n",
    "        header = f\"{'Checkpoint':<20} | {'eval_loss':<12} | {'eval_rouge1':<12} | {'eval_rouge2':<12} | {'eval_rougeL':<12} | {'eval_bleurt_f1':<15}\"\n",
    "        logging.info(header)\n",
    "        logging.info(\"-\" * len(header))\n",
    "        for result in all_results:\n",
    "            row = f\"{result['checkpoint']:<20} | {result.get('eval_loss', 'N/A'):<12.4f} | {result.get('eval_eval_rouge1', 'N/A'):<12.4f} | {result.get('eval_eval_rouge2', 'N/A'):<12.4f} | {result.get('eval_eval_rougeL', 'N/A'):<12.4f} | {result.get(metric_to_check, 'N/A'):<15.4f}\"\n",
    "            logging.info(row)\n",
    "        logging.info(\"=\"*80)\n",
    "\n",
    "        if not best_checkpoint_path:\n",
    "            logging.error(\"Could not determine the best checkpoint after evaluation.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"\\n--- Best Model Identified ---\")\n",
    "        logging.info(f\"Checkpoint: {best_checkpoint_path}\")\n",
    "        logging.info(f\"Metric ({metric_to_check}): {best_metric_value:.4f}\")\n",
    "\n",
    "        final_model_path = os.path.join(OUTPUT_DIR, \"final_model\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            shutil.rmtree(final_model_path)\n",
    "            \n",
    "        shutil.copytree(best_checkpoint_path, final_model_path)\n",
    "        logging.info(f\"Successfully copied best model to: {final_model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7ee0c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 02:25:57,558 [INFO] - --- Starting Manual Model Save ---\n",
      "2025-10-08 02:25:57,558 [INFO] - Source Checkpoint: mbart-large-50-cnn-summarizer-v14\\checkpoint-4152\n",
      "2025-10-08 02:25:57,558 [INFO] - Destination: mbart-large-50-cnn-summarizer-v14\\final_model\n",
      "2025-10-08 02:25:57,558 [WARNING] - Removing existing 'final_model' directory: mbart-large-50-cnn-summarizer-v14\\final_model\n",
      "2025-10-08 02:27:22,891 [INFO] - \n",
      "SUCCESS: Successfully copied 'checkpoint-4152' to 'mbart-large-50-cnn-summarizer-v14\\final_model'.\n",
      "2025-10-08 02:27:22,891 [INFO] - Your final model is now ready to use.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "OUTPUT_DIR = \"mbart-large-50-cnn-summarizer-v14\" \n",
    "# The specific checkpoint folder you identified as the best.\n",
    "BEST_CHECKPOINT_FOLDER = \"checkpoint-4152\"\n",
    "# ----------------------------------------------------------------\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"manual_save_log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] - %(message)s\", handlers=[logging.FileHandler(log_filename), logging.StreamHandler()])\n",
    "\n",
    "def save_specific_checkpoint():\n",
    "    \"\"\"\n",
    "    Manually copies a specified checkpoint directory to a 'final_model' directory.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        source_path = os.path.join(OUTPUT_DIR, BEST_CHECKPOINT_FOLDER)\n",
    "        destination_path = os.path.join(OUTPUT_DIR, \"final_model\")\n",
    "\n",
    "        logging.info(f\"--- Starting Manual Model Save ---\")\n",
    "        logging.info(f\"Source Checkpoint: {source_path}\")\n",
    "        logging.info(f\"Destination: {destination_path}\")\n",
    "\n",
    "        # Check if the source checkpoint directory exists\n",
    "        if not os.path.isdir(source_path):\n",
    "            logging.error(f\"FATAL: The source checkpoint directory '{source_path}' does not exist.\")\n",
    "            logging.error(\"Please ensure the OUTPUT_DIR and BEST_CHECKPOINT_FOLDER variables are set correctly.\")\n",
    "            return\n",
    "\n",
    "        # If a 'final_model' directory already exists, remove it for a clean copy\n",
    "        if os.path.exists(destination_path):\n",
    "            logging.warning(f\"Removing existing 'final_model' directory: {destination_path}\")\n",
    "            shutil.rmtree(destination_path)\n",
    "\n",
    "        # Copy the entire checkpoint directory to the 'final_model' destination\n",
    "        shutil.copytree(source_path, destination_path)\n",
    "\n",
    "        logging.info(f\"\\nSUCCESS: Successfully copied '{BEST_CHECKPOINT_FOLDER}' to '{destination_path}'.\")\n",
    "        logging.info(\"Your final model is now ready to use.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred during the copy process: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    save_specific_checkpoint()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5603de93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:05:53,936 [INFO] - --- Starting Post-Training Evaluation of Checkpoints (GPU-Accelerated) ---\n",
      "2025-10-08 01:05:53,951 [INFO] - Loading and preparing test data...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0dc3429bd7c94714b20efe968ca2ea50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:05:57,075 [WARNING] - Using a subset of 1000 examples for quick evaluation.\n",
      "2025-10-08 01:05:57,075 [INFO] - Test data prepared with 1000 examples.\n",
      "2025-10-08 01:05:57,085 [INFO] - Found 4 checkpoints to evaluate: ['checkpoint-2076', 'checkpoint-4152', 'checkpoint-6228', 'checkpoint-8304']\n",
      "2025-10-08 01:06:02,425 [INFO] - Initializing GPU-based BLEURT scorer (will auto-download checkpoint if needed)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\bleurt\\score.py:160: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:06:02,595 [WARNING] - From c:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\bleurt\\score.py:160: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint bleurt-20.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-08 01:06:02,597 [INFO] - Reading checkpoint bleurt-20.\n",
      "2025-10-08 01:06:02,599 [ERROR] - An unexpected error occurred: Could not find BLEURT checkpoint bleurt-20\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_22992\\636203867.py\", line 103, in main\n",
      "    bleurt_scorer_gpu = bleurt_scorer.BleurtScorer(\"bleurt-20\")\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\bleurt\\score.py\", line 161, in __init__\n",
      "    self.config = checkpoint_lib.read_bleurt_config(checkpoint)\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\bleurt\\checkpoint.py\", line 84, in read_bleurt_config\n",
      "    assert tf.io.gfile.exists(path), \\\n",
      "AssertionError: Could not find BLEURT checkpoint bleurt-20\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import unicodedata\n",
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "# Import the bleurt library directly for GPU-acceleration ---\n",
    "try:\n",
    "    from bleurt import score as bleurt_scorer\n",
    "    BLEURT_INSTALLED = True\n",
    "except ImportError:\n",
    "    BLEURT_INSTALLED = False\n",
    "\n",
    "# --- Configuration ---\n",
    "OUTPUT_DIR = \"mbart-large-50-cnn-summarizer-v14\" \n",
    "METRIC_NAME = \"rougeL\" \n",
    "DATA_PATH = \"../Dataset/new_large_CNN_dataset.csv\"\n",
    "MAX_SUMMARY_LENGTH_EVAL = 256\n",
    "EVAL_BATCH_SIZE = 16 \n",
    "NUM_EVAL_SAMPLES = 1000 \n",
    "\n",
    "#Control whether to run BLEURT on the GPU ---\n",
    "USE_GPU_FOR_BLEURT = True\n",
    "\n",
    "# --- Setup Logging ---\n",
    "log_filename = f\"evaluate_checkpoints_log_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] - %(message)s\", handlers=[logging.FileHandler(log_filename), logging.StreamHandler()])\n",
    "\n",
    "def sanitize_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    return text.replace('\"\"', '\"').strip()\n",
    "\n",
    "def normalize_text(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    return ' '.join(unicodedata.normalize('NFKC', text).split())\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        logging.info(\"--- Starting Post-Training Evaluation of Checkpoints (GPU-Accelerated) ---\")\n",
    "        \n",
    "        # 1. --- Load and Prepare the Test Dataset ---\n",
    "        logging.info(\"Loading and preparing test data...\")\n",
    "        \n",
    "        df_new = pd.read_csv(DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        df_new.dropna(subset=['raw_news_article', 'english_summary', 'hindi_summary'], inplace=True)\n",
    "        \n",
    "        for col in ['raw_news_article', 'english_summary', 'hindi_summary']:\n",
    "            df_new[col] = df_new[col].apply(sanitize_text).apply(normalize_text)\n",
    "        \n",
    "        raw_dataset = Dataset.from_pandas(df_new)\n",
    "\n",
    "        def format_dataset_mbart(batch):\n",
    "            inputs, targets, langs = [], [], []\n",
    "            for article, eng_summary, hin_summary in zip(batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']):\n",
    "                if isinstance(article, str) and article:\n",
    "                    inputs.append(article); targets.append(eng_summary); langs.append(\"en_XX\")\n",
    "                    inputs.append(article); targets.append(hin_summary); langs.append(\"hi_IN\")\n",
    "            return {'article': inputs, 'summary': targets, 'target_lang': langs}\n",
    "\n",
    "        processed_dataset = raw_dataset.map(format_dataset_mbart, batched=True, remove_columns=raw_dataset.column_names)\n",
    "        \n",
    "        train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        test_dataset_untokenized = train_test_split['test']\n",
    "        \n",
    "        if NUM_EVAL_SAMPLES:\n",
    "            logging.warning(f\"Using a subset of {NUM_EVAL_SAMPLES} examples for quick evaluation.\")\n",
    "            test_dataset_untokenized = test_dataset_untokenized.select(range(NUM_EVAL_SAMPLES))\n",
    "\n",
    "        logging.info(f\"Test data prepared with {len(test_dataset_untokenized)} examples.\")\n",
    "\n",
    "        # 2. --- Find all Checkpoints ---\n",
    "        checkpoint_dirs = sorted([d for d in os.listdir(OUTPUT_DIR) if d.startswith(\"checkpoint-\")], key=lambda x: int(x.split('-')[-1]))\n",
    "        if not checkpoint_dirs:\n",
    "            logging.error(f\"FATAL: No 'checkpoint-*' directories found in '{OUTPUT_DIR}'.\")\n",
    "            return\n",
    "        logging.info(f\"Found {len(checkpoint_dirs)} checkpoints to evaluate: {checkpoint_dirs}\")\n",
    "        \n",
    "        all_results = []\n",
    "        best_metric_value = None\n",
    "        best_checkpoint_path = None\n",
    "        metric_to_check = f\"eval_{METRIC_NAME}\"\n",
    "        is_loss = 'loss' in metric_to_check.lower()\n",
    "        \n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "        \n",
    "        bleurt_scorer_gpu = None\n",
    "        bleurt_metric_cpu = None\n",
    "        if USE_GPU_FOR_BLEURT:\n",
    "            if BLEURT_INSTALLED:\n",
    "                logging.info(\"Initializing GPU-based BLEURT scorer (will auto-download checkpoint if needed)...\")\n",
    "                # This will automatically download and cache the model the first time it's run\n",
    "                bleurt_scorer_gpu = bleurt_scorer.BleurtScorer(\"bleurt-20\")\n",
    "                logging.info(\"GPU-based BLEURT scorer initialized.\")\n",
    "            else:\n",
    "                logging.error(\"The 'bleurt' library is not installed. Please run 'pip install git+https://github.com/google-research/bleurt.git'.\")\n",
    "                logging.warning(\"Falling back to CPU-based BLEURT calculation.\")\n",
    "                bleurt_metric_cpu = evaluate.load(\"bleurt\", \"bleurt-20\")\n",
    "        else:\n",
    "            bleurt_metric_cpu = evaluate.load(\"bleurt\", \"bleurt-20\")\n",
    "\n",
    "\n",
    "        def compute_metrics_wrapper(eval_pred, tokenizer):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            \n",
    "            logging.info(\"Calculating ROUGE scores...\")\n",
    "            rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "            result = {\"rouge1\": rouge_result[\"rouge1\"], \"rouge2\": rouge_result[\"rouge2\"], \"rougeL\": rouge_result[\"rougeL\"]}\n",
    "            \n",
    "            if bleurt_scorer_gpu:\n",
    "                logging.info(\"Calculating BLEURT scores on GPU...\")\n",
    "                bleurt_scores = bleurt_scorer_gpu.score(references=decoded_labels, candidates=decoded_preds, batch_size=EVAL_BATCH_SIZE)\n",
    "                result[\"bleurt_f1\"] = np.mean(bleurt_scores)\n",
    "            elif bleurt_metric_cpu:\n",
    "                logging.info(\"Calculating BLEURT scores on CPU (this will be very slow)...\")\n",
    "                bleurt_result = bleurt_metric_cpu.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "                result[\"bleurt_f1\"] = np.mean(bleurt_result[\"scores\"])\n",
    "            \n",
    "            return {f\"eval_{k}\": v for k, v in result.items()}\n",
    "\n",
    "        # 3. --- Loop Through and Evaluate Each Checkpoint ---\n",
    "        for chkpt_dir in checkpoint_dirs:\n",
    "            chkpt_path = os.path.join(OUTPUT_DIR, chkpt_dir)\n",
    "            logging.info(f\"\\n--- Evaluating Checkpoint: {chkpt_path} ---\")\n",
    "            \n",
    "            model = MBartForConditionalGeneration.from_pretrained(chkpt_path)\n",
    "            tokenizer = MBart50TokenizerFast.from_pretrained(chkpt_path)\n",
    "            \n",
    "            def tokenize_for_eval(examples):\n",
    "                tokenizer.src_lang = \"en_XX\"\n",
    "                model_inputs = tokenizer(examples['article'], max_length=1024, truncation=True)\n",
    "                labels_batch = []\n",
    "                for i in range(len(examples['summary'])):\n",
    "                    tokenizer.tgt_lang = examples['target_lang'][i]\n",
    "                    labels = tokenizer(text_target=examples['summary'][i], max_length=MAX_SUMMARY_LENGTH_EVAL, truncation=True)\n",
    "                    labels_batch.append(labels['input_ids'])\n",
    "                model_inputs[\"labels\"] = labels_batch\n",
    "                return model_inputs\n",
    "\n",
    "            tokenized_test_dataset = test_dataset_untokenized.map(tokenize_for_eval, batched=True, remove_columns=['article', 'summary', 'target_lang'])\n",
    "\n",
    "            temp_training_args = Seq2SeqTrainingArguments(\n",
    "                output_dir=os.path.join(OUTPUT_DIR, \"temp_eval\"),\n",
    "                per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "                predict_with_generate=True,\n",
    "                fp16=torch.cuda.is_available()\n",
    "            )\n",
    "\n",
    "            trainer = Seq2SeqTrainer(\n",
    "                model=model, args=temp_training_args,\n",
    "                eval_dataset=tokenized_test_dataset, tokenizer=tokenizer,\n",
    "                data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    "                compute_metrics=lambda p: compute_metrics_wrapper(p, tokenizer)\n",
    "            )\n",
    "            \n",
    "            logging.info(f\"Running evaluation on {chkpt_dir}...\")\n",
    "            eval_results = trainer.evaluate()\n",
    "            \n",
    "            rounded_results = {k: round(v, 4) for k, v in eval_results.items()}\n",
    "            logging.info(f\"--- Results for {chkpt_dir} ---\")\n",
    "            for key, value in rounded_results.items():\n",
    "                logging.info(f\"  - {key}: {value}\")\n",
    "            all_results.append({'checkpoint': chkpt_dir, **rounded_results})\n",
    "            \n",
    "            metric_value = eval_results.get(metric_to_check)\n",
    "            if metric_value is not None:\n",
    "                if best_metric_value is None or (not is_loss and metric_value > best_metric_value) or (is_loss and metric_value < best_metric_value):\n",
    "                    best_metric_value, best_checkpoint_path = metric_value, chkpt_path\n",
    "                    logging.info(f\"*** New best checkpoint found: {chkpt_dir} with {metric_to_check}: {metric_value:.4f} ***\")\n",
    "        \n",
    "        # 4. --- Print Final Summary Table and Save the Best ---\n",
    "        if not all_results:\n",
    "            logging.error(\"No checkpoints were successfully evaluated.\")\n",
    "            return\n",
    "\n",
    "        logging.info(\"\\n\" + \"=\"*80)\n",
    "        logging.info(\"--- FINAL EVALUATION SUMMARY ---\".center(80))\n",
    "        logging.info(\"=\"*80)\n",
    "        header_cols = ['Checkpoint', 'eval_loss', 'eval_rouge1', 'eval_rouge2', 'eval_rougeL']\n",
    "        if USE_GPU_FOR_BLEURT or bleurt_metric_cpu: header_cols.append('eval_bleurt_f1')\n",
    "        header = \" | \".join([f\"{col:<15}\" for col in header_cols])\n",
    "        logging.info(header)\n",
    "        logging.info(\"-\" * len(header))\n",
    "        for result in all_results:\n",
    "            row_vals = [\n",
    "                result.get('checkpoint', 'N/A'),\n",
    "                f\"{result.get('eval_loss', 0):.4f}\",\n",
    "                f\"{result.get('eval_eval_rouge1', 0):.4f}\",\n",
    "                f\"{result.get('eval_eval_rouge2', 0):.4f}\",\n",
    "                f\"{result.get('eval_eval_rougeL', 0):.4f}\",\n",
    "            ]\n",
    "            if USE_GPU_FOR_BLEURT or bleurt_metric_cpu: row_vals.append(f\"{result.get('eval_eval_bleurt_f1', 0):.4f}\")\n",
    "            row = \" | \".join([f\"{val:<15}\" for val in row_vals])\n",
    "            logging.info(row)\n",
    "        logging.info(\"=\"*80)\n",
    "\n",
    "        if not best_checkpoint_path:\n",
    "            logging.error(\"Could not determine the best checkpoint after evaluation.\")\n",
    "            return\n",
    "\n",
    "        logging.info(f\"\\n--- Best Model Identified ---\")\n",
    "        logging.info(f\"Checkpoint: {best_checkpoint_path}\")\n",
    "        logging.info(f\"Metric ({metric_to_check}): {best_metric_value:.4f}\")\n",
    "\n",
    "        final_model_path = os.path.join(OUTPUT_DIR, \"final_model\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            shutil.rmtree(final_model_path)\n",
    "            \n",
    "        shutil.copytree(best_checkpoint_path, final_model_path)\n",
    "        logging.info(f\"Successfully copied best model to: {final_model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarizer_env3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
