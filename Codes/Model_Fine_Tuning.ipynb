{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2d9d1e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 02:05:09,912 [INFO] - Loading existing model from: mt5-base-cnn-summarizer-en-hi_v3/final_model\n",
      "c:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "2025-09-23 02:05:12,814 [INFO] - Loading new data from: ../Dataset/filtered_articles_CNN.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b0aeacb016428db8ef9c8e98bab941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/686 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 02:05:12,940 [INFO] - New data prepared. Samples: 1234 train, 138 test.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eb26634f63f423fbe5da24561a51c1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1234 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:4007: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276e62269d5b4cd18644a7a1e609b2c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/138 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 02:05:14,049 [INFO] - Tokenization complete.\n",
      "2025-09-23 02:05:14,050 [INFO] - Initializing Seq2SeqTrainer with fully compatible settings...\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_33624\\2965819654.py:167: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "2025-09-23 02:05:23,026 [INFO] - Starting continued fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='775' max='775' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [775/775 14:33, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 02:19:58,275 [INFO] - Fine-tuning finished successfully.\n",
      "2025-09-23 02:19:58,275 [ERROR] - Could not find or save the best model due to: [Errno 2] No such file or directory: 'mt5-base-cnn-summarizer-en-hi_v5\\\\trainer_state.json'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_33624\\2965819654.py\", line 49, in find_and_save_best_model\n",
      "    with open(state_path, \"r\") as f:\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 324, in _modified_open\n",
      "    return io_open(file, *args, **kwargs)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'mt5-base-cnn-summarizer-en-hi_v5\\\\trainer_state.json'\n"
     ]
    }
   ],
   "source": [
    "# run_finetuning_v5\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = \"mt5-base-cnn-summarizer-en-hi_v3/final_model\"\n",
    "NEW_MODEL_OUTPUT_DIR = \"mt5-base-cnn-summarizer-en-hi_v5\"\n",
    "NEW_DATA_PATH = \"../Dataset/filtered_articles_CNN.csv\"\n",
    "\n",
    "# Training hyperparameters\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# Generation parameters for evaluation\n",
    "NUM_BEAMS_EVAL = 6\n",
    "MAX_SUMMARY_LENGTH_EVAL = 256\n",
    "\n",
    "# Configure logging\n",
    "log_filename = f\"finetuning_log_v5_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "    handlers=[logging.FileHandler(log_filename), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "def find_and_save_best_model(output_dir):\n",
    "    \"\"\"Finds the best checkpoint based on trainer state and saves it.\"\"\"\n",
    "    try:\n",
    "        state_path = os.path.join(output_dir, \"trainer_state.json\")\n",
    "        with open(state_path, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "        \n",
    "        best_checkpoint_path = state.get(\"best_model_checkpoint\")\n",
    "        if not best_checkpoint_path:\n",
    "            logging.error(\"Could not find 'best_model_checkpoint' in trainer_state.json.\")\n",
    "            return\n",
    "\n",
    "        best_metric = state.get(\"best_metric\", \"N/A\")\n",
    "        logging.info(f\"Best checkpoint found: {best_checkpoint_path} with metric {best_metric}\")\n",
    "        \n",
    "        final_model_path = os.path.join(output_dir, \"final_model\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            shutil.rmtree(final_model_path)\n",
    "            \n",
    "        shutil.copytree(best_checkpoint_path, final_model_path)\n",
    "        logging.info(f\"Best model copied to {final_model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not find or save the best model due to: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        logging.info(f\"Loading existing model from: {BASE_MODEL_PATH}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_PATH)\n",
    "\n",
    "        logging.info(f\"Loading new data from: {NEW_DATA_PATH}\")\n",
    "        df_new = pd.read_csv(NEW_DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        df_new.dropna(subset=['raw_news_article', 'english_summary', 'hindi_summary'], inplace=True)\n",
    "        df_new.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        raw_dataset = Dataset.from_pandas(df_new)\n",
    "\n",
    "        PREFIX_ENG = \"summarize English: \"\n",
    "        PREFIX_HIN = \"summarize Hindi: \"\n",
    "\n",
    "        def format_dataset(batch):\n",
    "            inputs, targets = [], []\n",
    "            for article, eng_summary, hin_summary in zip(\n",
    "                batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']\n",
    "            ):\n",
    "                if isinstance(article, str):\n",
    "                    inputs.append(PREFIX_ENG + article)\n",
    "                    targets.append(eng_summary)\n",
    "                    inputs.append(PREFIX_HIN + article)\n",
    "                    targets.append(hin_summary)\n",
    "            return {'inputs': inputs, 'targets': targets}\n",
    "\n",
    "        processed_dataset = raw_dataset.map(\n",
    "            format_dataset, batched=True, remove_columns=raw_dataset.column_names\n",
    "        ).flatten()\n",
    "\n",
    "        train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        final_datasets = DatasetDict({\n",
    "            'train': train_test_split['train'],\n",
    "            'test': train_test_split['test']\n",
    "        })\n",
    "        logging.info(f\"New data prepared. Samples: {len(final_datasets['train'])} train, {len(final_datasets['test'])} test.\")\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            model_inputs = tokenizer(examples['inputs'], max_length=1024, truncation=True)\n",
    "            with tokenizer.as_target_tokenizer():\n",
    "                labels = tokenizer(examples['targets'], max_length=MAX_SUMMARY_LENGTH_EVAL, truncation=True)\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "\n",
    "        tokenized_datasets = final_datasets.map(tokenize_function, batched=True, remove_columns=['inputs', 'targets'])\n",
    "        logging.info(\"Tokenization complete.\")\n",
    "        \n",
    "        train_size = len(tokenized_datasets[\"train\"])\n",
    "        effective_batch_size = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n",
    "        steps_per_epoch = train_size // effective_batch_size\n",
    "        if steps_per_epoch == 0: steps_per_epoch = 1\n",
    "\n",
    "        logging.info(\"Initializing Seq2SeqTrainer with fully compatible settings...\")\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=NEW_MODEL_OUTPUT_DIR,\n",
    "            num_train_epochs=NUM_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            logging_dir=f\"{NEW_MODEL_OUTPUT_DIR}/logs\",\n",
    "            logging_steps=50,\n",
    "            \n",
    "            # --- FINAL COMPATIBILITY FIX ---\n",
    "            # Removing all modern arguments like 'evaluation_strategy' and 'save_strategy'\n",
    "            # and using only the older arguments that your library version understands.\n",
    "            do_eval=True,\n",
    "            eval_steps=steps_per_epoch,\n",
    "            save_steps=steps_per_epoch,\n",
    "            \n",
    "            save_total_limit=NUM_EPOCHS,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            load_best_model_at_end=False, # Disable the problematic feature\n",
    "            \n",
    "            # These are still valid in older versions\n",
    "            metric_for_best_model=\"rouge2\",\n",
    "            generation_max_length=MAX_SUMMARY_LENGTH_EVAL,\n",
    "            generation_num_beams=NUM_BEAMS_EVAL,\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "            return {k: round(v * 100, 4) for k, v in result.items()}\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        logging.info(\"Starting continued fine-tuning...\")\n",
    "        trainer.train()\n",
    "        logging.info(\"Fine-tuning finished successfully.\")\n",
    "\n",
    "        # Manually find and save the best model from all checkpoints\n",
    "        find_and_save_best_model(NEW_MODEL_OUTPUT_DIR)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e35f09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 02:42:56,163 [INFO] - Loading existing model from: mt5-base-cnn-summarizer-en-hi_v3/final_model\n",
      "c:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n",
      "2025-09-23 02:42:58,810 [INFO] - Loading new data from: ../Dataset/filtered_articles_CNN.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81f3c7957c9c402ca9c71eee8fc1957d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/686 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 02:42:58,936 [INFO] - New data prepared. Samples: 1234 train, 138 test.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20a7b61e76eb48e1976ebc0760f66605",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1234 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\tokenization_utils_base.py:4007: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "607f4894c7e04b88b983b259247e9aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/138 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 02:43:00,416 [INFO] - Tokenization complete.\n",
      "2025-09-23 02:43:00,416 [INFO] - Initializing Seq2SeqTrainer with manual best model selection strategy...\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_33624\\4155698402.py:167: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "2025-09-23 02:43:06,395 [INFO] - Starting continued fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='775' max='775' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [775/775 39:45, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-23 03:22:54,662 [INFO] - Fine-tuning finished successfully.\n",
      "2025-09-23 03:22:54,662 [ERROR] - Could not find or save the best model due to: [Errno 2] No such file or directory: 'mt5-base-cnn-summarizer-en-hi_v5\\\\trainer_state.json'\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_33624\\4155698402.py\", line 49, in find_and_save_best_model\n",
      "    with open(state_path, \"r\") as f:\n",
      "  File \"c:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 324, in _modified_open\n",
      "    return io_open(file, *args, **kwargs)\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'mt5-base-cnn-summarizer-en-hi_v5\\\\trainer_state.json'\n"
     ]
    }
   ],
   "source": [
    "# run_finetuning_v5_final_compatible.py\n",
    "\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = \"mt5-base-cnn-summarizer-en-hi_v3/final_model\"\n",
    "NEW_MODEL_OUTPUT_DIR = \"mt5-base-cnn-summarizer-en-hi_v5\"\n",
    "NEW_DATA_PATH = \"../Dataset/filtered_articles_CNN.csv\"\n",
    "\n",
    "# Training hyperparameters\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# Generation parameters for evaluation\n",
    "NUM_BEAMS_EVAL = 6\n",
    "MAX_SUMMARY_LENGTH_EVAL = 256\n",
    "\n",
    "# Configure logging\n",
    "log_filename = f\"finetuning_log_v5_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "    handlers=[logging.FileHandler(log_filename), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "def find_and_save_best_model(output_dir):\n",
    "    \"\"\"Finds the best checkpoint based on trainer state and saves it.\"\"\"\n",
    "    try:\n",
    "        state_path = os.path.join(output_dir, \"trainer_state.json\")\n",
    "        with open(state_path, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "        \n",
    "        best_checkpoint_path = state.get(\"best_model_checkpoint\")\n",
    "        if not best_checkpoint_path:\n",
    "            logging.error(\"Could not find 'best_model_checkpoint' in trainer_state.json.\")\n",
    "            return\n",
    "\n",
    "        best_metric = state.get(\"best_metric\", \"N/A\")\n",
    "        logging.info(f\"Best checkpoint found: {best_checkpoint_path} with metric {best_metric}\")\n",
    "        \n",
    "        final_model_path = os.path.join(output_dir, \"final_model\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            shutil.rmtree(final_model_path)\n",
    "            \n",
    "        shutil.copytree(best_checkpoint_path, final_model_path)\n",
    "        logging.info(f\"Best model copied to {final_model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not find or save the best model due to: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        logging.info(f\"Loading existing model from: {BASE_MODEL_PATH}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_PATH)\n",
    "\n",
    "        logging.info(f\"Loading new data from: {NEW_DATA_PATH}\")\n",
    "        df_new = pd.read_csv(NEW_DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        df_new.dropna(subset=['raw_news_article', 'english_summary', 'hindi_summary'], inplace=True)\n",
    "        df_new.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        raw_dataset = Dataset.from_pandas(df_new)\n",
    "\n",
    "        PREFIX_ENG = \"summarize English: \"\n",
    "        PREFIX_HIN = \"summarize Hindi: \"\n",
    "\n",
    "        def format_dataset(batch):\n",
    "            inputs, targets = [], []\n",
    "            for article, eng_summary, hin_summary in zip(\n",
    "                batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']\n",
    "            ):\n",
    "                if isinstance(article, str):\n",
    "                    inputs.append(PREFIX_ENG + article)\n",
    "                    targets.append(eng_summary)\n",
    "                    inputs.append(PREFIX_HIN + article)\n",
    "                    targets.append(hin_summary)\n",
    "            return {'inputs': inputs, 'targets': targets}\n",
    "\n",
    "        processed_dataset = raw_dataset.map(\n",
    "            format_dataset, batched=True, remove_columns=raw_dataset.column_names\n",
    "        ).flatten()\n",
    "\n",
    "        train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        final_datasets = DatasetDict({\n",
    "            'train': train_test_split['train'],\n",
    "            'test': train_test_split['test']\n",
    "        })\n",
    "        logging.info(f\"New data prepared. Samples: {len(final_datasets['train'])} train, {len(final_datasets['test'])} test.\")\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            model_inputs = tokenizer(examples['inputs'], max_length=1024, truncation=True)\n",
    "            with tokenizer.as_target_tokenizer():\n",
    "                labels = tokenizer(examples['targets'], max_length=MAX_SUMMARY_LENGTH_EVAL, truncation=True)\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "\n",
    "        tokenized_datasets = final_datasets.map(tokenize_function, batched=True, remove_columns=['inputs', 'targets'])\n",
    "        logging.info(\"Tokenization complete.\")\n",
    "        \n",
    "        train_size = len(tokenized_datasets[\"train\"])\n",
    "        effective_batch_size = BATCH_SIZE * GRADIENT_ACCUMULATION_STEPS\n",
    "        steps_per_epoch = train_size // effective_batch_size\n",
    "        if steps_per_epoch == 0: steps_per_epoch = 1\n",
    "\n",
    "        logging.info(\"Initializing Seq2SeqTrainer with manual best model selection strategy...\")\n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=NEW_MODEL_OUTPUT_DIR,\n",
    "            num_train_epochs=NUM_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            logging_dir=f\"{NEW_MODEL_OUTPUT_DIR}/logs\",\n",
    "            logging_steps=50,\n",
    "            \n",
    "            # --- COMPATIBILITY FIX ---\n",
    "            # Using older, explicit arguments for evaluation and saving\n",
    "            do_eval=True,\n",
    "            eval_steps=steps_per_epoch,\n",
    "            save_steps=steps_per_epoch,\n",
    "            \n",
    "            save_total_limit=NUM_EPOCHS,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            \n",
    "            # --- MANUAL BEST MODEL FIX ---\n",
    "            load_best_model_at_end=False, # Disable the problematic feature\n",
    "            \n",
    "            metric_for_best_model=\"rouge2\",\n",
    "            generation_max_length=MAX_SUMMARY_LENGTH_EVAL,\n",
    "            generation_num_beams=NUM_BEAMS_EVAL,\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "            return {k: round(v * 100, 4) for k, v in result.items()}\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        logging.info(\"Starting continued fine-tuning...\")\n",
    "        trainer.train()\n",
    "        logging.info(\"Fine-tuning finished successfully.\")\n",
    "\n",
    "        # Manually find and save the best model from all checkpoints\n",
    "        find_and_save_best_model(NEW_MODEL_OUTPUT_DIR)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa5bf865",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "import shutil\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# --- Configuration ---\n",
    "BASE_MODEL_PATH = \"mt5-base-cnn-summarizer-en-hi_v3/final_model\"\n",
    "NEW_MODEL_OUTPUT_DIR = \"mt5-base-cnn-summarizer-en-hi_v6\"\n",
    "NEW_DATA_PATH = \"../Dataset/filtered_articles_CNN.csv\"\n",
    "\n",
    "# Training hyperparameters\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 2\n",
    "GRADIENT_ACCUMULATION_STEPS = 4\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# Generation parameters for evaluation\n",
    "NUM_BEAMS_EVAL = 6\n",
    "MAX_SUMMARY_LENGTH_EVAL = 256\n",
    "\n",
    "# Configure logging\n",
    "log_filename = f\"finetuning_log_v5_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "    handlers=[logging.FileHandler(log_filename), logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "def find_and_save_best_model(output_dir):\n",
    "    \"\"\"Finds the best checkpoint based on trainer state and saves it.\"\"\"\n",
    "    try:\n",
    "        state_path = os.path.join(output_dir, \"trainer_state.json\")\n",
    "        with open(state_path, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "        \n",
    "        best_checkpoint_path = state.get(\"best_model_checkpoint\")\n",
    "        if not best_checkpoint_path:\n",
    "            logging.error(\"Could not find 'best_model_checkpoint' in trainer_state.json.\")\n",
    "            return\n",
    "\n",
    "        best_metric = state.get(\"best_metric\", \"N/A\")\n",
    "        logging.info(f\"Best checkpoint found: {best_checkpoint_path} with metric {best_metric}\")\n",
    "        \n",
    "        final_model_path = os.path.join(output_dir, \"final_model\")\n",
    "        if os.path.exists(final_model_path):\n",
    "            shutil.rmtree(final_model_path)\n",
    "            \n",
    "        shutil.copytree(best_checkpoint_path, final_model_path)\n",
    "        logging.info(f\"Best model copied to {final_model_path}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Could not find or save the best model due to: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "def main():\n",
    "    try:\n",
    "        logging.info(f\"Loading existing model from: {BASE_MODEL_PATH}\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH)\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(BASE_MODEL_PATH)\n",
    "\n",
    "        logging.info(f\"Loading new data from: {NEW_DATA_PATH}\")\n",
    "        df_new = pd.read_csv(NEW_DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "        df_new.dropna(subset=['raw_news_article', 'english_summary', 'hindi_summary'], inplace=True)\n",
    "        df_new.reset_index(drop=True, inplace=True)\n",
    "\n",
    "        raw_dataset = Dataset.from_pandas(df_new)\n",
    "\n",
    "        PREFIX_ENG = \"summarize English: \"\n",
    "        PREFIX_HIN = \"summarize Hindi: \"\n",
    "\n",
    "        def format_dataset(batch):\n",
    "            inputs, targets = [], []\n",
    "            for article, eng_summary, hin_summary in zip(\n",
    "                batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']\n",
    "            ):\n",
    "                if isinstance(article, str):\n",
    "                    inputs.append(PREFIX_ENG + article)\n",
    "                    targets.append(eng_summary)\n",
    "                    inputs.append(PREFIX_HIN + article)\n",
    "                    targets.append(hin_summary)\n",
    "            return {'inputs': inputs, 'targets': targets}\n",
    "\n",
    "        processed_dataset = raw_dataset.map(\n",
    "            format_dataset, batched=True, remove_columns=raw_dataset.column_names\n",
    "        ).flatten()\n",
    "\n",
    "        train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "        final_datasets = DatasetDict({\n",
    "            'train': train_test_split['train'],\n",
    "            'test': train_test_split['test']\n",
    "        })\n",
    "        logging.info(f\"New data prepared. Samples: {len(final_datasets['train'])} train, {len(final_datasets['test'])} test.\")\n",
    "\n",
    "        def tokenize_function(examples):\n",
    "            model_inputs = tokenizer(examples['inputs'], max_length=1024, truncation=True)\n",
    "            labels = tokenizer(text_target=examples['targets'], max_length=MAX_SUMMARY_LENGTH_EVAL, truncation=True)\n",
    "            model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "            return model_inputs\n",
    "\n",
    "        tokenized_datasets = final_datasets.map(tokenize_function, batched=True, remove_columns=['inputs', 'targets'])\n",
    "        logging.info(\"Tokenization complete.\")\n",
    "        \n",
    "        training_args = Seq2SeqTrainingArguments(\n",
    "            output_dir=NEW_MODEL_OUTPUT_DIR,\n",
    "            num_train_epochs=NUM_EPOCHS,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "            per_device_train_batch_size=BATCH_SIZE,\n",
    "            per_device_eval_batch_size=BATCH_SIZE,\n",
    "            gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
    "            weight_decay=WEIGHT_DECAY,\n",
    "            logging_dir=f\"{NEW_MODEL_OUTPUT_DIR}/logs\",\n",
    "            logging_steps=50,\n",
    "            \n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            \n",
    "            save_total_limit=NUM_EPOCHS,\n",
    "            predict_with_generate=True,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            \n",
    "            load_best_model_at_end=False,\n",
    "            metric_for_best_model=\"rouge2\",\n",
    "\n",
    "            generation_max_length=MAX_SUMMARY_LENGTH_EVAL,\n",
    "            generation_num_beams=NUM_BEAMS_EVAL,\n",
    "        )\n",
    "\n",
    "        data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "        rouge_metric = evaluate.load(\"rouge\")\n",
    "\n",
    "        def compute_metrics(eval_pred):\n",
    "            predictions, labels = eval_pred\n",
    "            predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "            decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "            labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "            decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "            result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "            return {k: round(v * 100, 4) for k, v in result.items()}\n",
    "\n",
    "        trainer = Seq2SeqTrainer(\n",
    "            model=model,\n",
    "            args=training_args,\n",
    "            train_dataset=tokenized_datasets[\"train\"],\n",
    "            eval_dataset=tokenized_datasets[\"test\"],\n",
    "            tokenizer=tokenizer,\n",
    "            data_collator=data_collator,\n",
    "            compute_metrics=compute_metrics,\n",
    "        )\n",
    "\n",
    "        logging.info(\"Starting continued fine-tuning...\")\n",
    "        trainer.train()\n",
    "        logging.info(\"Fine-tuning finished successfully.\")\n",
    "\n",
    "        find_and_save_best_model(NEW_MODEL_OUTPUT_DIR)\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred: {e}\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb54c00",
   "metadata": {},
   "source": [
    "Inference and Demonstration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fda5ab4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers[torch] sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "62c8a6d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: mt5-base-cnn-summarizer-en-hi_v5/final_model\n",
      "Using device: cuda\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "mt5-base-cnn-summarizer-en-hi_v5/final_model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\requests\\models.py:1026\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/mt5-base-cnn-summarizer-en-hi_v5/final_model/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\utils\\hub.py:478\u001b[0m, in \u001b[0;36mcached_files\u001b[1;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    477\u001b[0m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[1;32m--> 478\u001b[0m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1010\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m   1009\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1010\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1011\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m   1012\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1013\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m   1014\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1015\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1016\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1017\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1018\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m   1019\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1020\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1021\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1022\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1023\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1024\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1117\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m   1116\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[1;32m-> 1117\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1119\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1658\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[1;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[0;32m   1653\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1654\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m\n\u001b[0;32m   1655\u001b[0m ):\n\u001b[0;32m   1656\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m   1657\u001b[0m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[1;32m-> 1658\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[0;32m   1659\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1660\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1546\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[1;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[0;32m   1545\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1546\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1547\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\n\u001b[0;32m   1548\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\huggingface_hub\\file_download.py:1463\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers, endpoint)\u001b[0m\n\u001b[0;32m   1462\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1463\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1464\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1465\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1466\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1467\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1469\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1470\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1471\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1472\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\huggingface_hub\\file_download.py:286\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    285\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 286\u001b[0m     response \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m    287\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    288\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    289\u001b[0m         follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    291\u001b[0m     )\n\u001b[0;32m    293\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\huggingface_hub\\file_download.py:310\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    309\u001b[0m response \u001b[38;5;241m=\u001b[39m http_backoff(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, retry_on_exceptions\u001b[38;5;241m=\u001b[39m(), retry_on_status_codes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m429\u001b[39m,))\n\u001b[1;32m--> 310\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:459\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    450\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    451\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    457\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m https://huggingface.co/docs/huggingface_hub/authentication\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    458\u001b[0m     )\n\u001b[1;32m--> 459\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 401 Client Error. (Request ID: Root=1-68d1c539-0b6d51b75fbb5d4c395d8955;411af5b9-fd91-40bc-965a-9b77f2bed727)\n\nRepository Not Found for url: https://huggingface.co/mt5-base-cnn-summarizer-en-hi_v5/final_model/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated. For more details, see https://huggingface.co/docs/huggingface_hub/authentication\nInvalid username or password.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 12\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing device: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDEVICE\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m model \u001b[38;5;241m=\u001b[39m MT5ForConditionalGeneration\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_PATH)\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     14\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:1058\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m   1055\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1057\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[1;32m-> 1058\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1059\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[0;32m   1060\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:890\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[0;32m    887\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[0;32m    889\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 890\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    891\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    893\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    894\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    895\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    896\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    897\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    898\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    899\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    901\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    902\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    903\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    904\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    905\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    906\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    907\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\utils\\hub.py:321\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[0;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcached_file\u001b[39m(\n\u001b[0;32m    264\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m    265\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    267\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    268\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[0;32m    270\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    320\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 321\u001b[0m     file \u001b[38;5;241m=\u001b[39m cached_files(path_or_repo_id\u001b[38;5;241m=\u001b[39mpath_or_repo_id, filenames\u001b[38;5;241m=\u001b[39m[filename], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    322\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\utils\\hub.py:510\u001b[0m, in \u001b[0;36mcached_files\u001b[1;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    507\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;66;03m# We cannot recover from them\u001b[39;00m\n\u001b[0;32m    509\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RepositoryNotFoundError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, GatedRepoError):\n\u001b[1;32m--> 510\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    511\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    512\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    513\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `hf auth login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    514\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    515\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    516\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RevisionNotFoundError):\n\u001b[0;32m    517\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    518\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    519\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    520\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    521\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: mt5-base-cnn-summarizer-en-hi_v5/final_model is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `hf auth login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, MT5ForConditionalGeneration\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_PATH = \"mt5-base-cnn-summarizer-en-hi_v5/final_model\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# --- Load Model and Tokenizer ---\n",
    "print(f\"Loading model from: {MODEL_PATH}\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "model = MT5ForConditionalGeneration.from_pretrained(MODEL_PATH).to(DEVICE)\n",
    "model.eval()\n",
    "\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fcadafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_high_quality_summary(article_text):\n",
    "    \"\"\"\n",
    "    Takes a news article string and prints high-quality English and Hindi summaries.\n",
    "    \"\"\"\n",
    "    PREFIX_ENG = \"summarize English: \"\n",
    "    PREFIX_HIN = \"summarize Hindi: \"\n",
    "\n",
    "    # --- Generation Hyperparameters ---\n",
    "    NUM_BEAMS = 8\n",
    "    LENGTH_PENALTY = 2.0\n",
    "    NO_REPEAT_NGRAM_SIZE = 3\n",
    "    MIN_SUMMARY_LENGTH = 50\n",
    "    MAX_SUMMARY_LENGTH = 256\n",
    "    \n",
    "    # --- Print Source Article ---\n",
    "    print(\"=\"*80)\n",
    "    print(\"SOURCE ARTICLE:\")\n",
    "    print(\"=\"*80)\n",
    "    print(article_text)\n",
    "\n",
    "    # --- Generate English Summary ---\n",
    "    eng_input_text = PREFIX_ENG + article_text\n",
    "    eng_inputs = tokenizer(eng_input_text, return_tensors=\"pt\", max_length=1024, truncation=True).to(DEVICE)\n",
    "    \n",
    "    eng_summary_ids = model.generate(\n",
    "        eng_inputs.input_ids,\n",
    "        num_beams=NUM_BEAMS,\n",
    "        max_length=MAX_SUMMARY_LENGTH,\n",
    "        min_length=MIN_SUMMARY_LENGTH,\n",
    "        length_penalty=LENGTH_PENALTY,\n",
    "        no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    english_summary = tokenizer.decode(eng_summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATED ENGLISH SUMMARY:\")\n",
    "    print(\"=\"*80)\n",
    "    print(english_summary)\n",
    "\n",
    "    # --- Generate Hindi Summary ---\n",
    "    hin_input_text = PREFIX_HIN + article_text\n",
    "    hin_inputs = tokenizer(hin_input_text, return_tensors=\"pt\", max_length=1024, truncation=True).to(DEVICE)\n",
    "\n",
    "    hin_summary_ids = model.generate(\n",
    "        hin_inputs.input_ids,\n",
    "        num_beams=NUM_BEAMS,\n",
    "        max_length=MAX_SUMMARY_LENGTH,\n",
    "        min_length=MIN_SUMMARY_LENGTH,\n",
    "        length_penalty=LENGTH_PENALTY,\n",
    "        no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE,\n",
    "        early_stopping=True\n",
    "    )\n",
    "    hindi_summary = tokenizer.decode(hin_summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATED HINDI SUMMARY:\")\n",
    "    print(\"=\"*80)\n",
    "    print(hindi_summary)\n",
    "    print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7a6b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "article_to_test = \"\"\"\n",
    "India secured a decisive victory over Australia in the final match of the T20 series, winning by a margin of 35 runs in Bengaluru. Batting first, India posted a competitive total of 198 for 4, thanks to a powerful half-century from captain Suryakumar Yadav, who scored 78 off just 45 balls. In response, Australia's chase faltered early as they lost key wickets to India's fast bowlers.\n",
    "\"\"\"\n",
    "\n",
    "generate_high_quality_summary(article_to_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarizer_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
