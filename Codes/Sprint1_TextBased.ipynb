{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b2b9461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries for the project\n",
    "!pip install -q transformers datasets torch sentencepiece tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18e652ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All libraries imported and configured.\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configure tqdm to show progress bars\n",
    "tqdm.pandas()\n",
    "\n",
    "print(\" All libraries imported and configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ba327aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a sample of the XSum dataset...\n",
      "Dataset loaded with 200 examples.\n",
      "\n",
      "Creating synthetic Hindi summaries (this will show a progress bar)...\n",
      "\n",
      " Step 1 complete. Dataset prepared.\n",
      "\n",
      "--- Let's inspect one example to confirm: ---\n",
      "ENGLISH DOCUMENT: The full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed.\n",
      "Repair work is ongoing in Hawick and many roads in Peeblesshire remain badly affected by standing water.\n",
      "Trains on the west coast mainline face disru...\n",
      "HINDI SUMMARY: हिन्दी: Clean-up operations are continuing across the Scottish Borders and Dumfries and Galloway after flooding caused by Storm Frank.\n"
     ]
    }
   ],
   "source": [
    "# --- Define a placeholder translation function ---\n",
    "def pseudo_translate_to_hindi(text_batch):\n",
    "    \"\"\"\n",
    "    This is a dummy translation function that works on batches of text.\n",
    "    It simulates translation by adding a Hindi prefix.\n",
    "    \"\"\"\n",
    "    return [f\"हिन्दी: {text}\" for text in text_batch]\n",
    "\n",
    "\n",
    "# --- Load the dataset from Hugging Face Hub ---\n",
    "\n",
    "print(\"Loading a sample of the XSum dataset...\")\n",
    "dataset = load_dataset(\"xsum\", split=\"train[:200]\", trust_remote_code=True)\n",
    "print(f\"Dataset loaded with {len(dataset)} examples.\")\n",
    "\n",
    "\n",
    "# --- Create the cross-lingual column ---\n",
    "# We use .map() to apply our pseudo-translation to each summary.\n",
    "# The 'batched=True' argument processes multiple rows at once for speed.\n",
    "print(\"\\nCreating synthetic Hindi summaries (this will show a progress bar)...\")\n",
    "dataset = dataset.map(\n",
    "    lambda batch: {'hindi_summary': pseudo_translate_to_hindi(batch['summary'])},\n",
    "    batched=True,\n",
    "    batch_size=16  # Process in batches of 16\n",
    ")\n",
    "\n",
    "print(\"\\n Step 1 complete. Dataset prepared.\")\n",
    "print(\"\\n--- Let's inspect one example to confirm: ---\")\n",
    "print(f\"ENGLISH DOCUMENT: {dataset[0]['document'][:250]}...\")\n",
    "print(f\"HINDI SUMMARY: {dataset[0]['hindi_summary']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0766210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the mBART model and tokenizer...\n",
      " Step 2 complete. Model and tokenizer are ready.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading the mBART model and tokenizer...\")\n",
    "\n",
    "# Define the model name from Hugging Face\n",
    "model_name = \"facebook/mbart-large-50-one-to-many-mmt\"\n",
    "\n",
    "# Load the pre-trained model using safetensors to avoid the error\n",
    "model = MBartForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "# Load the tokenizer with specified source and target languages\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_name, src_lang=\"en_XX\", tgt_lang=\"hi_IN\")\n",
    "\n",
    "print(\" Step 2 complete. Model and tokenizer are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f5585c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying tokenizer to the entire dataset (this will show a progress bar)...\n",
      "\n",
      " Step 3 complete. Data is now tokenized and ready for training.\n",
      "\n",
      "--- Let's inspect the tokenized data: ---\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 200\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Define the function to tokenize our data\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the English documents (the inputs to the model)\n",
    "    model_inputs = tokenizer(examples['document'], max_length=512, truncation=True)\n",
    "\n",
    "    # Tokenize the Hindi summaries (the labels/targets for the model)\n",
    "    # The 'as_target_tokenizer' context manager ensures the tokenizer is set up for the target language.\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['hindi_summary'], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "print(\"Applying tokenizer to the entire dataset (this will show a progress bar)...\")\n",
    "# Use .map() to apply the preprocessing function to all examples\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names # Remove old text columns to save memory\n",
    ")\n",
    "\n",
    "print(\"\\n Step 3 complete. Data is now tokenized and ready for training.\")\n",
    "print(\"\\n--- Let's inspect the tokenized data: ---\")\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01a283e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.54.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cbefc3e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "TrainingArguments.__init__() got an unexpected keyword argument 'predict_with_generate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForSeq2Seq(tokenizer, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Define the training arguments (move predict_with_generate here)\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m training_args \u001b[38;5;241m=\u001b[39m \u001b[43mTrainingArguments\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./results\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_train_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mper_device_train_batch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2e-5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlogging_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43msave_total_limit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpredict_with_generate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# <-- Move this here\u001b[39;49;00m\n\u001b[0;32m     14\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Initialize the Trainer (remove predict_with_generate from here)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     18\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     19\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m     20\u001b[0m     train_dataset\u001b[38;5;241m=\u001b[39mtokenized_dataset,\n\u001b[0;32m     21\u001b[0m     data_collator\u001b[38;5;241m=\u001b[39mdata_collator,\n\u001b[0;32m     22\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: TrainingArguments.__init__() got an unexpected keyword argument 'predict_with_generate'"
     ]
    }
   ],
   "source": [
    "# The Data Collator intelligently pads batches of data to the same length.\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "# Define the training arguments (move predict_with_generate here)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=20,\n",
    "    save_total_limit=2,\n",
    "    predict_with_generate=True,  # <-- Move this here\n",
    ")\n",
    "\n",
    "# Initialize the Trainer (remove predict_with_generate from here)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "# --- Start the training ---\n",
    "print(\"\\n Starting model fine-tuning... The Trainer will show a detailed progress bar.\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n Step 4 complete. Model has been fine-tuned!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24d7f30f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# --- Save the final model and tokenizer ---\u001b[39;00m\n\u001b[0;32m      2\u001b[0m final_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./my_finetuned_news_summarizer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39msave_model(final_model_path)\n\u001b[0;32m      4\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(final_model_path)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel and tokenizer saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Save the final model and tokenizer ---\n",
    "final_model_path = \"./my_finetuned_news_summarizer\"\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "print(f\"Model and tokenizer saved to '{final_model_path}'\")\n",
    "\n",
    "\n",
    "# --- Test the new model with the pipeline ---\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"\\nLoading the fine-tuned model for inference...\")\n",
    "# Load the fine-tuned model using the pipeline for easy inference\n",
    "summarizer_pipe = pipeline(\n",
    "    \"summarization\",\n",
    "    model=final_model_path,\n",
    "    tokenizer=final_model_path,\n",
    "    src_lang=\"en_XX\",\n",
    "    tgt_lang=\"hi_IN\",\n",
    "    device=0 if torch.cuda.is_available() else -1 # Use GPU if available\n",
    ")\n",
    "\n",
    "# Our example article\n",
    "english_article_text = \"\"\"\n",
    "The Indian Space Research Organisation (ISRO) is set to launch its third lunar mission, Chandrayaan-3, aiming for a soft landing on the moon's surface. A successful landing would make India the fourth country in the world to achieve this feat, marking a major milestone for its space program.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- Generating summary with the fine-tuned model: ---\")\n",
    "result = summarizer_pipe(english_article_text, max_length=60)\n",
    "\n",
    "print(f\"\\nOriginal Article:\\n{english_article_text}\")\n",
    "print(f\"\\nFine-Tuned Hindi Summary:\\n{result[0]['summary_text']}\")\n",
    "\n",
    "print(\"\\n Step 5 complete. Project baseline is now fine-tuned and tested!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_summarizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
