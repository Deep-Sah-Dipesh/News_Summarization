{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b2b9461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary libraries for the project\n",
    "!pip install -q transformers datasets torch sentencepiece tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18e652ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All libraries imported and configured.\n"
     ]
    }
   ],
   "source": [
    "# Import all required libraries\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Configure tqdm to show progress bars\n",
    "tqdm.pandas()\n",
    "\n",
    "print(\" All libraries imported and configured.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ba327aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading a sample of the XSum dataset...\n",
      "Dataset loaded with 200 examples.\n",
      "\n",
      "Creating synthetic Hindi summaries (this will show a progress bar)...\n",
      "\n",
      " Step 1 complete. Dataset prepared.\n",
      "\n",
      "--- Let's inspect one example to confirm: ---\n",
      "ENGLISH DOCUMENT: The full cost of damage in Newton Stewart, one of the areas worst affected, is still being assessed.\n",
      "Repair work is ongoing in Hawick and many roads in Peeblesshire remain badly affected by standing water.\n",
      "Trains on the west coast mainline face disru...\n",
      "HINDI SUMMARY: हिन्दी: Clean-up operations are continuing across the Scottish Borders and Dumfries and Galloway after flooding caused by Storm Frank.\n"
     ]
    }
   ],
   "source": [
    "# --- Define a placeholder translation function ---\n",
    "def pseudo_translate_to_hindi(text_batch):\n",
    "    \"\"\"\n",
    "    This is a dummy translation function that works on batches of text.\n",
    "    It simulates translation by adding a Hindi prefix.\n",
    "    \"\"\"\n",
    "    return [f\"हिन्दी: {text}\" for text in text_batch]\n",
    "\n",
    "\n",
    "# --- Load the dataset from Hugging Face Hub ---\n",
    "\n",
    "print(\"Loading a sample of the XSum dataset...\")\n",
    "dataset = load_dataset(\"xsum\", split=\"train[:200]\", trust_remote_code=True)\n",
    "print(f\"Dataset loaded with {len(dataset)} examples.\")\n",
    "\n",
    "\n",
    "# --- Create the cross-lingual column ---\n",
    "# We use .map() to apply our pseudo-translation to each summary.\n",
    "# The 'batched=True' argument processes multiple rows at once for speed.\n",
    "print(\"\\nCreating synthetic Hindi summaries (this will show a progress bar)...\")\n",
    "dataset = dataset.map(\n",
    "    lambda batch: {'hindi_summary': pseudo_translate_to_hindi(batch['summary'])},\n",
    "    batched=True,\n",
    "    batch_size=16  # Process in batches of 16\n",
    ")\n",
    "\n",
    "print(\"\\n Step 1 complete. Dataset prepared.\")\n",
    "print(\"\\n--- Let's inspect one example to confirm: ---\")\n",
    "print(f\"ENGLISH DOCUMENT: {dataset[0]['document'][:250]}...\")\n",
    "print(f\"HINDI SUMMARY: {dataset[0]['hindi_summary']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e0766210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the mBART model and tokenizer...\n",
      " Step 2 complete. Model and tokenizer are ready.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading the mBART model and tokenizer...\")\n",
    "\n",
    "# Define the model name from Hugging Face\n",
    "model_name = \"facebook/mbart-large-50-one-to-many-mmt\"\n",
    "\n",
    "# Load the pre-trained model using safetensors to avoid the error\n",
    "model = MBartForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    use_safetensors=True\n",
    ")\n",
    "\n",
    "# Load the tokenizer with specified source and target languages\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(model_name, src_lang=\"en_XX\", tgt_lang=\"hi_IN\")\n",
    "\n",
    "print(\" Step 2 complete. Model and tokenizer are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f5585c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying tokenizer to the entire dataset (this will show a progress bar)...\n",
      "\n",
      " Step 3 complete. Data is now tokenized and ready for training.\n",
      "\n",
      "--- Let's inspect the tokenized data: ---\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 200\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# Define the function to tokenize our data\n",
    "def preprocess_function(examples):\n",
    "    # Tokenize the English documents (the inputs to the model)\n",
    "    model_inputs = tokenizer(examples['document'], max_length=512, truncation=True)\n",
    "\n",
    "    # Tokenize the Hindi summaries (the labels/targets for the model)\n",
    "    # The 'as_target_tokenizer' context manager ensures the tokenizer is set up for the target language.\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['hindi_summary'], max_length=128, truncation=True)\n",
    "\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs\n",
    "\n",
    "print(\"Applying tokenizer to the entire dataset (this will show a progress bar)...\")\n",
    "# Use .map() to apply the preprocessing function to all examples\n",
    "tokenized_dataset = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset.column_names # Remove old text columns to save memory\n",
    ")\n",
    "\n",
    "print(\"\\n Step 3 complete. Data is now tokenized and ready for training.\")\n",
    "print(\"\\n--- Let's inspect the tokenized data: ---\")\n",
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01a283e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.54.0\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "57ec3d44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\admin\\.conda\\envs\\nlp_summarizer\\lib\\site-packages (4.54.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\admin\\.conda\\envs\\nlp_summarizer\\lib\\site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\users\\admin\\.conda\\envs\\nlp_summarizer\\lib\\site-packages (from transformers) (0.34.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\admin\\.conda\\envs\\nlp_summarizer\\lib\\site-packages (from transformers) (2.0.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\admin\\.conda\\envs\\nlp_summarizer\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\admin\\.conda\\envs\\nlp_summarizer\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\admin\\.conda\\envs\\nlp_summarizer\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\.conda\\envs\\nlp_summarizer\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\admin\\.conda\\envs\\nlp_summarizer\\lib\\site-packages (from transformers) (0.21.4.dev0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\admin\\.conda\\envs\\nlp_summarizer\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\admin\\.conda\\envs\\nlp_summarizer\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\admin\\.conda\\envs\\nlp_summarizer\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\admin\\.conda\\envs\\nlp_summarizer\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\admin\\.conda\\envs\\nlp_summarizer\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\admin\\.conda\\envs\\nlp_summarizer\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\admin\\.conda\\envs\\nlp_summarizer\\lib\\site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\admin\\.conda\\envs\\nlp_summarizer\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\admin\\.conda\\envs\\nlp_summarizer\\lib\\site-packages (from requests->transformers) (2025.7.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cbefc3e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TrainingArguments' object has no attribute 'generation_config'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 15\u001b[0m\n\u001b[0;32m      3\u001b[0m data_collator \u001b[38;5;241m=\u001b[39m DataCollatorForSeq2Seq(tokenizer, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m      5\u001b[0m training_args \u001b[38;5;241m=\u001b[39m TrainingArguments(\n\u001b[0;32m      6\u001b[0m     output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./results\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      7\u001b[0m     num_train_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     save_total_limit\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m     13\u001b[0m )\n\u001b[1;32m---> 15\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[43mSeq2SeqTrainer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraining_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata_collator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_collator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m Starting model fine-tuning... The Trainer will show a detailed progress bar.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\Users\\admin\\.conda\\envs\\nlp_summarizer\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\.conda\\envs\\nlp_summarizer\\lib\\site-packages\\transformers\\trainer_seq2seq.py:89\u001b[0m, in \u001b[0;36mSeq2SeqTrainer.__init__\u001b[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, processing_class, model_init, compute_loss_func, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     73\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     74\u001b[0m     args\u001b[38;5;241m=\u001b[39margs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     84\u001b[0m     preprocess_logits_for_metrics\u001b[38;5;241m=\u001b[39mpreprocess_logits_for_metrics,\n\u001b[0;32m     85\u001b[0m )\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Override self.model.generation_config if a GenerationConfig is specified in args.\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;66;03m# Priority: args.generation_config > model.generation_config > default GenerationConfig.\u001b[39;00m\n\u001b[1;32m---> 89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneration_config\u001b[49m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     gen_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_generation_config(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgeneration_config)\n\u001b[0;32m     91\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mgeneration_config \u001b[38;5;241m=\u001b[39m gen_config\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TrainingArguments' object has no attribute 'generation_config'"
     ]
    }
   ],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=20,\n",
    "    save_total_limit=2,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "print(\"\\n Starting model fine-tuning... The Trainer will show a detailed progress bar.\")\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\n Step 4 complete. Model has been fine-tuned!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "24d7f30f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# --- Save the final model and tokenizer ---\u001b[39;00m\n\u001b[0;32m      2\u001b[0m final_model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./my_finetuned_news_summarizer\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39msave_model(final_model_path)\n\u001b[0;32m      4\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(final_model_path)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel and tokenizer saved to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfinal_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Save the final model and tokenizer ---\n",
    "final_model_path = \"./my_finetuned_news_summarizer\"\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path)\n",
    "print(f\"Model and tokenizer saved to '{final_model_path}'\")\n",
    "\n",
    "\n",
    "# --- Test the new model with the pipeline ---\n",
    "from transformers import pipeline\n",
    "\n",
    "print(\"\\nLoading the fine-tuned model for inference...\")\n",
    "# Load the fine-tuned model using the pipeline for easy inference\n",
    "summarizer_pipe = pipeline(\n",
    "    \"summarization\",\n",
    "    model=final_model_path,\n",
    "    tokenizer=final_model_path,\n",
    "    src_lang=\"en_XX\",\n",
    "    tgt_lang=\"hi_IN\",\n",
    "    device=0 if torch.cuda.is_available() else -1 # Use GPU if available\n",
    ")\n",
    "\n",
    "# Our example article\n",
    "english_article_text = \"\"\"\n",
    "The Indian Space Research Organisation (ISRO) is set to launch its third lunar mission, Chandrayaan-3, aiming for a soft landing on the moon's surface. A successful landing would make India the fourth country in the world to achieve this feat, marking a major milestone for its space program.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- Generating summary with the fine-tuned model: ---\")\n",
    "result = summarizer_pipe(english_article_text, max_length=60)\n",
    "\n",
    "print(f\"\\nOriginal Article:\\n{english_article_text}\")\n",
    "print(f\"\\nFine-Tuned Hindi Summary:\\n{result[0]['summary_text']}\")\n",
    "\n",
    "print(\"\\n Step 5 complete. Project baseline is now fine-tuned and tested!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_summarizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
