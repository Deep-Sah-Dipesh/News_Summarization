{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f37b07be",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\Lib\\site-packages\\~andas.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\Lib\\site-packages\\~andas'.\n",
      "  You can safely remove it manually.\n",
      "ERROR: Could not install packages due to an OSError: [Errno 2] No such file or directory: 'c:\\\\users\\\\admin\\\\anaconda3\\\\envs\\\\summarizer_env\\\\lib\\\\site-packages\\\\pandas-2.3.2.dist-info\\\\METADATA'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install -q --upgrade transformers datasets pandas evaluate rouge_score sentencepiece bert-score accelerate\n",
    "\n",
    "# Install BLEURT directly from its GitHub repository\n",
    "!pip install -q git+https://github.com/google-research/bleurt.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e625919",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mevaluate\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnotebook\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DatasetDict\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\evaluate\\__init__.py:29\u001b[0m\n\u001b[0;32m     25\u001b[0m SCRIPTS_VERSION \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmain\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m version\u001b[38;5;241m.\u001b[39mparse(__version__)\u001b[38;5;241m.\u001b[39mis_devrelease \u001b[38;5;28;01melse\u001b[39;00m __version__\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m version\n\u001b[1;32m---> 29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation_suite\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EvaluationSuite\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     31\u001b[0m     AudioClassificationEvaluator,\n\u001b[0;32m     32\u001b[0m     AutomaticSpeechRecognitionEvaluator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     42\u001b[0m     evaluator,\n\u001b[0;32m     43\u001b[0m )\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mhub\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m push_to_hub\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\evaluate\\evaluation_suite\\__init__.py:7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Callable, Dict, Optional, Union\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset, DownloadConfig, DownloadMode, load_dataset\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Version\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluator\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\datasets\\__init__.py:17\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2020 The HuggingFace Datasets Authors and the TensorFlow Datasets Authors.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# See the License for the specific language governing permissions and\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m __version__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m4.1.1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_dataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Column, Dataset\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ReadInstruction\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbuilder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowBasedBuilder, BuilderConfig, DatasetBuilder, GeneratorBasedBuilder\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\datasets\\arrow_dataset.py:76\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrequests\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m HTTPError\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconcurrent\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m thread_map\n\u001b[1;32m---> 76\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[0;32m     77\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_reader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowReader\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marrow_writer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ArrowWriter, OptimizedTypedSequence\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\datasets\\config.py:38\u001b[0m\n\u001b[0;32m     36\u001b[0m DILL_VERSION \u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdill\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     37\u001b[0m FSSPEC_VERSION \u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfsspec\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m---> 38\u001b[0m PANDAS_VERSION \u001b[38;5;241m=\u001b[39m \u001b[43mversion\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mversion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpandas\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     39\u001b[0m PYARROW_VERSION \u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m     40\u001b[0m HF_HUB_VERSION \u001b[38;5;241m=\u001b[39m version\u001b[38;5;241m.\u001b[39mparse(importlib\u001b[38;5;241m.\u001b[39mmetadata\u001b[38;5;241m.\u001b[39mversion(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuggingface_hub\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\packaging\\version.py:56\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(version)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mparse\u001b[39m(version: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Version:\n\u001b[0;32m     48\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Parse the given version string.\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    >>> parse('1.0.dev1')\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[38;5;124;03m    :raises InvalidVersion: When the version string is not a valid version.\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVersion\u001b[49m\u001b[43m(\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\packaging\\version.py:200\u001b[0m, in \u001b[0;36mVersion.__init__\u001b[1;34m(self, version)\u001b[0m\n\u001b[0;32m    189\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Initialize a Version object.\u001b[39;00m\n\u001b[0;32m    190\u001b[0m \n\u001b[0;32m    191\u001b[0m \u001b[38;5;124;03m:param version:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[38;5;124;03m    exception will be raised.\u001b[39;00m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# Validate the version and parse it into pieces\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m match \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_regex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mversion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m match:\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InvalidVersion(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid version: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mversion\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import evaluate\n",
    "from tqdm.notebook import tqdm\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, MT5ForConditionalGeneration\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_PATH = \"mbart-large-50-cnn-summarizer-en-hi_v11\\final_model\"\n",
    "FULL_DATA_PATH = \"../Dataset/filtered_articles_CNN.csv\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "PREFIX_ENG = \"summarize English: \"\n",
    "PREFIX_HIN = \"summarize Hindi: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7958e8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n",
      "c:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\transformers\\convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and moving to cuda...\n",
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "print(\"Loading tokenizer...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "print(f\"Loading model and moving to {DEVICE}...\")\n",
    "\n",
    "model = MT5ForConditionalGeneration.from_pretrained(MODEL_PATH).to(DEVICE)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a3a576",
   "metadata": {},
   "source": [
    "Quantitative Evaluation - Trial 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49701b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading full dataset from: ../Dataset/final_cleaned_dataset_CNN.csv\n",
      "Processing and splitting the dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa54bbdf20d74750a5c5036e2fe301ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/4919 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recreated test set with 100 samples for evaluation.\n"
     ]
    }
   ],
   "source": [
    "# Load the full original dataset\n",
    "print(f\"Loading full dataset from: {FULL_DATA_PATH}\")\n",
    "df = pd.read_csv(FULL_DATA_PATH, engine=\"python\", on_bad_lines=\"skip\")\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "raw_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "\n",
    "# Define the same formatting function used in training\n",
    "def format_dataset(batch):\n",
    "    inputs, targets = [], []\n",
    "    for article, eng_summary, hin_summary in zip(\n",
    "        batch[\"raw_news_article\"], batch[\"english_summary\"], batch[\"hindi_summary\"]\n",
    "    ):\n",
    "        if isinstance(article, str):\n",
    "            inputs.append(PREFIX_ENG + article)\n",
    "            targets.append(eng_summary)\n",
    "            inputs.append(PREFIX_HIN + article)\n",
    "            targets.append(hin_summary)\n",
    "    return {\"inputs\": inputs, \"targets\": targets}\n",
    "\n",
    "\n",
    "# Process and split the dataset\n",
    "print(\"Processing and splitting the dataset...\")\n",
    "processed_dataset = raw_dataset.map(\n",
    "    format_dataset, batched=True, remove_columns=raw_dataset.column_names\n",
    ").flatten()\n",
    "\n",
    "# IMPORTANT: Use the same test_size and seed to get the identical test set\n",
    "train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "# For a quick test, let's use a smaller sample. Remove .select() for the full evaluation.\n",
    "test_sample = test_dataset.select(range(100))\n",
    "\n",
    "print(f\"Recreated test set with {len(test_sample)} samples for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4692ccbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading evaluation metrics...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "afbb9e33a83547b6ac9d3cda3ecd6b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b63c14456dc84c69b02251faf2db4925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/405M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\bleurt\\score.py:160: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "INFO:tensorflow:Reading checkpoint C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\default\\downloads\\extracted\\64a145a740562dda9fae1ce4fb71155ccaf922d41c2355bee049709b8590e973\\bleurt-base-128.\n",
      "INFO:tensorflow:Config file found, reading.\n",
      "INFO:tensorflow:Will load checkpoint bert_custom\n",
      "INFO:tensorflow:Loads full paths and checks that files exists.\n",
      "INFO:tensorflow:... name:bert_custom\n",
      "INFO:tensorflow:... vocab_file:vocab.txt\n",
      "INFO:tensorflow:... bert_config_file:bert_config.json\n",
      "INFO:tensorflow:... do_lower_case:True\n",
      "INFO:tensorflow:... max_seq_length:128\n",
      "INFO:tensorflow:Creating BLEURT scorer.\n",
      "INFO:tensorflow:Creating WordPiece tokenizer.\n",
      "WARNING:tensorflow:From c:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\bleurt\\lib\\bert_tokenization.py:94: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n",
      "INFO:tensorflow:Creating Eager Mode predictor.\n",
      "INFO:tensorflow:Loading model.\n",
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76afd796dbff42ebb59eedce027187c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summaries for the test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f6a77158b3c94c0c8dabd1abb80416c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Computing English Metrics ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "432691f9197f4aa7a705af270bbaf4da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\admin\\.cache\\huggingface\\hub\\models--roberta-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "addfbfbd9ba5492b8cd0424af1cf05ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1510914aa33246e895d717f04d44d32f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6552fa054042de8f1d1023c4b1fffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32336834469142dba3b9ae40b7b29f4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ec660a71430457e932bdaf02b74fbe6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-2: 17.94\n",
      "  BLEURT Score: -0.3045\n",
      "  BERTScore Precision: 0.8991\n",
      "\n",
      "--- Computing Hindi Metrics ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cc7b63362264f2181a2ed0d524c08a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\anaconda3\\envs\\summarizer_env\\lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\admin\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57925167b8e94bd790f6bc3fbf954b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/625 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ea16033dc045d1957bc1a906901329",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/996k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a18a5e3ee18949b38c7ca2eeccb29388",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.96M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "679a6ac70a484909b4bf38d76cebebca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/714M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-2: 18.13\n",
      "  BLEURT Score: -0.2792\n",
      "  BERTScore Precision: 0.7319\n"
     ]
    }
   ],
   "source": [
    "# Load metrics\n",
    "print(\"Loading evaluation metrics...\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "bleurt_metric = evaluate.load(\"bleurt\", module_type=\"metric\", checkpoint=\"BLEURT-20\")\n",
    "bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "# Generate predictions\n",
    "predictions = []\n",
    "references = []\n",
    "print(\"Generating summaries for the test set...\")\n",
    "\n",
    "for example in tqdm(test_sample):\n",
    "    inputs = tokenizer(\n",
    "        example[\"inputs\"], return_tensors=\"pt\", max_length=1024, truncation=True\n",
    "    ).to(DEVICE)\n",
    "    summary_ids = model.generate(\n",
    "        inputs.input_ids, max_length=256, num_beams=4, early_stopping=True\n",
    "    )\n",
    "    prediction = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    predictions.append(prediction)\n",
    "    references.append(example[\"targets\"])\n",
    "\n",
    "# Separate by language (even indices are English, odd are Hindi)\n",
    "eng_preds = predictions[::2]\n",
    "hin_preds = predictions[1::2]\n",
    "eng_refs = references[::2]\n",
    "hin_refs = references[1::2]\n",
    "\n",
    "# Compute and display results\n",
    "print(\"\\n--- Computing English Metrics ---\")\n",
    "rouge_eng = rouge_metric.compute(predictions=eng_preds, references=eng_refs)\n",
    "bleurt_eng = bleurt_metric.compute(predictions=eng_preds, references=eng_refs)\n",
    "bert_eng = bertscore_metric.compute(\n",
    "    predictions=eng_preds, references=eng_refs, lang=\"en\"\n",
    ")\n",
    "\n",
    "print(f\"  ROUGE-2: {rouge_eng['rouge2'] * 100:.2f}\")\n",
    "print(f\"  BLEURT Score: {sum(bleurt_eng['scores']) / len(bleurt_eng['scores']):.4f}\")\n",
    "print(\n",
    "    f\"  BERTScore Precision: {sum(bert_eng['precision']) / len(bert_eng['precision']):.4f}\"\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n--- Computing Hindi Metrics ---\")\n",
    "rouge_hin = rouge_metric.compute(predictions=hin_preds, references=hin_refs)\n",
    "bleurt_hin = bleurt_metric.compute(predictions=hin_preds, references=hin_refs)\n",
    "bert_hin = bertscore_metric.compute(\n",
    "    predictions=hin_preds, references=hin_refs, lang=\"hi\"\n",
    ")\n",
    "\n",
    "print(f\"  ROUGE-2: {rouge_hin['rouge2'] * 100:.2f}\")\n",
    "print(f\"  BLEURT Score: {sum(bleurt_hin['scores']) / len(bleurt_hin['scores']):.4f}\")\n",
    "print(\n",
    "    f\"  BERTScore Precision: {sum(bert_hin['precision']) / len(bert_hin['precision']):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14509c36",
   "metadata": {},
   "source": [
    "Quantitative Evaluation - Trial 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6b73a65f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'FULL_DATA_PATH' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load the full original dataset\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading full dataset from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mFULL_DATA_PATH\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      3\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(FULL_DATA_PATH, engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m, on_bad_lines\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskip\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m df\u001b[38;5;241m.\u001b[39mdropna(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'FULL_DATA_PATH' is not defined"
     ]
    }
   ],
   "source": [
    "# Load the full original dataset\n",
    "print(f\"Loading full dataset from: {FULL_DATA_PATH}\")\n",
    "df = pd.read_csv(FULL_DATA_PATH, engine=\"python\", on_bad_lines=\"skip\")\n",
    "df.dropna(inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "raw_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "\n",
    "# Define the same formatting function used in training\n",
    "def format_dataset(batch):\n",
    "    inputs, targets = [], []\n",
    "    for article, eng_summary, hin_summary in zip(\n",
    "        batch[\"raw_news_article\"], batch[\"english_summary\"], batch[\"hindi_summary\"]\n",
    "    ):\n",
    "        if isinstance(article, str):\n",
    "            inputs.append(PREFIX_ENG + article)\n",
    "            targets.append(eng_summary)\n",
    "            inputs.append(PREFIX_HIN + article)\n",
    "            targets.append(hin_summary)\n",
    "    return {\"inputs\": inputs, \"targets\": targets}\n",
    "\n",
    "\n",
    "# Process and split the dataset\n",
    "print(\"Processing and splitting the dataset...\")\n",
    "processed_dataset = raw_dataset.map(\n",
    "    format_dataset, batched=True, remove_columns=raw_dataset.column_names\n",
    ").flatten()\n",
    "\n",
    "# IMPORTANT: Use the same test_size and seed to get the identical test set\n",
    "train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "test_dataset = train_test_split[\"test\"]\n",
    "\n",
    "# For a quick test, let's use a smaller sample. Remove .select() for the full evaluation.\n",
    "test_sample = test_dataset.select(range(100))\n",
    "\n",
    "print(f\"Recreated test set with {len(test_sample)} samples for evaluation.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8349c514",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading evaluation metrics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:evaluate_modules.metrics.evaluate-metric--bleurt.98e148b2f8c4a88aba5037e4e0e90c9fd9ec35dc37a054ded8cfef0fa801ffab.bleurt:Using default BLEURT-Base checkpoint for sequence maximum length 128. You can use a bigger model for better results with e.g.: evaluate.load('bleurt', 'bleurt-large-512').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\default\\downloads\\extracted\\64a145a740562dda9fae1ce4fb71155ccaf922d41c2355bee049709b8590e973\\bleurt-base-128.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reading checkpoint C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\default\\downloads\\extracted\\64a145a740562dda9fae1ce4fb71155ccaf922d41c2355bee049709b8590e973\\bleurt-base-128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Config file found, reading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint bert_custom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Will load checkpoint bert_custom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loads full paths and checks that files exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:bert_custom\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... name:bert_custom\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:vocab.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... vocab_file:vocab.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... bert_config_file:bert_config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... do_lower_case:True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:... max_seq_length:128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating BLEURT scorer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating WordPiece tokenizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating WordPiece tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:WordPiece tokenizer instantiated.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Creating Eager Mode predictor.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Loading model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summaries for the test set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74acfe960ab34074bb2ca63b663ce209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Computing English Metrics ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ROUGE-2: 17.94\n",
      "  BLEURT Score: -0.3045\n",
      "  BERTScore Precision: 0.8991\n",
      "\n",
      "--- Computing Hindi Metrics ---\n",
      "  ROUGE-2: 18.13\n",
      "  BLEURT Score: -0.2792\n",
      "  BERTScore Precision: 0.7319\n"
     ]
    }
   ],
   "source": [
    "# Load metrics\n",
    "print(\"Loading evaluation metrics...\")\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "bleurt_metric = evaluate.load(\"bleurt\", module_type=\"metric\", checkpoint=\"BLEURT-20\")\n",
    "bertscore_metric = evaluate.load(\"bertscore\")\n",
    "\n",
    "# Generate predictions\n",
    "predictions = []\n",
    "references = []\n",
    "print(\"Generating summaries for the test set...\")\n",
    "\n",
    "for example in tqdm(test_sample):\n",
    "    inputs = tokenizer(\n",
    "        example[\"inputs\"], return_tensors=\"pt\", max_length=1024, truncation=True\n",
    "    ).to(DEVICE)\n",
    "    summary_ids = model.generate(\n",
    "        inputs.input_ids, max_length=256, num_beams=4, early_stopping=True\n",
    "    )\n",
    "    prediction = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    predictions.append(prediction)\n",
    "    references.append(example[\"targets\"])\n",
    "\n",
    "# Separate by language (even indices are English, odd are Hindi)\n",
    "eng_preds = predictions[::2]\n",
    "hin_preds = predictions[1::2]\n",
    "eng_refs = references[::2]\n",
    "hin_refs = references[1::2]\n",
    "\n",
    "# Compute and display results\n",
    "print(\"\\n--- Computing English Metrics ---\")\n",
    "rouge_eng = rouge_metric.compute(predictions=eng_preds, references=eng_refs)\n",
    "bleurt_eng = bleurt_metric.compute(predictions=eng_preds, references=eng_refs)\n",
    "bert_eng = bertscore_metric.compute(\n",
    "    predictions=eng_preds, references=eng_refs, lang=\"en\"\n",
    ")\n",
    "\n",
    "print(f\"  ROUGE-2: {rouge_eng['rouge2'] * 100:.2f}\")\n",
    "print(f\"  BLEURT Score: {sum(bleurt_eng['scores']) / len(bleurt_eng['scores']):.4f}\")\n",
    "print(\n",
    "    f\"  BERTScore Precision: {sum(bert_eng['precision']) / len(bert_eng['precision']):.4f}\"\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\n--- Computing Hindi Metrics ---\")\n",
    "rouge_hin = rouge_metric.compute(predictions=hin_preds, references=hin_refs)\n",
    "bleurt_hin = bleurt_metric.compute(predictions=hin_preds, references=hin_refs)\n",
    "bert_hin = bertscore_metric.compute(\n",
    "    predictions=hin_preds, references=hin_refs, lang=\"hi\"\n",
    ")\n",
    "\n",
    "print(f\"  ROUGE-2: {rouge_hin['rouge2'] * 100:.2f}\")\n",
    "print(f\"  BLEURT Score: {sum(bleurt_hin['scores']) / len(bleurt_hin['scores']):.4f}\")\n",
    "print(\n",
    "    f\"  BERTScore Precision: {sum(bert_hin['precision']) / len(bert_hin['precision']):.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3358751",
   "metadata": {},
   "source": [
    "Interactive Summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "423264b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_article(article_text):\n",
    "    \"\"\"\n",
    "    Generates and prints English and Hindi summaries for a given article text.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 50)\n",
    "    print(\"               SOURCE ARTICLE\")\n",
    "    print(\"=\" * 50)\n",
    "    print(article_text)\n",
    "\n",
    "    # --- Generate English Summary ---\n",
    "    english_input = PREFIX_ENG + article_text\n",
    "    eng_inputs = tokenizer(\n",
    "        english_input, return_tensors=\"pt\", max_length=1024, truncation=True\n",
    "    ).to(DEVICE)\n",
    "    eng_summary_ids = model.generate(\n",
    "        eng_inputs.input_ids, max_length=150, num_beams=5, early_stopping=True\n",
    "    )\n",
    "    english_summary = tokenizer.decode(eng_summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"               ENGLISH SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(english_summary)\n",
    "\n",
    "    # --- Generate Hindi Summary ---\n",
    "    hindi_input = PREFIX_HIN + article_text\n",
    "    hin_inputs = tokenizer(\n",
    "        hindi_input, return_tensors=\"pt\", max_length=1024, truncation=True\n",
    "    ).to(DEVICE)\n",
    "    hin_summary_ids = model.generate(\n",
    "        hin_inputs.input_ids, max_length=200, num_beams=5, early_stopping=True\n",
    "    )\n",
    "    hindi_summary = tokenizer.decode(hin_summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"                 HINDI SUMMARY\")\n",
    "    print(\"=\" * 50)\n",
    "    print(hindi_summary)\n",
    "    print(\"\\n\" + \"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a673f0ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "               SOURCE ARTICLE\n",
      "==================================================\n",
      "\n",
      "India's Chandrayaan-3 mission has successfully soft-landed on the lunar surface, making it the fourth country to achieve this feat. The Vikram lander touched down near the Moon's south pole, an unexplored region believed to contain water ice. The successful landing is a historic moment for India's space program, demonstrating advanced capabilities in landing technology. The Pragyan rover will now descend from the lander to explore the lunar terrain and conduct scientific experiments for one lunar day, which is equivalent to 14 Earth days. The mission aims to study the Moon's geology and the potential for a sustained human presence.\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'PREFIX_ENG' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Paste any news article here to test the model\u001b[39;00m\n\u001b[0;32m      2\u001b[0m article_to_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124mIndia\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms Chandrayaan-3 mission has successfully soft-landed on the lunar surface, making it the fourth country to achieve this feat. The Vikram lander touched down near the Moon\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms south pole, an unexplored region believed to contain water ice. The successful landing is a historic moment for India\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms space program, demonstrating advanced capabilities in landing technology. The Pragyan rover will now descend from the lander to explore the lunar terrain and conduct scientific experiments for one lunar day, which is equivalent to 14 Earth days. The mission aims to study the Moon\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms geology and the potential for a sustained human presence.\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m----> 6\u001b[0m \u001b[43msummarize_article\u001b[49m\u001b[43m(\u001b[49m\u001b[43marticle_to_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[4], line 11\u001b[0m, in \u001b[0;36msummarize_article\u001b[1;34m(article_text)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(article_text)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# --- Generate English Summary ---\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m english_input \u001b[38;5;241m=\u001b[39m \u001b[43mPREFIX_ENG\u001b[49m \u001b[38;5;241m+\u001b[39m article_text\n\u001b[0;32m     12\u001b[0m eng_inputs \u001b[38;5;241m=\u001b[39m tokenizer(\n\u001b[0;32m     13\u001b[0m     english_input, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     14\u001b[0m )\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     15\u001b[0m eng_summary_ids \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m     16\u001b[0m     eng_inputs\u001b[38;5;241m.\u001b[39minput_ids, max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m150\u001b[39m, num_beams\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, early_stopping\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     17\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'PREFIX_ENG' is not defined"
     ]
    }
   ],
   "source": [
    "# Paste any news article here to test the model\n",
    "article_to_test = \"\"\"\n",
    "India's Chandrayaan-3 mission has successfully soft-landed on the lunar surface, making it the fourth country to achieve this feat. The Vikram lander touched down near the Moon's south pole, an unexplored region believed to contain water ice. The successful landing is a historic moment for India's space program, demonstrating advanced capabilities in landing technology. The Pragyan rover will now descend from the lander to explore the lunar terrain and conduct scientific experiments for one lunar day, which is equivalent to 14 Earth days. The mission aims to study the Moon's geology and the potential for a sustained human presence.\n",
    "\"\"\"\n",
    "\n",
    "summarize_article(article_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bebc5783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Loading model and tokenizer from: mbart-large-50-cnn-summarizer-en-hi_v11/final_model...\n",
      "Model and tokenizer loaded successfully and are ready to use.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import textwrap\n",
    "\n",
    "# --- Configuration ---\n",
    "MODEL_PATH = \"mbart-large-50-cnn-summarizer-en-hi_v11/final_model\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# --- Global variables to hold the loaded model and tokenizer ---\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"\n",
    "    Loads the fine-tuned mBART model and tokenizer into memory.\n",
    "    \"\"\"\n",
    "    global model, tokenizer\n",
    "    \n",
    "    if model is not None and tokenizer is not None:\n",
    "        print(\"Model and tokenizer are already loaded.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Using device: {DEVICE}\")\n",
    "    try:\n",
    "        print(f\"Loading model and tokenizer from: {MODEL_PATH}...\")\n",
    "        model = MBartForConditionalGeneration.from_pretrained(MODEL_PATH).to(DEVICE)\n",
    "        tokenizer = MBart50TokenizerFast.from_pretrained(MODEL_PATH)\n",
    "        print(\"Model and tokenizer loaded successfully and are ready to use.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        print(\"Please ensure the MODEL_PATH is set correctly.\")\n",
    "        model, tokenizer = None, None\n",
    "\n",
    "def generate_high_quality_summary(article_text):\n",
    "    \"\"\"\n",
    "    Takes a news article string and prints high-quality English and Hindi summaries.\n",
    "    Assumes the model and tokenizer have already been loaded by load_model().\n",
    "    \"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        print(\"Model is not loaded. Please run the `load_model()` function in the setup cell first.\")\n",
    "        return\n",
    "\n",
    "    # --- FINAL Generation Hyperparameters for high-quality abstractive summaries ---\n",
    "    NUM_BEAMS = 10\n",
    "    NO_REPEAT_NGRAM_SIZE = 3\n",
    "    MIN_SUMMARY_LENGTH = 30\n",
    "    MAX_SUMMARY_LENGTH = 150\n",
    "    REPETITION_PENALTY = 3.0 # Increase penalty to strongly discourage repetition\n",
    "    LENGTH_PENALTY = 1.0    # Use a neutral length penalty\n",
    "    \n",
    "    # Hybrid settings: Use sampling within the beam search for more creativity\n",
    "    DO_SAMPLE = True\n",
    "    EARLY_STOPPING = True\n",
    "    TOP_K = 50\n",
    "    TOP_P = 0.95\n",
    "    TEMPERATURE = 0.8 # Control randomness for more factual but creative output\n",
    "    \n",
    "    # --- Print Source Article ---\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SOURCE ARTICLE:\")\n",
    "    print(\"=\"*80)\n",
    "    wrapped_article = \"\\n\".join(textwrap.wrap(article_text, width=80))\n",
    "    print(wrapped_article)\n",
    "\n",
    "    # --- Tokenize the article (source language is English) ---\n",
    "    tokenizer.src_lang = \"en_XX\"\n",
    "    inputs = tokenizer(article_text, return_tensors=\"pt\", max_length=1024, truncation=True).to(DEVICE)\n",
    "\n",
    "    # --- Generate English Summary ---\n",
    "    eng_summary_ids = model.generate(\n",
    "        inputs.input_ids,\n",
    "        num_beams=NUM_BEAMS,\n",
    "        max_length=MAX_SUMMARY_LENGTH,\n",
    "        min_length=MIN_SUMMARY_LENGTH,\n",
    "        length_penalty=LENGTH_PENALTY,\n",
    "        no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE,\n",
    "        repetition_penalty=REPETITION_PENALTY,\n",
    "        do_sample=DO_SAMPLE,\n",
    "        early_stopping=EARLY_STOPPING,\n",
    "        top_k=TOP_K,\n",
    "        top_p=TOP_P,\n",
    "        temperature=TEMPERATURE,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"]\n",
    "    )\n",
    "    # english_summary = tokenizer.decode(eng_summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # print(\"\\n\" + \"=\"*80)\n",
    "    # print(\"GENERATED ENGLISH SUMMARY:\")\n",
    "    # print(\"=\"*80)\n",
    "    # wrapped_eng_summary = \"\\n\".join(textwrap.wrap(english_summary, width=80))\n",
    "    # print(wrapped_eng_summary)\n",
    "\n",
    "    # --- Generate Hindi Summary ---\n",
    "    hin_summary_ids = model.generate(\n",
    "        inputs.input_ids,\n",
    "        num_beams=NUM_BEAMS,\n",
    "        max_length=MAX_SUMMARY_LENGTH,\n",
    "        min_length=MIN_SUMMARY_LENGTH,\n",
    "        length_penalty=LENGTH_PENALTY,\n",
    "        no_repeat_ngram_size=NO_REPEAT_NGRAM_SIZE,\n",
    "        repetition_penalty=REPETITION_PENALTY,\n",
    "        do_sample=DO_SAMPLE,\n",
    "        early_stopping=EARLY_STOPPING,\n",
    "        top_k=TOP_K,\n",
    "        top_p=TOP_P,\n",
    "        temperature=TEMPERATURE,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"]\n",
    "    )\n",
    "    hindi_summary = tokenizer.decode(hin_summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATED HINDI SUMMARY:\")\n",
    "    print(\"=\"*80)\n",
    "    wrapped_hin_summary = \"\\n\".join(textwrap.wrap(hindi_summary, width=80))\n",
    "    print(wrapped_hin_summary)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# --- Load the model automatically when this cell is run ---\n",
    "load_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2bafb4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "================================================================================\n",
      " India secured a decisive victory over Australia in the final match of the T20\n",
      "series, winning by a margin of 35 runs in Bengaluru. Batting first, India posted\n",
      "a competitive total of 198 for 4, thanks to a powerful half-century from captain\n",
      "Suryakumar Yadav, who scored 78 off just 45 balls. In response, Australia's\n",
      "chase faltered early as they lost key wickets to India's fast bowlers.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY:\n",
      "================================================================================\n",
      "    20        35  \n",
      "          ,  \n",
      "   198  (4 )      \n",
      "   ,   45   78    \n",
      ",            \n",
      "      \n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "article_to_test = \"\"\"\n",
    "India secured a decisive victory over Australia in the final match of the T20 series, winning by a margin of 35 runs in Bengaluru. Batting first, India posted a competitive total of 198 for 4, thanks to a powerful half-century from captain Suryakumar Yadav, who scored 78 off just 45 balls. In response, Australia's chase faltered early as they lost key wickets to India's fast bowlers.\n",
    "\"\"\"\n",
    "\n",
    "generate_high_quality_summary(article_to_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "da7180ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "================================================================================\n",
      " A landmark international treaty to combat plastic pollution has been agreed\n",
      "upon by delegates from over 170 countries at a United Nations Environment\n",
      "Assembly session held in Nairobi. Hailed as the most significant environmental\n",
      "pact since the Paris Agreement, the resolution establishes an Intergovernmental\n",
      "Negotiating Committee (INC) tasked with drafting a legally binding agreement by\n",
      "the end of 2026. The future treaty aims to address the full lifecycle of\n",
      "plastic, from its production and design to its disposal and recycling. The\n",
      "negotiations were complex, with debates centering on whether the treaty should\n",
      "focus solely on plastic waste management or include caps on virgin plastic\n",
      "production. Major plastic-producing nations and fossil fuel companies had\n",
      "advocated for a focus on recycling, while a coalition of environmental groups\n",
      "and many developing nations pushed for stricter controls on production itself.\n",
      "The final resolution provides a broad mandate for the INC to consider all\n",
      "options.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY:\n",
      "================================================================================\n",
      "        170    \n",
      "           \n",
      "             \n",
      ",  2026           - \n",
      " (INC)            \n",
      " ,             \n",
      "            \n",
      "   virgin   \n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "article_to_test = \"\"\"\n",
    "A landmark international treaty to combat plastic pollution has been agreed upon by delegates from over 170 countries at a United Nations Environment Assembly session held in Nairobi. Hailed as the most significant environmental pact since the Paris Agreement, the resolution establishes an Intergovernmental Negotiating Committee (INC) tasked with drafting a legally binding agreement by the end of 2026. The future treaty aims to address the full lifecycle of plastic, from its production and design to its disposal and recycling. The negotiations were complex, with debates centering on whether the treaty should focus solely on plastic waste management or include caps on virgin plastic production. Major plastic-producing nations and fossil fuel companies had advocated for a focus on recycling, while a coalition of environmental groups and many developing nations pushed for stricter controls on production itself. The final resolution provides a broad mandate for the INC to consider all options.\n",
    "\"\"\"\n",
    "\n",
    "generate_high_quality_summary(article_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3eeda6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "================================================================================\n",
      " India's Chandrayaan-3 mission has successfully soft-landed on the lunar\n",
      "surface, making it the fourth country to achieve this feat. The Vikram lander\n",
      "touched down near the Moon's south pole, an unexplored region believed to\n",
      "contain water ice. The successful landing is a historic moment for India's space\n",
      "program, demonstrating advanced capabilities in landing technology. The Pragyan\n",
      "rover will now descend from the lander to explore the lunar terrain and conduct\n",
      "scientific experiments for one lunar day, which is equivalent to 14 Earth days.\n",
      "The mission aims to study the Moon's geology and the potential for a sustained\n",
      "human presence.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY:\n",
      "================================================================================\n",
      "  -3         - \n",
      "           ,   \n",
      "               explored  \n",
      "  ,          \n",
      "            \n",
      "   ,  14            \n",
      "           \n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "article_to_test = \"\"\"\n",
    "India's Chandrayaan-3 mission has successfully soft-landed on the lunar surface, making it the fourth country to achieve this feat. The Vikram lander touched down near the Moon's south pole, an unexplored region believed to contain water ice. The successful landing is a historic moment for India's space program, demonstrating advanced capabilities in landing technology. The Pragyan rover will now descend from the lander to explore the lunar terrain and conduct scientific experiments for one lunar day, which is equivalent to 14 Earth days. The mission aims to study the Moon's geology and the potential for a sustained human presence.\n",
    "\"\"\"\n",
    "\n",
    "generate_high_quality_summary(article_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f07fda96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "================================================================================\n",
      " The Indian Space Research Organisation (ISRO) has successfully completed a\n",
      "critical test for its ambitious Gaganyaan mission, which aims to send Indian\n",
      "astronauts to space. The test involved the final integrated validation of the\n",
      "crew module's parachute system at a facility in Chandigarh. The parachutes are\n",
      "essential for ensuring the safe return and landing of the crew module. Officials\n",
      "confirmed that the system performed flawlessly under simulated flight\n",
      "conditions. This milestone moves India one step closer to launching its first\n",
      "crewed spaceflight, which is currently scheduled for late 2025. The Gaganyaan\n",
      "programme is a top priority for the nation's space agency, marking its entry\n",
      "into human space exploration.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY:\n",
      "================================================================================\n",
      "    (ISRO)    Gaganyaan  \n",
      "           \n",
      "             \n",
      " ,           \n",
      "            \n",
      "              \n",
      "     ,    2025      ISO\n",
      "Gaganyan    \n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "article_to_test = \"\"\"\n",
    "The Indian Space Research Organisation (ISRO) has successfully completed a critical test for its ambitious Gaganyaan mission, which aims to send Indian astronauts to space. The test involved the final integrated validation of the crew module's parachute system at a facility in Chandigarh. The parachutes are essential for ensuring the safe return and landing of the crew module. Officials confirmed that the system performed flawlessly under simulated flight conditions. This milestone moves India one step closer to launching its first crewed spaceflight, which is currently scheduled for late 2025. The Gaganyaan programme is a top priority for the nation's space agency, marking its entry into human space exploration.\n",
    "\"\"\"\n",
    "\n",
    "generate_high_quality_summary(article_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "419df0a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "================================================================================\n",
      " Google has announced a significant upgrade to its core AI model, Gemini. The\n",
      "new version, named Gemini 1.5 Pro, is designed to handle a much larger amount of\n",
      "information at once. The company claims it can process up to 1 million tokens,\n",
      "which is equivalent to an entire feature-length movie or over 700,000 words of\n",
      "text. This massive context window allows the model to understand and reason\n",
      "about very large documents, codebases, or hours of video content without\n",
      "forgetting earlier details. The new model is initially being made available to\n",
      "developers and enterprise customers through Google's AI Studio and Vertex AI\n",
      "platforms. This development is seen as a major step in the competition against\n",
      "other leading AI models like OpenAI's GPT-4.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY:\n",
      "================================================================================\n",
      "Google    AI  Gemini 1.5 Pro       \n",
      ",             1 \n",
      "     ,    -   700,000  \n",
      "             \n",
      "     ,         \n",
      "               AI\n",
      "Studio  Vertex AI          \n",
      "   \n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "article_to_test = \"\"\"\n",
    "Google has announced a significant upgrade to its core AI model, Gemini. The new version, named Gemini 1.5 Pro, is designed to handle a much larger amount of information at once. The company claims it can process up to 1 million tokens, which is equivalent to an entire feature-length movie or over 700,000 words of text. This massive context window allows the model to understand and reason about very large documents, codebases, or hours of video content without forgetting earlier details. The new model is initially being made available to developers and enterprise customers through Google's AI Studio and Vertex AI platforms. This development is seen as a major step in the competition against other leading AI models like OpenAI's GPT-4.\"\"\"\n",
    "\n",
    "generate_high_quality_summary(article_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5febf24a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "================================================================================\n",
      "TThe Reserve Bank of India (RBI) has announced that it will keep the repo rate\n",
      "unchanged at 6.5% for the eighth consecutive time. The decision was made by the\n",
      "Monetary Policy Committee (MPC) following its recent three-day meeting. RBI\n",
      "Governor Shaktikanta Das stated that the committee is focused on ensuring\n",
      "inflation aligns with the target of 4% while supporting economic growth. The\n",
      "central bank also retained its GDP growth forecast for the current fiscal year\n",
      "at 7.2%. The decision was widely expected by economists, who believe that a\n",
      "stable policy rate is necessary to manage potential food price inflation and\n",
      "global economic uncertainties before considering any rate cuts later in the\n",
      "year.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY:\n",
      "================================================================================\n",
      "   (RBI)        6.5%   \n",
      "      Monetary Policy Committee (MPC)    \n",
      "      ,  RBI        \n",
      "  4%            \n",
      "     , RBI        7.2% \n",
      "           \n",
      "          \n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "article_to_test = \"\"\"TThe Reserve Bank of India (RBI) has announced that it will keep the repo rate unchanged at 6.5% for the eighth consecutive time. The decision was made by the Monetary Policy Committee (MPC) following its recent three-day meeting. RBI Governor Shaktikanta Das stated that the committee is focused on ensuring inflation aligns with the target of 4% while supporting economic growth. The central bank also retained its GDP growth forecast for the current fiscal year at 7.2%. The decision was widely expected by economists, who believe that a stable policy rate is necessary to manage potential food price inflation and global economic uncertainties before considering any rate cuts later in the year.\"\"\"\n",
    "generate_high_quality_summary(article_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715804aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f1ea7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE (truncated):\n",
      "================================================================================\n",
      "The United States has long dominated the global technology market, but China is\n",
      "determined to challenge that supremacy by investing heavily in artificial\n",
      "intelligence, robotics, and the production of high-end chips that power these\n",
      "advanced technologies. With Beijing pouring billions into semiconductor\n",
      "development, the gap between the two nations is narrowing rapidly. Nvidia CEO\n",
      "Jensen Huang recently warned that China is just nanoseconds behind the US in\n",
      "chip progress, highlighting the growing competitiveness of Chinese firms. The\n",
      "rise of DeepSeek in 2024 marked a turning point  the Chinese startup launched a\n",
      "ChatGPT rival that was cheaper to train and used fewer high-end chips,\n",
      "momentarily shaking Nvidias market value. Since then, Chinas momentum in the\n",
      "tech sector has only accelerated, with giants like Alibaba and Huawei unveiling\n",
      "powerful new chips that reportedly rival Nvidias processors. Alibabas latest\n",
      "chip was said to match Nvidias H20 in performance while using less ene...\n",
      "\n",
      "(Article length: 318 words. Target summary length: 30-510 tokens)\n",
      "\n",
      "================================================================================\n",
      "GENERATED ENGLISH SUMMARY:\n",
      "================================================================================\n",
      "The United States has historically dominated the global technology market, but\n",
      "China is determined to challenge its dominance by investing heavily in\n",
      "artificial intelligence, robotics, and high-end chips. With Beijing pouring\n",
      "billions into semiconductor development, the technological gap between the two\n",
      "nations is rapidly narrowing. Nvidia CEO Jensen Huang recently warned that China\n",
      "is just \"nanoseconds behind\" the US in chip progress, highlighting the growing\n",
      "competitiveness of Chinese firms. A pivotal moment occurred when DeepSeek\n",
      "launched a cheaper ChatGPT rival, shattering Nvidia's market value. Since then,\n",
      "China's tech sector has accelerated, with giants like Alibaba and Huawei\n",
      "unveiling powerful new chips, reportedly competing with Nvidias processors.\n",
      "Alibaba's H20 chip delivered performance while using less energy, while Huawei\n",
      "announced its strongest chips. Other domestic players like MetaX and Cambricon\n",
      "Technologies are gaining traction, supplying chips to state-owned China Unicom,\n",
      "as investors bet on Beijing's self-reliance. Tech giants such as Tencent and\n",
      "trade shows are promoting local innovations\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY:\n",
      "================================================================================\n",
      "            \n",
      ",   -        \n",
      "         ,         \n",
      "   Nvidia CEO Jensen Huang          \" \" ,\n",
      "    2024  DeepSeek         startup \n",
      "ChatGPT (ChatGPT)    ,  Nvidia     \n",
      "Alibaba  Huawei         ,  H20\n",
      "        MetaX  Cambricon Technologies  ,\n",
      " -          Tech giants Tencent \n",
      "Trade Shows         Nvidia    \n",
      "         \n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "article_to_test = \"\"\"The United States has long dominated the global technology market, but China is determined to challenge that supremacy by investing heavily in artificial intelligence, robotics, and the production of high-end chips that power these advanced technologies. With Beijing pouring billions into semiconductor development, the gap between the two nations is narrowing rapidly. Nvidia CEO Jensen Huang recently warned that China is just nanoseconds behind the US in chip progress, highlighting the growing competitiveness of Chinese firms. The rise of DeepSeek in 2024 marked a turning point  the Chinese startup launched a ChatGPT rival that was cheaper to train and used fewer high-end chips, momentarily shaking Nvidias market value. Since then, Chinas momentum in the tech sector has only accelerated, with giants like Alibaba and Huawei unveiling powerful new chips that reportedly rival Nvidias processors. Alibabas latest chip was said to match Nvidias H20 in performance while using less energy, while Huawei announced its strongest chips yet alongside a three-year plan to challenge Nvidias AI market dominance and open its technology to the public to reduce dependence on US systems. Other domestic players, including MetaX and Cambricon Technologies, are also gaining tractionMetaX now supplies chips to state-owned China Unicom, while Cambricons stock value has more than doubled as investors bet on Beijings push for self-reliance. Tech giants such as Tencent have joined the movement, shifting toward Chinese-made chips, and state-backed trade shows are promoting local innovations to attract global investors. Nvidia has acknowledged the growing competition but maintains confidence that customers will choose the best technology available. However, experts caution that Chinas claims should be treated carefully due to limited public testing data. According to computer scientist Jawad Haj-Yahya, Chinese semiconductors are approaching US performance levels in predictive AI but still lag in complex analytics. While the technological gap is clearly narrowing, it remains uncertain whether China can fully catch up with American innovation in the near future.\"\"\"\n",
    "generate_summary(article_to_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarizer_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
