{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "788f8f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# --- Main Paths ---\n",
    "BASE_MODEL = \"facebook/mbart-large-50\"\n",
    "DATA_PATH = \"../../Dataset/new_large_CNN_dataset.csv\" \n",
    "MODEL_OUTPUT_DIR = \"mbart-large-50-cnn-summarizer-v15\"\n",
    "FINAL_SAVE_PATH = os.path.join(MODEL_OUTPUT_DIR, \"final_model\")\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 4\n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "WEIGHT_DECAY = 0.1 # Regularization to prevent overfitting\n",
    "\n",
    "# --- Evaluation & Generation ---\n",
    "METRIC_FOR_BEST_MODEL = \"eval_bleurt_f1\"\n",
    "MAX_INPUT_LENGTH = 1024\n",
    "MAX_SUMMARY_LENGTH = 256\n",
    "EVAL_BEAMS = 5\n",
    "\n",
    "# --- Compute ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5345b93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def sanitize_text(text: str) -> str:\n",
    "    \"\"\"Basic text cleanup.\"\"\"\n",
    "    if not isinstance(text, str): \n",
    "        return \"\"\n",
    "    return text.replace('\"\"', '\"').strip()\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalize unicode characters.\"\"\"\n",
    "    if not isinstance(text, str): \n",
    "        return \"\"\n",
    "    return ' '.join(unicodedata.normalize('NFKC', text).split())\n",
    "\n",
    "def load_and_prep_dataset(data_path: str) -> DatasetDict:\n",
    "    \"\"\"Loads, cleans, formats, and splits the dataset.\"\"\"\n",
    "    \n",
    "    # Load and clean\n",
    "    df = pd.read_csv(data_path, engine='python', on_bad_lines='skip')\n",
    "    df.dropna(subset=['raw_news_article', 'english_summary', 'hindi_summary'], inplace=True)\n",
    "    \n",
    "    for col in ['raw_news_article', 'english_summary', 'hindi_summary']:\n",
    "        df[col] = df[col].apply(sanitize_text).apply(normalize_text)\n",
    "    \n",
    "    raw_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "    # Format for multilingual training\n",
    "    processed_dataset = raw_dataset.map(\n",
    "        _format_dataset_mbart, \n",
    "        batched=True, \n",
    "        remove_columns=raw_dataset.column_names\n",
    "    )\n",
    "    \n",
    "    # Split\n",
    "    train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    return DatasetDict({\n",
    "        'train': train_test_split['train'],\n",
    "        'test': train_test_split['test']\n",
    "    })\n",
    "\n",
    "def _format_dataset_mbart(batch):\n",
    "    \"\"\"Duplicates each article for its English and Hindi summary.\"\"\"\n",
    "    inputs, targets, langs = [], [], []\n",
    "    for article, eng_summary, hin_summary in zip(\n",
    "        batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']\n",
    "    ):\n",
    "        if isinstance(article, str) and article:\n",
    "            inputs.append(article)\n",
    "            targets.append(eng_summary)\n",
    "            langs.append(\"en_XX\")\n",
    "            \n",
    "            inputs.append(article)\n",
    "            targets.append(hin_summary)\n",
    "            langs.append(\"hi_IN\")\n",
    "            \n",
    "    return {'article': inputs, 'summary': targets, 'target_lang': langs}\n",
    "\n",
    "def tokenize_function(examples, tokenizer, max_input_len, max_summary_len):\n",
    "    \"\"\"Tokenizes articles (inputs) and summaries (labels).\"\"\"\n",
    "    \n",
    "    tokenizer.src_lang = \"en_XX\"\n",
    "    model_inputs = tokenizer(\n",
    "        examples['article'], \n",
    "        max_length=max_input_len, \n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    labels_batch = []\n",
    "    for i in range(len(examples['summary'])):\n",
    "        tokenizer.tgt_lang = examples['target_lang'][i]\n",
    "        labels = tokenizer(\n",
    "            text_target=examples['summary'][i], \n",
    "            max_length=max_summary_len, \n",
    "            truncation=True\n",
    "        )\n",
    "        labels_batch.append(labels['input_ids'])\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels_batch\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ef46716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\bleurt\\score.py:160: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "INFO:tensorflow:Reading checkpoint C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\bleurt-20\\downloads\\extracted\\8db8856a80394ae84b010e83ab663d4a3ccfa244ce3d0dbe00143f73e65ff123\\BLEURT-20.\n",
      "INFO:tensorflow:Config file found, reading.\n",
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n",
      "INFO:tensorflow:Loads full paths and checks that files exists.\n",
      "INFO:tensorflow:... name:BLEURT-20\n",
      "INFO:tensorflow:... bert_config_file:bert_config.json\n",
      "INFO:tensorflow:... max_seq_length:512\n",
      "INFO:tensorflow:... vocab_file:None\n",
      "INFO:tensorflow:... do_lower_case:None\n",
      "INFO:tensorflow:... sp_model:sent_piece\n",
      "INFO:tensorflow:... dynamic_seq_length:True\n",
      "INFO:tensorflow:Creating BLEURT scorer.\n",
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n",
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n",
      "INFO:tensorflow:Will load model: C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\bleurt-20\\downloads\\extracted\\8db8856a80394ae84b010e83ab663d4a3ccfa244ce3d0dbe00143f73e65ff123\\BLEURT-20\\sent_piece.model.\n",
      "INFO:tensorflow:SentencePiece tokenizer created.\n",
      "INFO:tensorflow:Creating Eager Mode predictor.\n",
      "INFO:tensorflow:Loading model.\n",
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Initialize metrics globally to avoid reloading\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "bleurt_metric = evaluate.load(\"bleurt\", \"bleurt-20\")\n",
    "\n",
    "def compute_metrics(eval_pred, tokenizer):\n",
    "    \"\"\"Decodes predictions and computes ROUGE and BLEURT scores.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Compute ROUGE\n",
    "    rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    \n",
    "    # Compute BLEURT\n",
    "    bleurt_result = bleurt_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    \n",
    "    result = {\n",
    "        \"rouge1\": rouge_result[\"rouge1\"],\n",
    "        \"rouge2\": rouge_result[\"rouge2\"],\n",
    "        \"rougeL\": rouge_result[\"rougeL\"],\n",
    "        \"bleurt_f1\": np.mean(bleurt_result[\"scores\"])\n",
    "    }\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a1d51da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aeb7cdbfa7f43e8a50946bbf66d0dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019c85b7f5784378a9e6f7151cbe2c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46533bdd07304e389cdfa024e7afe3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_27548\\1931657635.py:96: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='429' max='8304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 429/8304 07:38 < 2:20:50, 0.93 it/s, Epoch 0.21/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 114\u001b[0m\n\u001b[0;32m    111\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluate.py\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to find the best model and save it.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 114\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 108\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# 6. Start Training\u001b[39;00m\n\u001b[0;32m    107\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Training started for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 108\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Training finished successfully ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    110\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoints and logs are saved in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mMODEL_OUTPUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\trainer.py:2674\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2667\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2668\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2670\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2672\u001b[0m )\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2680\u001b[0m ):\n\u001b[0;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\trainer.py:4020\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   4017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   4019\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 4020\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4022\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   4023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4024\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4025\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   4026\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\trainer.py:4110\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   4108\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   4109\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m-> 4110\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   4112\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   4113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\accelerate\\utils\\operations.py:818\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\accelerate\\utils\\operations.py:806\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\amp\\autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\models\\mbart\\modeling_mbart.py:1439\u001b[0m, in \u001b[0;36mMBartForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1436\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1437\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id)\n\u001b[1;32m-> 1439\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1440\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1450\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1451\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1452\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1453\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1456\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1457\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\n\u001b[0;32m   1459\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\models\\mbart\\modeling_mbart.py:1267\u001b[0m, in \u001b[0;36mMBartModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1261\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1262\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1263\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1264\u001b[0m     )\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1267\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1276\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1278\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1281\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\models\\mbart\\modeling_mbart.py:1116\u001b[0m, in \u001b[0;36mMBartDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dropout_probability \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayerdrop:\n\u001b[0;32m   1114\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 1116\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[0;32m   1120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1124\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1128\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\models\\mbart\\modeling_mbart.py:420\u001b[0m, in \u001b[0;36mMBartDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_values, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[0;32m    417\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_layer_norm(hidden_states)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 420\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    428\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    429\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\models\\mbart\\modeling_mbart.py:228\u001b[0m, in \u001b[0;36mMBartAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, past_key_values, attention_mask, layer_head_mask, output_attentions, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    225\u001b[0m kv_input_shape \u001b[38;5;241m=\u001b[39m (bsz, src_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# get query proj\u001b[39;00m\n\u001b[1;32m--> 228\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mq_input_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    230\u001b[0m is_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# Import custom modules\n",
    "import config\n",
    "import data_utils\n",
    "import metrics_utils\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Configures logging to file and stream.\"\"\"\n",
    "    os.makedirs(config.MODEL_OUTPUT_DIR, exist_ok=True)\n",
    "    log_filename = os.path.join(\n",
    "        config.MODEL_OUTPUT_DIR, \n",
    "        f\"training_log_v15_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "    )\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_filename),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    logging.info(f\"--- Starting mBART v15 Training from Scratch ---\")\n",
    "    logging.info(f\"Logging to: {log_filename}\")\n",
    "\n",
    "def main():\n",
    "    setup_logging()\n",
    "\n",
    "    # 1. Load and Prepare Data\n",
    "    logging.info(f\"Loading and processing data from: {config.DATA_PATH}\")\n",
    "    tokenized_datasets = data_utils.load_and_prep_dataset(config.DATA_PATH)\n",
    "    logging.info(f\"Dataset split: {len(tokenized_datasets['train'])} train, {len(tokenized_datasets['test'])} test\")\n",
    "\n",
    "    # 2. Load Base Model and Tokenizer\n",
    "    logging.info(f\"Loading base model and tokenizer from: {config.BASE_MODEL}\")\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(config.BASE_MODEL)\n",
    "    model = MBartForConditionalGeneration.from_pretrained(config.BASE_MODEL, use_safetensors=True)\n",
    "    \n",
    "    # 3. Tokenize Datasets\n",
    "    logging.info(\"Tokenizing datasets...\")\n",
    "    tokenized_datasets = tokenized_datasets.map(\n",
    "        partial(\n",
    "            data_utils.tokenize_function,\n",
    "            tokenizer=tokenizer,\n",
    "            max_input_len=config.MAX_INPUT_LENGTH,\n",
    "            max_summary_len=config.MAX_SUMMARY_LENGTH\n",
    "        ),\n",
    "        batched=True,\n",
    "        remove_columns=['article', 'summary', 'target_lang']\n",
    "    )\n",
    "    \n",
    "    # 4. Define Training Arguments\n",
    "    logging.info(\"Configuring training arguments...\")\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=config.MODEL_OUTPUT_DIR,\n",
    "        \n",
    "        # Hyperparameters\n",
    "        num_train_epochs=config.NUM_EPOCHS,\n",
    "        learning_rate=config.LEARNING_RATE,\n",
    "        per_device_train_batch_size=config.BATCH_SIZE,\n",
    "        per_device_eval_batch_size=config.BATCH_SIZE,\n",
    "        gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n",
    "        weight_decay=config.WEIGHT_DECAY,\n",
    "        \n",
    "        # Logging and Saving (per epoch, as requested)\n",
    "        logging_dir=os.path.join(config.MODEL_OUTPUT_DIR, \"logs\"),\n",
    "        logging_strategy=\"epoch\",\n",
    "        eval_strategy=\"epoch\", # Corrected from evaluation_strategy\n",
    "        save_strategy=\"epoch\",\n",
    "        \n",
    "        # We will run a separate script to find the best model\n",
    "        load_best_model_at_end=False,\n",
    "        save_total_limit=config.NUM_EPOCHS, # Save all checkpoints\n",
    "        \n",
    "        # Other settings\n",
    "        predict_with_generate=True,\n",
    "        fp16=config.DEVICE.type == 'cuda',\n",
    "        report_to=\"tensorboard\",\n",
    "        generation_max_length=config.MAX_SUMMARY_LENGTH,\n",
    "        generation_num_beams=config.EVAL_BEAMS,\n",
    "    )\n",
    "    \n",
    "    # 5. Initialize Trainer\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=partial(metrics_utils.compute_metrics, tokenizer=tokenizer),\n",
    "    )\n",
    "\n",
    "    # 6. Start Training\n",
    "    logging.info(f\"--- Training started for {config.NUM_EPOCHS} epochs ---\")\n",
    "    trainer.train()\n",
    "    logging.info(\"--- Training finished successfully ---\")\n",
    "    logging.info(f\"Checkpoints and logs are saved in: {config.MODEL_OUTPUT_DIR}\")\n",
    "    logging.info(\"Run 'evaluate.py' to find the best model and save it.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b37aec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f9508a1193403c84ae7ea51cf68117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b91884832dd42f0bfcca9165d279a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b02694ddfb4b05b40f57dacf3bd05c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_27548\\1931657635.py:96: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1378' max='8304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1378/8304 26:18 < 2:12:22, 0.87 it/s, Epoch 0.66/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 114\u001b[0m\n\u001b[0;32m    111\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluate.py\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to find the best model and save it.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 114\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 108\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# 6. Start Training\u001b[39;00m\n\u001b[0;32m    107\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Training started for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 108\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Training finished successfully ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    110\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoints and logs are saved in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mMODEL_OUTPUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\trainer.py:2674\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2667\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2668\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2670\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2672\u001b[0m )\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2680\u001b[0m ):\n\u001b[0;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\trainer.py:4071\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   4068\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   4069\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 4071\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   4073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\accelerate\\accelerator.py:2730\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2728\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2729\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2730\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\_tensor.py:624\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    615\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    616\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    617\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    622\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    623\u001b[0m     )\n\u001b[1;32m--> 624\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# Import custom modules\n",
    "import config\n",
    "import data_utils\n",
    "import metrics_utils\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Configures logging to file and stream.\"\"\"\n",
    "    os.makedirs(config.MODEL_OUTPUT_DIR, exist_ok=True)\n",
    "    log_filename = os.path.join(\n",
    "        config.MODEL_OUTPUT_DIR, \n",
    "        f\"training_log_v15_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "    )\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_filename),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    logging.info(f\"--- Starting mBART v15 Training from Scratch ---\")\n",
    "    logging.info(f\"Logging to: {log_filename}\")\n",
    "\n",
    "def main():\n",
    "    setup_logging()\n",
    "\n",
    "    # 1. Load and Prepare Data\n",
    "    logging.info(f\"Loading and processing data from: {config.DATA_PATH}\")\n",
    "    tokenized_datasets = data_utils.load_and_prep_dataset(config.DATA_PATH)\n",
    "    logging.info(f\"Dataset split: {len(tokenized_datasets['train'])} train, {len(tokenized_datasets['test'])} test\")\n",
    "\n",
    "    # 2. Load Base Model and Tokenizer\n",
    "    logging.info(f\"Loading base model and tokenizer from: {config.BASE_MODEL}\")\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(config.BASE_MODEL)\n",
    "    model = MBartForConditionalGeneration.from_pretrained(config.BASE_MODEL, use_safetensors=True)\n",
    "    \n",
    "    # 3. Tokenize Datasets\n",
    "    logging.info(\"Tokenizing datasets...\")\n",
    "    tokenized_datasets = tokenized_datasets.map(\n",
    "        partial(\n",
    "            data_utils.tokenize_function,\n",
    "            tokenizer=tokenizer,\n",
    "            max_input_len=config.MAX_INPUT_LENGTH,\n",
    "            max_summary_len=config.MAX_SUMMARY_LENGTH\n",
    "        ),\n",
    "        batched=True,\n",
    "        remove_columns=['article', 'summary', 'target_lang']\n",
    "    )\n",
    "    \n",
    "    # 4. Define Training Arguments\n",
    "    logging.info(\"Configuring training arguments...\")\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=config.MODEL_OUTPUT_DIR,\n",
    "        \n",
    "        # Hyperparameters\n",
    "        num_train_epochs=config.NUM_EPOCHS,\n",
    "        learning_rate=config.LEARNING_RATE,\n",
    "        per_device_train_batch_size=config.BATCH_SIZE,\n",
    "        per_device_eval_batch_size=config.BATCH_SIZE,\n",
    "        gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n",
    "        weight_decay=config.WEIGHT_DECAY,\n",
    "        \n",
    "        # Logging and Saving (per epoch, as requested)\n",
    "        logging_dir=os.path.join(config.MODEL_OUTPUT_DIR, \"logs\"),\n",
    "        logging_strategy=\"epoch\",\n",
    "        eval_strategy=\"epoch\", # Corrected from evaluation_strategy\n",
    "        save_strategy=\"epoch\",\n",
    "        \n",
    "        # We will run a separate script to find the best model\n",
    "        load_best_model_at_end=False,\n",
    "        save_total_limit=config.NUM_EPOCHS, # Save all checkpoints\n",
    "        \n",
    "        # Other settings\n",
    "        predict_with_generate=True,\n",
    "        fp16=config.DEVICE.type == 'cuda',\n",
    "        report_to=\"tensorboard\",\n",
    "        generation_max_length=config.MAX_SUMMARY_LENGTH,\n",
    "        generation_num_beams=config.EVAL_BEAMS,\n",
    "    )\n",
    "    \n",
    "    # 5. Initialize Trainer\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=partial(metrics_utils.compute_metrics, tokenizer=tokenizer),\n",
    "    )\n",
    "\n",
    "    # 6. Start Training\n",
    "    logging.info(f\"--- Training started for {config.NUM_EPOCHS} epochs ---\")\n",
    "    trainer.train()\n",
    "    logging.info(\"--- Training finished successfully ---\")\n",
    "    logging.info(f\"Checkpoints and logs are saved in: {config.MODEL_OUTPUT_DIR}\")\n",
    "    logging.info(\"Run 'evaluate.py' to find the best model and save it.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7e1b65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 18:21:14,270 [INFO] - --- Starting mBART v15 Training ---\n",
      "2025-11-13 18:21:14,275 [INFO] - Logging to: mbart-large-50-cnn-summarizer-v15\\training_log_v15_2025-11-13_18-21-14.log\n",
      "2025-11-13 18:21:14,276 [INFO] - Loading and processing data from: ../../Dataset/new_large_CNN_dataset.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e96f6c3694774737bc9d2848efe3ac9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 18:21:17,640 [INFO] - Dataset split: 16601 train, 1845 test\n",
      "2025-11-13 18:21:17,642 [INFO] - Loading base model and tokenizer from: facebook/mbart-large-50\n",
      "2025-11-13 18:21:26,440 [INFO] - Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab0d5a9680f475dafc859a10d0c8301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9428dc4e74ff40adba7dd00f576ee927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 18:22:12,913 [INFO] - Configuring training arguments...\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_27548\\1042724579.py:99: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "2025-11-13 18:22:14,899 [INFO] - --- Training started for 4 epochs ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='949' max='8304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 949/8304 17:24 < 2:15:09, 0.91 it/s, Epoch 0.46/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 117\u001b[0m\n\u001b[0;32m    114\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluate.py\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to find the best model and save it.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 111\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# 6. Start Training\u001b[39;00m\n\u001b[0;32m    110\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Training started for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 111\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Training finished successfully ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    113\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoints and logs are saved in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mMODEL_OUTPUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\trainer.py:2674\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2667\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2668\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2670\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2672\u001b[0m )\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2680\u001b[0m ):\n\u001b[0;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\trainer.py:4071\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   4068\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   4069\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 4071\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   4073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\accelerate\\accelerator.py:2730\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2728\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2729\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2730\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\_tensor.py:624\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    615\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    616\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    617\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    622\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    623\u001b[0m     )\n\u001b[1;32m--> 624\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# Import custom modules\n",
    "import config\n",
    "import data_utils\n",
    "import metrics_utils\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Configures logging to file and stream.\"\"\"\n",
    "    os.makedirs(config.MODEL_OUTPUT_DIR, exist_ok=True)\n",
    "    log_filename = os.path.join(\n",
    "        config.MODEL_OUTPUT_DIR, \n",
    "        f\"training_log_v15_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "    )\n",
    "    # Use force=True to re-configure logging in a notebook environment\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_filename),\n",
    "            logging.StreamHandler()\n",
    "        ],\n",
    "        force=True\n",
    "    )\n",
    "    logging.info(f\"--- Starting mBART v15 Training ---\")\n",
    "    logging.info(f\"Logging to: {log_filename}\")\n",
    "\n",
    "def main():\n",
    "    setup_logging()\n",
    "\n",
    "    # 1. Load and Prepare Data\n",
    "    logging.info(f\"Loading and processing data from: {config.DATA_PATH}\")\n",
    "    tokenized_datasets = data_utils.load_and_prep_dataset(config.DATA_PATH)\n",
    "    logging.info(f\"Dataset split: {len(tokenized_datasets['train'])} train, {len(tokenized_datasets['test'])} test\")\n",
    "\n",
    "    # 2. Load Base Model and Tokenizer\n",
    "    logging.info(f\"Loading base model and tokenizer from: {config.BASE_MODEL}\")\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(config.BASE_MODEL)\n",
    "    model = MBartForConditionalGeneration.from_pretrained(config.BASE_MODEL, use_safetensors=True)\n",
    "    \n",
    "    # 3. Tokenize Datasets\n",
    "    logging.info(\"Tokenizing datasets...\")\n",
    "    tokenized_datasets = tokenized_datasets.map(\n",
    "        partial(\n",
    "            data_utils.tokenize_function,\n",
    "            tokenizer=tokenizer,\n",
    "            max_input_len=config.MAX_INPUT_LENGTH,\n",
    "            max_summary_len=config.MAX_SUMMARY_LENGTH\n",
    "        ),\n",
    "        batched=True,\n",
    "        remove_columns=['article', 'summary', 'target_lang']\n",
    "    )\n",
    "    \n",
    "    # 4. Define Training Arguments\n",
    "    logging.info(\"Configuring training arguments...\")\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=config.MODEL_OUTPUT_DIR,\n",
    "        \n",
    "        # Hyperparameters\n",
    "        num_train_epochs=config.NUM_EPOCHS,\n",
    "        learning_rate=config.LEARNING_RATE,\n",
    "        per_device_train_batch_size=config.BATCH_SIZE,\n",
    "        per_device_eval_batch_size=config.BATCH_SIZE,\n",
    "        gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n",
    "        weight_decay=config.WEIGHT_DECAY,\n",
    "        \n",
    "        # --- LOGGING FIX ---\n",
    "        logging_dir=os.path.join(config.MODEL_OUTPUT_DIR, \"logs\"),\n",
    "        logging_strategy=\"steps\",   # Log every N steps\n",
    "        logging_steps=150,          # Log training loss every 100 steps\n",
    "        eval_strategy=\"epoch\",      # Run full evaluation every epoch\n",
    "        save_strategy=\"epoch\",\n",
    "        # ---------------------\n",
    "        \n",
    "        load_best_model_at_end=False,\n",
    "        save_total_limit=config.NUM_EPOCHS, # Save all checkpoints\n",
    "        \n",
    "        # Other settings\n",
    "        predict_with_generate=True,\n",
    "        fp16=config.DEVICE.type == 'cuda',\n",
    "        report_to=\"tensorboard\",\n",
    "        generation_max_length=config.MAX_SUMMARY_LENGTH,\n",
    "        generation_num_beams=config.EVAL_BEAMS,\n",
    "    )\n",
    "    \n",
    "    # 5. Initialize Trainer\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=partial(metrics_utils.compute_metrics, tokenizer=tokenizer),\n",
    "    )\n",
    "\n",
    "    # 6. Start Training\n",
    "    logging.info(f\"--- Training started for {config.NUM_EPOCHS} epochs ---\")\n",
    "    trainer.train()\n",
    "    logging.info(\"--- Training finished successfully ---\")\n",
    "    logging.info(f\"Checkpoints and logs are saved in: {config.MODEL_OUTPUT_DIR}\")\n",
    "    logging.info(\"Run 'evaluate.py' to find the best model and save it.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63292ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# Import custom modules\n",
    "import config\n",
    "import data_utils\n",
    "import metrics_utils\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Configures logging to file and stream.\"\"\"\n",
    "    os.makedirs(config.MODEL_OUTPUT_DIR, exist_ok=True)\n",
    "    log_filename = os.path.join(\n",
    "        config.MODEL_OUTPUT_DIR, \n",
    "        f\"training_log_v15_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "    )\n",
    "    # Use force=True to re-configure logging in a notebook environment\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_filename)\n",
    "            # REMOVED the StreamHandler(). This will stop the conflict\n",
    "            # and allow the Trainer to log to the console by default.\n",
    "        ],\n",
    "        force=True\n",
    "    )\n",
    "    logging.info(f\"--- Starting mBART v15 Training ---\")\n",
    "    logging.info(f\"Logging to: {log_filename}\")\n",
    "\n",
    "def main():\n",
    "    setup_logging()\n",
    "\n",
    "    # 1. Load and Prepare Data\n",
    "    logging.info(f\"Loading and processing data from: {config.DATA_PATH}\")\n",
    "    final_datasets = data_utils.load_and_prep_dataset(config.DATA_PATH)\n",
    "    logging.info(f\"Dataset split: {len(final_datasets['train'])} train, {len(final_datasets['test'])} test\")\n",
    "\n",
    "    # 2. Load Base Model and Tokenizer\n",
    "    logging.info(f\"Loading base model and tokenizer from: {config.BASE_MODEL}\")\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(config.BASE_MODEL)\n",
    "    # Use safetensors=True to avoid the torch.load vulnerability error\n",
    "    model = MBartForConditionalGeneration.from_pretrained(config.BASE_MODEL, use_safetensors=True)\n",
    "    \n",
    "    # 3. Tokenize Datasets\n",
    "    logging.info(\"Tokenizing datasets...\")\n",
    "    # Use functools.partial to pass static arguments to the map function\n",
    "    _tokenize_fn = partial(\n",
    "        data_utils.tokenize_function,\n",
    "        tokenizer=tokenizer,\n",
    "        max_input_len=config.MAX_INPUT_LENGTH,\n",
    "        max_summary_len=config.MAX_SUMMARY_LENGTH\n",
    "    )\n",
    "    tokenized_datasets = final_datasets.map(\n",
    "        _tokenize_fn,\n",
    "        batched=True,\n",
    "        remove_columns=['article', 'summary', 'target_lang']\n",
    "    )\n",
    "    \n",
    "    # 4. Define Training Arguments\n",
    "    logging.info(\"Configuring training arguments...\")\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=config.MODEL_OUTPUT_DIR,\n",
    "        \n",
    "        # Hyperparameters\n",
    "        num_train_epochs=config.NUM_EPOCHS,\n",
    "        learning_rate=config.LEARNING_RATE,\n",
    "        per_device_train_batch_size=config.BATCH_SIZE,\n",
    "        per_device_eval_batch_size=config.BATCH_SIZE,\n",
    "        gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n",
    "        weight_decay=config.WEIGHT_DECAY,\n",
    "        \n",
    "        # --- LOGGING FIX ---\n",
    "        logging_dir=os.path.join(config.MODEL_OUTPUT_DIR, \"logs\"),\n",
    "        logging_strategy=\"steps\",   # Log every N steps\n",
    "        logging_steps=50,           # Log training loss every 50 steps\n",
    "        log_level=\"info\",           # This will now correctly print to the notebook\n",
    "        eval_strategy=\"epoch\",      # Run full evaluation every epoch\n",
    "        save_strategy=\"epoch\",\n",
    "        # ---------------------\n",
    "        \n",
    "        load_best_model_at_end=False, # We run a separate script to evaluate\n",
    "        save_total_limit=config.NUM_EPOCHS, # Save all checkpoints\n",
    "        \n",
    "        # Other settings\n",
    "        predict_with_generate=True,\n",
    "        fp16=config.DEVICE.type == 'cuda',\n",
    "        report_to=\"tensorboard\",\n",
    "        generation_max_length=config.MAX_SUMMARY_LENGTH,\n",
    "        generation_num_beams=config.EVAL_BEAMS,\n",
    "    )\n",
    "    \n",
    "    # 5. Initialize Trainer\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "    # Use partial to pass the tokenizer to the metrics function\n",
    "    _compute_metrics_fn = partial(metrics_utils.compute_metrics, tokenizer=tokenizer)\n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=_compute_metrics_fn,\n",
    "    )\n",
    "\n",
    "    # 6. Start Training\n",
    "    logging.info(f\"--- Training started for {config.NUM_EPOCHS} epochs ---\")\n",
    "    trainer.train()\n",
    "    logging.info(\"--- Training finished successfully ---\")\n",
    "    logging.info(f\"Checkpoints and logs are saved in: {config.MODEL_OUTPUT_DIR}\")\n",
    "    logging.info(\"Run 'evaluate.py' to find the best model and save it.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9ab61d",
   "metadata": {},
   "source": [
    "Evaluation and Saving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4451d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import logging\n",
    "import config \n",
    "import logging_utils\n",
    "\n",
    "def find_best_checkpoint():\n",
    "    \"\"\"\n",
    "    Parses trainer_state.json files to find the best checkpoint\n",
    "    and copies it to the final_model directory.\n",
    "    \"\"\"\n",
    "    log_path = logging_utils.setup_logging(config.MODEL_OUTPUT_DIR, \"evaluation_log_v15\")\n",
    "    print(f\"--- Starting Evaluation & Save Script ---\")\n",
    "    print(f\"Logging all output to: {log_path}\\n\")\n",
    "    logging.info(\"--- Starting Post-Training Evaluation & Save Script ---\")\n",
    "    \n",
    "    logging.info(f\"Using '{config.METRIC_FOR_BEST_MODEL}' as the key metric.\")\n",
    "    print(f\"Using '{config.METRIC_FOR_BEST_MODEL}' as the key metric.\")\n",
    "    \n",
    "    best_metric_value = -1.0 \n",
    "    best_checkpoint_path = None\n",
    "    all_results = []\n",
    "\n",
    "    checkpoint_dirs = [\n",
    "        d for d in os.listdir(config.MODEL_OUTPUT_DIR) \n",
    "        if d.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(config.MODEL_OUTPUT_DIR, d))\n",
    "    ]\n",
    "    \n",
    "    if not checkpoint_dirs:\n",
    "        logging.error(f\"No checkpoint directories found in {config.MODEL_OUTPUT_DIR}\")\n",
    "        print(f\"Error: No checkpoint directories found in {config.MODEL_OUTPUT_DIR}\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"Found {len(checkpoint_dirs)} checkpoints to evaluate.\")\n",
    "    print(f\"Found {len(checkpoint_dirs)} checkpoints to evaluate.\")\n",
    "\n",
    "    for chkpt_dir in checkpoint_dirs:\n",
    "        state_path = os.path.join(config.MODEL_OUTPUT_DIR, chkpt_dir, \"trainer_state.json\")\n",
    "        if not os.path.exists(state_path):\n",
    "            logging.warning(f\"No trainer_state.json found in {chkpt_dir}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        with open(state_path, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "        \n",
    "        eval_log = None\n",
    "        for log in reversed(state[\"log_history\"]):\n",
    "            if config.METRIC_FOR_BEST_MODEL in log:\n",
    "                eval_log = log\n",
    "                break\n",
    "        \n",
    "        if eval_log:\n",
    "            all_results.append(eval_log)\n",
    "            metric_value = eval_log[config.METRIC_FOR_BEST_MODEL]\n",
    "            \n",
    "            if metric_value > best_metric_value:\n",
    "                best_metric_value = metric_value\n",
    "                best_checkpoint_path = os.path.join(config.MODEL_OUTPUT_DIR, chkpt_dir)\n",
    "                logging.info(f\"*** New best checkpoint: {chkpt_dir} ({config.METRIC_FOR_BEST_MODEL}: {metric_value}) ***\")\n",
    "                print(f\"*** New best checkpoint: {chkpt_dir} ({config.METRIC_FOR_BEST_MODEL}: {metric_value}) ***\")\n",
    "        else:\n",
    "            logging.warning(f\"No evaluation metrics found in {state_path}.\")\n",
    "\n",
    "    # Log summary table\n",
    "    summary_header = \"\\n\" + \"=\"*80 + \"\\n\" + \"--- FINAL EVALUATION SUMMARY ---\".center(80) + \"\\n\"\n",
    "    table_header = f\"{'Checkpoint':<20} | {'Step':<10} | {'Loss':<10} | {config.METRIC_FOR_BEST_MODEL:<12} | {'RougeL':<10}\"\n",
    "    summary_divider = \"-\" * len(table_header)\n",
    "    \n",
    "    logging.info(summary_header); print(summary_header)\n",
    "    logging.info(table_header); print(table_header)\n",
    "    logging.info(summary_divider); print(summary_divider)\n",
    "    \n",
    "    for log in sorted(all_results, key=lambda x: x['step']):\n",
    "        name = f\"checkpoint-{log['step']}\"\n",
    "        loss = log.get('eval_loss', 0.0)\n",
    "        bleurt = log.get(config.METRIC_FOR_BEST_MODEL, 0.0)\n",
    "        rougeL = log.get('eval_rougeL', 0.0)\n",
    "        row = f\"{name:<20} | {log['step']:<10} | {loss:<10.4f} | {bleurt:<12.4f} | {rougeL:<10.4f}\"\n",
    "        logging.info(row); print(row)\n",
    "    \n",
    "    logging.info(\"=\"*80 + \"\\n\"); print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    # Save the best model\n",
    "    if best_checkpoint_path:\n",
    "        logging.info(f\"Best model identified: {best_checkpoint_path}\")\n",
    "        print(f\"Best model identified: {best_checkpoint_path}\")\n",
    "        \n",
    "        if os.path.exists(config.FINAL_SAVE_PATH):\n",
    "            logging.warning(f\"Removing existing directory: {config.FINAL_SAVE_PATH}\")\n",
    "            print(f\"Removing existing directory: {config.FINAL_SAVE_PATH}\")\n",
    "            shutil.rmtree(config.FINAL_SAVE_PATH)\n",
    "            \n",
    "        logging.info(f\"Copying {best_checkpoint_path} to {config.FINAL_SAVE_PATH}...\")\n",
    "        print(f\"Copying {best_checkpoint_path} to {config.FINAL_SAVE_PATH}...\")\n",
    "        shutil.copytree(best_checkpoint_path, config.FINAL_SAVE_PATH)\n",
    "        \n",
    "        logging.info(\"--- Best model saved successfully! ---\")\n",
    "        print(\"\\n--- Best model saved successfully! ---\")\n",
    "        logging.info(f\"You can now test it by running: python test.py\")\n",
    "        print(f\"You can now test it by running: python test.py\")\n",
    "    else:\n",
    "        logging.error(\"Could not determine the best model.\")\n",
    "        print(\"Error: Could not determine the best model.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        find_best_checkpoint()\n",
    "    except (KeyboardInterrupt):\n",
    "        print(\"\\n--- Evaluation Interrupted ---\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- Evaluation FAILED --- \\nError: {e}\\nSee log file for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa3af74",
   "metadata": {},
   "source": [
    "Testing and Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "128902db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file mbart-large-50-cnn-summarizer-v15\\final_model\\config.json\n",
      "Model config MBartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"MBartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"early_stopping\": null,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": null,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": null,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"MBart50Tokenizer\",\n",
      "  \"transformers_version\": \"4.57.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250054\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Test Script ---\n",
      "Logging all output to: mbart-large-50-cnn-summarizer-v15\\testing_log_v15_2025-11-14_07-35-31.log\n",
      "\n",
      "Loading final model from: mbart-large-50-cnn-summarizer-v15\\final_model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file mbart-large-50-cnn-summarizer-v15\\final_model\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "loading configuration file mbart-large-50-cnn-summarizer-v15\\final_model\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": [\n",
      "    2\n",
      "  ],\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Could not locate the custom_generate/generate.py inside mbart-large-50-cnn-summarizer-v15\\final_model.\n",
      "loading file sentencepiece.bpe.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model v15 loaded successfully on cuda ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "         A major tech firm today unveiled its latest flagship smartphone,\n",
      "featuring a revolutionary new camera system with 'periscope zoom'\n",
      "technology. The device, which also boasts a foldable OLED display          and\n",
      "5G connectivity, aims to redefine the premium mobile market.          Analysts\n",
      "are optimistic, noting that the innovative camera could be          a key\n",
      "differentiator in a crowded field. However, concerns remain          about the\n",
      "device's high price point, which exceeds $1,500,          potentially limiting\n",
      "its mass-market appeal despite the advanced features.\n",
      "\n",
      "================================================================================\n",
      "GENERATED ENGLISH SUMMARY (v15):\n",
      "A major tech firm has unveiled its latest flagship smartphone, featuring a\n",
      "revolutionary new camera system with 'periscope zoom' technology. This device,\n",
      "equipped with a foldable OLED display and 5G connectivity, aims to redefine the\n",
      "premium mobile market. Analysts are optimistic, noting that the innovative\n",
      "camera could be a key differentiator in a crowded field. However, concerns\n",
      "remain regarding its high $1,500 price point, potentially limiting its mass-\n",
      "market appeal despite advanced features.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "            , \n",
      "''          foldable OLED\n",
      "            \n",
      "    ,          \n",
      "       ,    ,  \n",
      "$1,500             \n",
      "   \n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import textwrap\n",
    "import os\n",
    "import config \n",
    "import logging_utils\n",
    "import logging\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Loads the final v15 model and tokenizer.\"\"\"\n",
    "    global model, tokenizer\n",
    "    \n",
    "    if not os.path.exists(config.FINAL_SAVE_PATH):\n",
    "        print(f\"Error: Model path not found: {config.FINAL_SAVE_PATH}\")\n",
    "        logging.error(f\"Model path not found: {config.FINAL_SAVE_PATH}\")\n",
    "        print(\"Please run 'train.py' and 'evaluate.py' first.\")\n",
    "        return False\n",
    "\n",
    "    print(f\"Loading final model from: {config.FINAL_SAVE_PATH}...\")\n",
    "    logging.info(f\"Loading final model from: {config.FINAL_SAVE_PATH}...\")\n",
    "    try:\n",
    "        model = MBartForConditionalGeneration.from_pretrained(config.FINAL_SAVE_PATH).to(config.DEVICE)\n",
    "        tokenizer = MBart50TokenizerFast.from_pretrained(config.FINAL_SAVE_PATH)\n",
    "        model.eval()\n",
    "        print(f\"--- Model v15 loaded successfully on {config.DEVICE} ---\")\n",
    "        logging.info(f\"--- Model v15 loaded successfully on {config.DEVICE} ---\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        logging.error(f\"Error loading model: {e}\", exc_info=True)\n",
    "        return False\n",
    "\n",
    "def summarize_text(article_text: str):\n",
    "    \"\"\"Generates a high-quality summary.\"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        print(\"Model is not loaded. Please load the model first.\")\n",
    "        logging.error(\"summarize_text called before model was loaded.\")\n",
    "        return\n",
    "\n",
    "    gen_kwargs = {\n",
    "        \"num_beams\": 12,\n",
    "        \"length_penalty\": 2.0,\n",
    "        \"repetition_penalty\": 2.5,\n",
    "        \"no_repeat_ngram_size\": 3,\n",
    "        \"do_sample\": False,\n",
    "        \"early_stopping\": True,\n",
    "        \"min_length\": 30,\n",
    "        \"max_length\": 250,\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SOURCE ARTICLE:\")\n",
    "    print(textwrap.fill(article_text, width=80))\n",
    "    logging.info(f\"--- Summarizing Source Article --- \\n{article_text}\\n\")\n",
    "\n",
    "    tokenizer.src_lang = \"en_XX\"\n",
    "    inputs = tokenizer(article_text, return_tensors=\"pt\", max_length=1024, truncation=True).to(config.DEVICE)\n",
    "\n",
    "    # --- Generate English ---\n",
    "    eng_summary_ids = model.generate(\n",
    "        inputs.input_ids,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"],\n",
    "        **gen_kwargs\n",
    "    )\n",
    "    english_summary = tokenizer.decode(eng_summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATED ENGLISH SUMMARY (v15):\")\n",
    "    print(textwrap.fill(english_summary, width=80))\n",
    "    logging.info(f\"--- Generated English Summary --- \\n{english_summary}\\n\")\n",
    "\n",
    "    # --- Generate Hindi ---\n",
    "    hin_summary_ids = model.generate(\n",
    "        inputs.input_ids,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"],\n",
    "        **gen_kwargs\n",
    "    )\n",
    "    hindi_summary = tokenizer.decode(hin_summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATED HINDI SUMMARY (v15):\")\n",
    "    print(textwrap.fill(hindi_summary, width=80)) \n",
    "    logging.info(f\"--- Generated Hindi Summary --- \\n{hindi_summary}\\n\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "def main():\n",
    "    log_path = logging_utils.setup_logging(config.MODEL_OUTPUT_DIR, \"testing_log_v15\")\n",
    "    print(f\"--- Starting Test Script ---\")\n",
    "    print(f\"Logging all output to: {log_path}\\n\")\n",
    "    logging.info(\"--- Starting Test Script ---\")\n",
    "    \n",
    "    if load_model():\n",
    "        article_to_test = \"\"\"\n",
    "        A major tech firm today unveiled its latest flagship smartphone, \n",
    "        featuring a revolutionary new camera system with 'periscope zoom' \n",
    "        technology. The device, which also boasts a foldable OLED display \n",
    "        and 5G connectivity, aims to redefine the premium mobile market. \n",
    "        Analysts are optimistic, noting that the innovative camera could be \n",
    "        a key differentiator in a crowded field. However, concerns remain \n",
    "        about the device's high price point, which exceeds $1,500, \n",
    "        potentially limiting its mass-market appeal despite the advanced features.\n",
    "        \"\"\"\n",
    "        summarize_text(article_to_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except (KeyboardInterrupt):\n",
    "        print(\"\\n--- Test Interrupted ---\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- Test FAILED --- \\nError: {e}\\nSee log file for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c43bc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file mbart-large-50-cnn-summarizer-v15\\final_model\\config.json\n",
      "Model config MBartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"MBartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"early_stopping\": null,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": null,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": null,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"MBart50Tokenizer\",\n",
      "  \"transformers_version\": \"4.57.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250054\n",
      "}\n",
      "\n",
      "loading weights file mbart-large-50-cnn-summarizer-v15\\final_model\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Test Script ---\n",
      "Loading final model from: mbart-large-50-cnn-summarizer-v15\\final_model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file mbart-large-50-cnn-summarizer-v15\\final_model\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": [\n",
      "    2\n",
      "  ],\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Could not locate the custom_generate/generate.py inside mbart-large-50-cnn-summarizer-v15\\final_model.\n",
      "loading file sentencepiece.bpe.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model v15 loaded successfully on cuda ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "         A major tech firm today unveiled its latest flagship smartphone,\n",
      "featuring a revolutionary new camera system with 'periscope zoom'\n",
      "technology. The device, which also boasts a foldable OLED display          and\n",
      "5G connectivity, aims to redefine the premium mobile market.          Analysts\n",
      "are optimistic, noting that the innovative camera could be          a key\n",
      "differentiator in a crowded field. However, concerns remain          about the\n",
      "device's high price point, which exceeds $1,500,          potentially limiting\n",
      "its mass-market appeal despite the advanced features.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "            , \n",
      "''          foldable OLED\n",
      "            \n",
      "    ,          \n",
      "       ,    ,  \n",
      "$1,500             \n",
      "   \n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import textwrap\n",
    "import os\n",
    "import config \n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Loads the final v15 model and tokenizer.\"\"\"\n",
    "    global model, tokenizer\n",
    "    \n",
    "    if not os.path.exists(config.FINAL_SAVE_PATH):\n",
    "        print(f\"Error: Model path not found: {config.FINAL_SAVE_PATH}\")\n",
    "        print(\"Please run 'train.py' and 'evaluate.py' first.\")\n",
    "        return False\n",
    "\n",
    "    print(f\"Loading final model from: {config.FINAL_SAVE_PATH}...\")\n",
    "    try:\n",
    "        model = MBartForConditionalGeneration.from_pretrained(config.FINAL_SAVE_PATH).to(config.DEVICE)\n",
    "        tokenizer = MBart50TokenizerFast.from_pretrained(config.FINAL_SAVE_PATH)\n",
    "        model.eval()\n",
    "        print(f\"--- Model v15 loaded successfully on {config.DEVICE} ---\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return False\n",
    "\n",
    "def summarize_text(article_text: str):\n",
    "    \"\"\"Generates a high-quality Hindi summary.\"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        print(\"Model is not loaded. Please load the model first.\")\n",
    "        return\n",
    "\n",
    "    # Generation parameters are important for quality\n",
    "    gen_kwargs = {\n",
    "        \"num_beams\": 12,\n",
    "        \"length_penalty\": 2.0,\n",
    "        \"repetition_penalty\": 2.5,\n",
    "        \"no_repeat_ngram_size\": 3,\n",
    "        \"do_sample\": False,\n",
    "        \"early_stopping\": True,\n",
    "        \"min_length\": 30,\n",
    "        \"max_length\": 250,\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SOURCE ARTICLE:\")\n",
    "    print(textwrap.fill(article_text, width=80))\n",
    "\n",
    "    tokenizer.src_lang = \"en_XX\"\n",
    "    inputs = tokenizer(article_text, return_tensors=\"pt\", max_length=1024, truncation=True).to(config.DEVICE)\n",
    "\n",
    "    # --- Generate Hindi Summary ---\n",
    "    hin_summary_ids = model.generate(\n",
    "        inputs.input_ids,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"],\n",
    "        **gen_kwargs\n",
    "    )\n",
    "    hindi_summary = tokenizer.decode(hin_summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATED HINDI SUMMARY (v15):\")\n",
    "    print(textwrap.fill(hindi_summary, width=80)) \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "def main():\n",
    "    print(f\"--- Starting Test Script ---\")\n",
    "    \n",
    "    if load_model():\n",
    "        article_to_test = \"\"\"\n",
    "        A major tech firm today unveiled its latest flagship smartphone, \n",
    "        featuring a revolutionary new camera system with 'periscope zoom' \n",
    "        technology. The device, which also boasts a foldable OLED display \n",
    "        and 5G connectivity, aims to redefine the premium mobile market. \n",
    "        Analysts are optimistic, noting that the innovative camera could be \n",
    "        a key differentiator in a crowded field. However, concerns remain \n",
    "        about the device's high price point, which exceeds $1,500, \n",
    "        potentially limiting its mass-market appeal despite the advanced features.\n",
    "        \"\"\"\n",
    "        summarize_text(article_to_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n--- Test Interrupted by user ---\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- Test FAILED with an unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78c13929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file mbart-large-50-cnn-summarizer-v15\\final_model\\config.json\n",
      "Model config MBartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"MBartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"early_stopping\": null,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": null,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": null,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"MBart50Tokenizer\",\n",
      "  \"transformers_version\": \"4.57.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250054\n",
      "}\n",
      "\n",
      "loading weights file mbart-large-50-cnn-summarizer-v15\\final_model\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading final model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file mbart-large-50-cnn-summarizer-v15\\final_model\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": [\n",
      "    2\n",
      "  ],\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Could not locate the custom_generate/generate.py inside mbart-large-50-cnn-summarizer-v15\\final_model.\n",
      "loading file sentencepiece.bpe.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model v15 Loaded ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "         A major tech firm today unveiled its latest flagship smartphone,\n",
      "featuring a revolutionary new camera system with 'periscope zoom'\n",
      "technology. The device, which also boasts a foldable OLED display          and\n",
      "5G connectivity, aims to redefine the premium mobile market.          Analysts\n",
      "are optimistic, noting that the innovative camera could be          a key\n",
      "differentiator in a crowded field. However, concerns remain          about the\n",
      "device's high price point, which exceeds $1,500,          potentially limiting\n",
      "its mass-market appeal despite the advanced features.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "            , \n",
      "''          foldable OLED\n",
      "            \n",
      "    ,          \n",
      "       ,    ,  \n",
      "$1,500             \n",
      "   \n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import textwrap\n",
    "import os\n",
    "import config \n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Loads the final v15 model and tokenizer.\"\"\"\n",
    "    global model, tokenizer\n",
    "    \n",
    "    if not os.path.exists(config.FINAL_SAVE_PATH):\n",
    "        print(f\"Error: Model path not found: {config.FINAL_SAVE_PATH}\")\n",
    "        print(\"Please run 'train.py' and 'evaluate.py' first.\")\n",
    "        return False\n",
    "\n",
    "    # Minimal loading message\n",
    "    print(\"Loading final model...\")\n",
    "    try:\n",
    "        model = MBartForConditionalGeneration.from_pretrained(config.FINAL_SAVE_PATH).to(config.DEVICE)\n",
    "        tokenizer = MBart50TokenizerFast.from_pretrained(config.FINAL_SAVE_PATH)\n",
    "        model.eval()\n",
    "        print(\"--- Model v15 Loaded ---\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return False\n",
    "\n",
    "def summarize_text(article_text: str):\n",
    "    \"\"\"Generates a high-quality Hindi summary.\"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        print(\"Model is not loaded. Please load the model first.\")\n",
    "        return\n",
    "\n",
    "    # Generation parameters are important for quality\n",
    "    gen_kwargs = {\n",
    "        \"num_beams\": 12,\n",
    "        \"length_penalty\": 2.0,\n",
    "        \"repetition_penalty\": 2.5,\n",
    "        \"no_repeat_ngram_size\": 3,\n",
    "        \"do_sample\": False,\n",
    "        \"early_stopping\": True,\n",
    "        \"min_length\": 30,\n",
    "        \"max_length\": 250,\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SOURCE ARTICLE:\")\n",
    "    print(textwrap.fill(article_text, width=80))\n",
    "\n",
    "    tokenizer.src_lang = \"en_XX\"\n",
    "    inputs = tokenizer(article_text, return_tensors=\"pt\", max_length=1024, truncation=True).to(config.DEVICE)\n",
    "\n",
    "    # --- Generate Hindi Summary ---\n",
    "    hin_summary_ids = model.generate(\n",
    "        inputs.input_ids,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"],\n",
    "        **gen_kwargs\n",
    "    )\n",
    "    hindi_summary = tokenizer.decode(hin_summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATED HINDI SUMMARY (v15):\")\n",
    "    print(textwrap.fill(hindi_summary, width=80)) \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "def main():\n",
    "    if load_model():\n",
    "        article_to_test = \"\"\"\n",
    "        A major tech firm today unveiled its latest flagship smartphone, \n",
    "        featuring a revolutionary new camera system with 'periscope zoom' \n",
    "        technology. The device, which also boasts a foldable OLED display \n",
    "        and 5G connectivity, aims to redefine the premium mobile market. \n",
    "        Analysts are optimistic, noting that the innovative camera could be \n",
    "        a key differentiator in a crowded field. However, concerns remain \n",
    "        about the device's high price point, which exceeds $1,500, \n",
    "        potentially limiting its mass-market appeal despite the advanced features.\n",
    "        \"\"\"\n",
    "        summarize_text(article_to_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n--- Test Interrupted by user ---\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- Test FAILED with an unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71156ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import textwrap\n",
    "import os\n",
    "import config\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "gen_kwargs = {}\n",
    "\n",
    "def load_model_and_settings():\n",
    "    \"\"\"Loads the v15 model, tokenizer, and high-quality generation parameters.\"\"\"\n",
    "    global model, tokenizer, gen_kwargs\n",
    "    \n",
    "    if model is not None:\n",
    "        print(\"Model is already loaded.\")\n",
    "        return True\n",
    "\n",
    "    if not os.path.exists(config.FINAL_SAVE_PATH):\n",
    "        print(f\"Error: Model path not found: {config.FINAL_SAVE_PATH}\")\n",
    "        print(\"Please run 'train.py' and 'evaluate.py' first.\")\n",
    "        return False\n",
    "\n",
    "    print(\"Loading final model...\")\n",
    "    try:\n",
    "        model = MBartForConditionalGeneration.from_pretrained(config.FINAL_SAVE_PATH).to(config.DEVICE)\n",
    "        tokenizer = MBart50TokenizerFast.from_pretrained(config.FINAL_SAVE_PATH)\n",
    "        model.eval()\n",
    "        \n",
    "        gen_kwargs = {\n",
    "            \"num_beams\": 12,\n",
    "            \"length_penalty\": 2.0,\n",
    "            \"repetition_penalty\": 2.5,\n",
    "            \"no_repeat_ngram_size\": 3,\n",
    "            \"do_sample\": False,\n",
    "            \"early_stopping\": True,\n",
    "            \"min_length\": 30,\n",
    "            \"max_length\": 250,\n",
    "        }\n",
    "        \n",
    "        print(f\"--- Model v15 Loaded on {config.DEVICE} ---\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return False\n",
    "\n",
    "def summarize_hindi(article_text: str):\n",
    "    \"\"\"Generates a high-quality Hindi summary for the given text.\"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        print(\"Model is not loaded. Please run the setup cell (Cell 1) first.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SOURCE ARTICLE:\")\n",
    "    print(textwrap.fill(article_text, width=80))\n",
    "\n",
    "    tokenizer.src_lang = \"en_XX\"\n",
    "    inputs = tokenizer(article_text, return_tensors=\"pt\", max_length=1024, truncation=True).to(config.DEVICE)\n",
    "\n",
    "    # Generate Hindi Summary\n",
    "    hin_summary_ids = model.generate(\n",
    "        inputs.input_ids,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"],\n",
    "        **gen_kwargs  \n",
    "    )\n",
    "    hindi_summary = tokenizer.decode(hin_summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATED HINDI SUMMARY (v15):\")\n",
    "    print(textwrap.fill(hindi_summary, width=80)) \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "load_model_and_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "705c9aee",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'summarizer_tool'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msummarizer_tool\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m summarizer_tool\u001b[38;5;241m.\u001b[39mload_model_and_settings():\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'summarizer_tool'"
     ]
    }
   ],
   "source": [
    "import summarizer_tool\n",
    "\n",
    "def main():\n",
    "    if summarizer_tool.load_model_and_settings():\n",
    "        \n",
    "        articles_to_test = [\n",
    "            # Article 1: Tech\n",
    "            \"\"\"\n",
    "            A major tech firm today unveiled its latest flagship smartphone, \n",
    "            featuring a revolutionary new camera system with 'periscope zoom' \n",
    "            technology. The device, which also boasts a foldable OLED display \n",
    "            and 5G connectivity, aims to redefine the premium mobile market. \n",
    "            Analysts are optimistic, noting that the innovative camera could be \n",
    "            a key differentiator in a crowded field. However, concerns remain \n",
    "            about the device's high price point, which exceeds $1,500, \n",
    "            potentially limiting its mass-market appeal despite the advanced features.\n",
    "            \"\"\",\n",
    "            \n",
    "            # Article 2: Finance\n",
    "            \"\"\"\n",
    "            Global stock markets experienced a volatile week as central banks \n",
    "            around the world signaled a more aggressive stance on combating inflation. \n",
    "            The US Federal Reserve hinted at larger-than-expected interest rate hikes, \n",
    "            causing a sell-off in technology stocks and growth-oriented sectors. \n",
    "            Meanwhile, the European Central Bank is facing pressure to act as energy \n",
    "            prices continue to soar across the continent, impacting both consumer \n",
    "            spending and industrial production.\n",
    "            \"\"\",\n",
    "            \n",
    "            # Article 3: Sports\n",
    "            \"\"\"\n",
    "            India secured a decisive victory over Australia in the final match of the T20 \n",
    "            series, winning by a margin of 35 runs in Bengaluru. Batting first, India posted \n",
    "            a competitive total of 198 for 4, thanks to a powerful half-century from captain \n",
    "            Suryakumar Yadav, who scored 78 off just 45 balls. In response, Australia's \n",
    "            chase faltered early as they lost key wickets to India's fast bowlers.\n",
    "            \"\"\",\n",
    "            \n",
    "            # Article 4: Science\n",
    "            \"\"\"\n",
    "            NASA's Artemis program achieved a major milestone this week as the Orion \n",
    "            spacecraft successfully completed its uncrewed flyby of the Moon and is now on \n",
    "            its return trajectory to Earth. The mission, Artemis I, is a critical test of \n",
    "            the agency's deep space exploration systems. The spacecraft's heat shield will \n",
    "            face its most extreme test during re-entry, when it will endure temperatures \n",
    "            of nearly 5,000 degrees Fahrenheit.\n",
    "            \"\"\",\n",
    "\n",
    "            # Article 5: Environment\n",
    "            \"\"\"\n",
    "            A new international report has found that the rate of deforestation in the \n",
    "            Amazon rainforest accelerated by nearly 20% in the last year, reaching its \n",
    "            highest level in over a decade. The report attributes the surge to increased \n",
    "            illegal logging, agricultural expansion, and mining activities.\n",
    "            \"\"\",\n",
    "\n",
    "            # Article 6: Health\n",
    "            \"\"\"\n",
    "            Researchers have published a landmark study detailing a new gene-editing \n",
    "            technique that shows promise in correcting genetic defects responsible for \n",
    "            certain inherited diseases. The method, using a modified CRISPR-Cas9 system, \n",
    "            demonstrated significantly higher precision and lower off-target mutations \n",
    "            in lab experiments compared to existing technologies.\n",
    "            \"\"\",\n",
    "\n",
    "            # Article 7: Politics\n",
    "            \"\"\"\n",
    "            Diplomats from several nations met in Geneva to resume peace talks aimed at \n",
    "            resolving a long-standing regional conflict. The negotiations, which had been \n",
    "            stalled for months, were restarted following a recent de-escalation of \n",
    "            hostilities. The primary goal is to establish a lasting ceasefire.\n",
    "            \"\"\",\n",
    "\n",
    "            # Article 8: Entertainment\n",
    "            \"\"\"\n",
    "            The highly anticipated sequel to a blockbuster science fiction film has \n",
    "            officially begun production, with the studio releasing the first on-set photo. \n",
    "            The image features the return of the original cast members alongside several new \n",
    "            additions. The movie is currently slated for a summer release next year.\n",
    "            \"\"\",\n",
    "\n",
    "            # Article 9: Automotive\n",
    "            \"\"\"\n",
    "            A legacy automaker has announced a massive $50 billion investment into its \n",
    "            electric vehicle (EV) division, signaling a dramatic acceleration of its \n",
    "            transition away from internal combustion engines. The company plans to launch 15 \n",
    "            new all-electric models over the next three years, including SUVs and a \n",
    "            pickup truck.\n",
    "            \"\"\",\n",
    "\n",
    "            # Article 10: Trade\n",
    "            \"\"\"\n",
    "            A new trade agreement between two major economic blocs was signed this week, \n",
    "            aiming to reduce tariffs and streamline regulations across dozens of industries. \n",
    "            The pact, which covers everything from agricultural goods to digital services, \n",
    "            is the culmination of nearly five years of intense negotiations.\n",
    "            \"\"\"\n",
    "        ]\n",
    "        \n",
    "        # Loop through and summarize each article\n",
    "        for i, article in enumerate(articles_to_test, 1):\n",
    "            print(f\"\\n\\n--- SUMMARIZING ARTICLE {i}/{len(articles_to_test)} ---\")\n",
    "            summarizer_tool.summarize_hindi(article)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n--- Test Interrupted ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2625c7ea",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'summarizer_tool'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msummarizer_tool\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Load the model once\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m summarizer_tool\u001b[38;5;241m.\u001b[39mload_model_and_settings():\n\u001b[0;32m      6\u001b[0m         \n\u001b[0;32m      7\u001b[0m         \u001b[38;5;66;03m# New article to test (Finance)\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'summarizer_tool'"
     ]
    }
   ],
   "source": [
    "import summarizer_tool\n",
    "\n",
    "def main():\n",
    "    # Load the model once\n",
    "    if summarizer_tool.load_model_and_settings():\n",
    "        \n",
    "        # New article to test (Finance)\n",
    "        article_to_test = \"\"\"\n",
    "        Global stock markets experienced a volatile week as central banks \n",
    "        around the world signaled a more aggressive stance on combating inflation. \n",
    "        The US Federal Reserve hinted at larger-than-expected interest rate hikes, \n",
    "        causing a sell-off in technology stocks and growth-oriented sectors. \n",
    "        Meanwhile, the European Central Bank is facing pressure to act as energy \n",
    "        prices continue to soar across the continent, impacting both consumer \n",
    "        spending and industrial production.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Generate the summary\n",
    "        summarizer_tool.summarize_hindi(article_to_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n--- Test Interrupted ---\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarizer_env3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
