{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "788f8f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# --- Main Paths ---\n",
    "BASE_MODEL = \"facebook/mbart-large-50\"\n",
    "DATA_PATH = \"../../Dataset/new_large_CNN_dataset.csv\" \n",
    "MODEL_OUTPUT_DIR = \"mbart-large-50-cnn-summarizer-v15\"\n",
    "FINAL_SAVE_PATH = os.path.join(MODEL_OUTPUT_DIR, \"final_model\")\n",
    "\n",
    "# --- Training Hyperparameters ---\n",
    "LEARNING_RATE = 2e-5\n",
    "NUM_EPOCHS = 4\n",
    "BATCH_SIZE = 1\n",
    "GRADIENT_ACCUMULATION_STEPS = 8\n",
    "WEIGHT_DECAY = 0.1 # Regularization to prevent overfitting\n",
    "\n",
    "# --- Evaluation & Generation ---\n",
    "METRIC_FOR_BEST_MODEL = \"eval_bleurt_f1\"\n",
    "MAX_INPUT_LENGTH = 1024\n",
    "MAX_SUMMARY_LENGTH = 256\n",
    "EVAL_BEAMS = 5\n",
    "\n",
    "# --- Compute ---\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5345b93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict\n",
    "\n",
    "def sanitize_text(text: str) -> str:\n",
    "    \"\"\"Basic text cleanup.\"\"\"\n",
    "    if not isinstance(text, str): \n",
    "        return \"\"\n",
    "    return text.replace('\"\"', '\"').strip()\n",
    "\n",
    "def normalize_text(text: str) -> str:\n",
    "    \"\"\"Normalize unicode characters.\"\"\"\n",
    "    if not isinstance(text, str): \n",
    "        return \"\"\n",
    "    return ' '.join(unicodedata.normalize('NFKC', text).split())\n",
    "\n",
    "def load_and_prep_dataset(data_path: str) -> DatasetDict:\n",
    "    \"\"\"Loads, cleans, formats, and splits the dataset.\"\"\"\n",
    "    \n",
    "    # Load and clean\n",
    "    df = pd.read_csv(data_path, engine='python', on_bad_lines='skip')\n",
    "    df.dropna(subset=['raw_news_article', 'english_summary', 'hindi_summary'], inplace=True)\n",
    "    \n",
    "    for col in ['raw_news_article', 'english_summary', 'hindi_summary']:\n",
    "        df[col] = df[col].apply(sanitize_text).apply(normalize_text)\n",
    "    \n",
    "    raw_dataset = Dataset.from_pandas(df)\n",
    "\n",
    "    # Format for multilingual training\n",
    "    processed_dataset = raw_dataset.map(\n",
    "        _format_dataset_mbart, \n",
    "        batched=True, \n",
    "        remove_columns=raw_dataset.column_names\n",
    "    )\n",
    "    \n",
    "    # Split\n",
    "    train_test_split = processed_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "    return DatasetDict({\n",
    "        'train': train_test_split['train'],\n",
    "        'test': train_test_split['test']\n",
    "    })\n",
    "\n",
    "def _format_dataset_mbart(batch):\n",
    "    \"\"\"Duplicates each article for its English and Hindi summary.\"\"\"\n",
    "    inputs, targets, langs = [], [], []\n",
    "    for article, eng_summary, hin_summary in zip(\n",
    "        batch['raw_news_article'], batch['english_summary'], batch['hindi_summary']\n",
    "    ):\n",
    "        if isinstance(article, str) and article:\n",
    "            inputs.append(article)\n",
    "            targets.append(eng_summary)\n",
    "            langs.append(\"en_XX\")\n",
    "            \n",
    "            inputs.append(article)\n",
    "            targets.append(hin_summary)\n",
    "            langs.append(\"hi_IN\")\n",
    "            \n",
    "    return {'article': inputs, 'summary': targets, 'target_lang': langs}\n",
    "\n",
    "def tokenize_function(examples, tokenizer, max_input_len, max_summary_len):\n",
    "    \"\"\"Tokenizes articles (inputs) and summaries (labels).\"\"\"\n",
    "    \n",
    "    tokenizer.src_lang = \"en_XX\"\n",
    "    model_inputs = tokenizer(\n",
    "        examples['article'], \n",
    "        max_length=max_input_len, \n",
    "        truncation=True\n",
    "    )\n",
    "    \n",
    "    labels_batch = []\n",
    "    for i in range(len(examples['summary'])):\n",
    "        tokenizer.tgt_lang = examples['target_lang'][i]\n",
    "        labels = tokenizer(\n",
    "            text_target=examples['summary'][i], \n",
    "            max_length=max_summary_len, \n",
    "            truncation=True\n",
    "        )\n",
    "        labels_batch.append(labels['input_ids'])\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels_batch\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ef46716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\bleurt\\score.py:160: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
      "\n",
      "INFO:tensorflow:Reading checkpoint C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\bleurt-20\\downloads\\extracted\\8db8856a80394ae84b010e83ab663d4a3ccfa244ce3d0dbe00143f73e65ff123\\BLEURT-20.\n",
      "INFO:tensorflow:Config file found, reading.\n",
      "INFO:tensorflow:Will load checkpoint BLEURT-20\n",
      "INFO:tensorflow:Loads full paths and checks that files exists.\n",
      "INFO:tensorflow:... name:BLEURT-20\n",
      "INFO:tensorflow:... bert_config_file:bert_config.json\n",
      "INFO:tensorflow:... max_seq_length:512\n",
      "INFO:tensorflow:... vocab_file:None\n",
      "INFO:tensorflow:... do_lower_case:None\n",
      "INFO:tensorflow:... sp_model:sent_piece\n",
      "INFO:tensorflow:... dynamic_seq_length:True\n",
      "INFO:tensorflow:Creating BLEURT scorer.\n",
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n",
      "INFO:tensorflow:Creating SentencePiece tokenizer.\n",
      "INFO:tensorflow:Will load model: C:\\Users\\admin\\.cache\\huggingface\\metrics\\bleurt\\bleurt-20\\downloads\\extracted\\8db8856a80394ae84b010e83ab663d4a3ccfa244ce3d0dbe00143f73e65ff123\\BLEURT-20\\sent_piece.model.\n",
      "INFO:tensorflow:SentencePiece tokenizer created.\n",
      "INFO:tensorflow:Creating Eager Mode predictor.\n",
      "INFO:tensorflow:Loading model.\n",
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:BLEURT initialized.\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Initialize metrics globally to avoid reloading\n",
    "rouge_metric = evaluate.load(\"rouge\")\n",
    "bleurt_metric = evaluate.load(\"bleurt\", \"bleurt-20\")\n",
    "\n",
    "def compute_metrics(eval_pred, tokenizer):\n",
    "    \"\"\"Decodes predictions and computes ROUGE and BLEURT scores.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    \n",
    "    predictions = np.where(predictions != -100, predictions, tokenizer.pad_token_id)\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    \n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "    # Compute ROUGE\n",
    "    rouge_result = rouge_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    \n",
    "    # Compute BLEURT\n",
    "    bleurt_result = bleurt_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    \n",
    "    result = {\n",
    "        \"rouge1\": rouge_result[\"rouge1\"],\n",
    "        \"rouge2\": rouge_result[\"rouge2\"],\n",
    "        \"rougeL\": rouge_result[\"rougeL\"],\n",
    "        \"bleurt_f1\": np.mean(bleurt_result[\"scores\"])\n",
    "    }\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5a1d51da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4aeb7cdbfa7f43e8a50946bbf66d0dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "019c85b7f5784378a9e6f7151cbe2c40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46533bdd07304e389cdfa024e7afe3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_27548\\1931657635.py:96: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='429' max='8304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 429/8304 07:38 < 2:20:50, 0.93 it/s, Epoch 0.21/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 114\u001b[0m\n\u001b[0;32m    111\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluate.py\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to find the best model and save it.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 114\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 108\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# 6. Start Training\u001b[39;00m\n\u001b[0;32m    107\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Training started for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 108\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Training finished successfully ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    110\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoints and logs are saved in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mMODEL_OUTPUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\trainer.py:2674\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2667\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2668\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2670\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2672\u001b[0m )\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2680\u001b[0m ):\n\u001b[0;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\trainer.py:4020\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   4017\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   4019\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 4020\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4022\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   4023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   4024\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4025\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   4026\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\trainer.py:4110\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   4108\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_items_in_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_items_in_batch\n\u001b[0;32m   4109\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m-> 4110\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minputs)\n\u001b[0;32m   4111\u001b[0m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   4112\u001b[0m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   4113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mpast_index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\accelerate\\utils\\operations.py:818\u001b[0m, in \u001b[0;36mconvert_outputs_to_fp32.<locals>.forward\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 818\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\accelerate\\utils\\operations.py:806\u001b[0m, in \u001b[0;36mConvertOutputsToFp32.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    805\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m--> 806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m convert_to_fp32(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_forward(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs))\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\amp\\autocast_mode.py:44\u001b[0m, in \u001b[0;36mautocast_decorator.<locals>.decorate_autocast\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdecorate_autocast\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     43\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m autocast_instance:\n\u001b[1;32m---> 44\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\models\\mbart\\modeling_mbart.py:1439\u001b[0m, in \u001b[0;36mMBartForConditionalGeneration.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1436\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m decoder_input_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m decoder_inputs_embeds \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1437\u001b[0m         decoder_input_ids \u001b[38;5;241m=\u001b[39m shift_tokens_right(labels, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id)\n\u001b[1;32m-> 1439\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1440\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1441\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1442\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1443\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1444\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1445\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1446\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1447\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1448\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1449\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1450\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1451\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1452\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1453\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1454\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1455\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1456\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1457\u001b[0m lm_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlm_head(outputs[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinal_logits_bias\n\u001b[0;32m   1459\u001b[0m masked_lm_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\models\\mbart\\modeling_mbart.py:1267\u001b[0m, in \u001b[0;36mMBartModel.forward\u001b[1;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1260\u001b[0m     encoder_outputs \u001b[38;5;241m=\u001b[39m BaseModelOutput(\n\u001b[0;32m   1261\u001b[0m         last_hidden_state\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m   1262\u001b[0m         hidden_states\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1263\u001b[0m         attentions\u001b[38;5;241m=\u001b[39mencoder_outputs[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(encoder_outputs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   1264\u001b[0m     )\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, past_key_values, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[1;32m-> 1267\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1268\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_input_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder_inputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1276\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1277\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1278\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1279\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1280\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1281\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_dict:\n\u001b[0;32m   1284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs \u001b[38;5;241m+\u001b[39m encoder_outputs\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\models\\mbart\\modeling_mbart.py:1116\u001b[0m, in \u001b[0;36mMBartDecoder.forward\u001b[1;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[0;32m   1113\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dropout_probability \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayerdrop:\n\u001b[0;32m   1114\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m-> 1116\u001b[0m layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1117\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1118\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# as a positional argument for gradient checkpointing\u001b[39;49;00m\n\u001b[0;32m   1120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1121\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1122\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcross_attn_head_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1123\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1124\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1125\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1126\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1127\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1128\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[0m, in \u001b[0;36mGradientCheckpointingLayer.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     91\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning_once(message)\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs), \u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\models\\mbart\\modeling_mbart.py:420\u001b[0m, in \u001b[0;36mMBartDecoderLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_values, output_attentions, use_cache, cache_position)\u001b[0m\n\u001b[0;32m    417\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_attn_layer_norm(hidden_states)\n\u001b[0;32m    419\u001b[0m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[1;32m--> 420\u001b[0m hidden_states, self_attn_weights \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    421\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    422\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    423\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    428\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mdropout(hidden_states, p\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining)\n\u001b[0;32m    429\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m residual \u001b[38;5;241m+\u001b[39m hidden_states\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\utils\\deprecation.py:172\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action\u001b[38;5;241m.\u001b[39mNOTIFY, Action\u001b[38;5;241m.\u001b[39mNOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[0;32m    170\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m--> 172\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\models\\mbart\\modeling_mbart.py:228\u001b[0m, in \u001b[0;36mMBartAttention.forward\u001b[1;34m(self, hidden_states, key_value_states, past_key_values, attention_mask, layer_head_mask, output_attentions, cache_position, **kwargs)\u001b[0m\n\u001b[0;32m    225\u001b[0m kv_input_shape \u001b[38;5;241m=\u001b[39m (bsz, src_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhead_dim)\n\u001b[0;32m    227\u001b[0m \u001b[38;5;66;03m# get query proj\u001b[39;00m\n\u001b[1;32m--> 228\u001b[0m query_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mq_proj\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mq_input_shape)\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    230\u001b[0m is_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m past_key_values \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1740\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1739\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1748\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1750\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1751\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1753\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1754\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# Import custom modules\n",
    "import config\n",
    "import data_utils\n",
    "import metrics_utils\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Configures logging to file and stream.\"\"\"\n",
    "    os.makedirs(config.MODEL_OUTPUT_DIR, exist_ok=True)\n",
    "    log_filename = os.path.join(\n",
    "        config.MODEL_OUTPUT_DIR, \n",
    "        f\"training_log_v15_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "    )\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_filename),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    logging.info(f\"--- Starting mBART v15 Training from Scratch ---\")\n",
    "    logging.info(f\"Logging to: {log_filename}\")\n",
    "\n",
    "def main():\n",
    "    setup_logging()\n",
    "\n",
    "    # 1. Load and Prepare Data\n",
    "    logging.info(f\"Loading and processing data from: {config.DATA_PATH}\")\n",
    "    tokenized_datasets = data_utils.load_and_prep_dataset(config.DATA_PATH)\n",
    "    logging.info(f\"Dataset split: {len(tokenized_datasets['train'])} train, {len(tokenized_datasets['test'])} test\")\n",
    "\n",
    "    # 2. Load Base Model and Tokenizer\n",
    "    logging.info(f\"Loading base model and tokenizer from: {config.BASE_MODEL}\")\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(config.BASE_MODEL)\n",
    "    model = MBartForConditionalGeneration.from_pretrained(config.BASE_MODEL, use_safetensors=True)\n",
    "    \n",
    "    # 3. Tokenize Datasets\n",
    "    logging.info(\"Tokenizing datasets...\")\n",
    "    tokenized_datasets = tokenized_datasets.map(\n",
    "        partial(\n",
    "            data_utils.tokenize_function,\n",
    "            tokenizer=tokenizer,\n",
    "            max_input_len=config.MAX_INPUT_LENGTH,\n",
    "            max_summary_len=config.MAX_SUMMARY_LENGTH\n",
    "        ),\n",
    "        batched=True,\n",
    "        remove_columns=['article', 'summary', 'target_lang']\n",
    "    )\n",
    "    \n",
    "    # 4. Define Training Arguments\n",
    "    logging.info(\"Configuring training arguments...\")\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=config.MODEL_OUTPUT_DIR,\n",
    "        \n",
    "        # Hyperparameters\n",
    "        num_train_epochs=config.NUM_EPOCHS,\n",
    "        learning_rate=config.LEARNING_RATE,\n",
    "        per_device_train_batch_size=config.BATCH_SIZE,\n",
    "        per_device_eval_batch_size=config.BATCH_SIZE,\n",
    "        gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n",
    "        weight_decay=config.WEIGHT_DECAY,\n",
    "        \n",
    "        # Logging and Saving (per epoch, as requested)\n",
    "        logging_dir=os.path.join(config.MODEL_OUTPUT_DIR, \"logs\"),\n",
    "        logging_strategy=\"epoch\",\n",
    "        eval_strategy=\"epoch\", # Corrected from evaluation_strategy\n",
    "        save_strategy=\"epoch\",\n",
    "        \n",
    "        # We will run a separate script to find the best model\n",
    "        load_best_model_at_end=False,\n",
    "        save_total_limit=config.NUM_EPOCHS, # Save all checkpoints\n",
    "        \n",
    "        # Other settings\n",
    "        predict_with_generate=True,\n",
    "        fp16=config.DEVICE.type == 'cuda',\n",
    "        report_to=\"tensorboard\",\n",
    "        generation_max_length=config.MAX_SUMMARY_LENGTH,\n",
    "        generation_num_beams=config.EVAL_BEAMS,\n",
    "    )\n",
    "    \n",
    "    # 5. Initialize Trainer\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=partial(metrics_utils.compute_metrics, tokenizer=tokenizer),\n",
    "    )\n",
    "\n",
    "    # 6. Start Training\n",
    "    logging.info(f\"--- Training started for {config.NUM_EPOCHS} epochs ---\")\n",
    "    trainer.train()\n",
    "    logging.info(\"--- Training finished successfully ---\")\n",
    "    logging.info(f\"Checkpoints and logs are saved in: {config.MODEL_OUTPUT_DIR}\")\n",
    "    logging.info(\"Run 'evaluate.py' to find the best model and save it.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b37aec2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76f9508a1193403c84ae7ea51cf68117",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b91884832dd42f0bfcca9165d279a28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1b02694ddfb4b05b40f57dacf3bd05c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_27548\\1931657635.py:96: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1378' max='8304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1378/8304 26:18 < 2:12:22, 0.87 it/s, Epoch 0.66/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 114\u001b[0m\n\u001b[0;32m    111\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluate.py\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to find the best model and save it.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 114\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[7], line 108\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;66;03m# 6. Start Training\u001b[39;00m\n\u001b[0;32m    107\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Training started for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 108\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Training finished successfully ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    110\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoints and logs are saved in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mMODEL_OUTPUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\trainer.py:2674\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2667\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2668\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2670\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2672\u001b[0m )\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2680\u001b[0m ):\n\u001b[0;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\trainer.py:4071\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   4068\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   4069\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 4071\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   4073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\accelerate\\accelerator.py:2730\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2728\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2729\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2730\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\_tensor.py:624\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    615\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    616\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    617\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    622\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    623\u001b[0m     )\n\u001b[1;32m--> 624\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# Import custom modules\n",
    "import config\n",
    "import data_utils\n",
    "import metrics_utils\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Configures logging to file and stream.\"\"\"\n",
    "    os.makedirs(config.MODEL_OUTPUT_DIR, exist_ok=True)\n",
    "    log_filename = os.path.join(\n",
    "        config.MODEL_OUTPUT_DIR, \n",
    "        f\"training_log_v15_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "    )\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_filename),\n",
    "            logging.StreamHandler()\n",
    "        ]\n",
    "    )\n",
    "    logging.info(f\"--- Starting mBART v15 Training from Scratch ---\")\n",
    "    logging.info(f\"Logging to: {log_filename}\")\n",
    "\n",
    "def main():\n",
    "    setup_logging()\n",
    "\n",
    "    # 1. Load and Prepare Data\n",
    "    logging.info(f\"Loading and processing data from: {config.DATA_PATH}\")\n",
    "    tokenized_datasets = data_utils.load_and_prep_dataset(config.DATA_PATH)\n",
    "    logging.info(f\"Dataset split: {len(tokenized_datasets['train'])} train, {len(tokenized_datasets['test'])} test\")\n",
    "\n",
    "    # 2. Load Base Model and Tokenizer\n",
    "    logging.info(f\"Loading base model and tokenizer from: {config.BASE_MODEL}\")\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(config.BASE_MODEL)\n",
    "    model = MBartForConditionalGeneration.from_pretrained(config.BASE_MODEL, use_safetensors=True)\n",
    "    \n",
    "    # 3. Tokenize Datasets\n",
    "    logging.info(\"Tokenizing datasets...\")\n",
    "    tokenized_datasets = tokenized_datasets.map(\n",
    "        partial(\n",
    "            data_utils.tokenize_function,\n",
    "            tokenizer=tokenizer,\n",
    "            max_input_len=config.MAX_INPUT_LENGTH,\n",
    "            max_summary_len=config.MAX_SUMMARY_LENGTH\n",
    "        ),\n",
    "        batched=True,\n",
    "        remove_columns=['article', 'summary', 'target_lang']\n",
    "    )\n",
    "    \n",
    "    # 4. Define Training Arguments\n",
    "    logging.info(\"Configuring training arguments...\")\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=config.MODEL_OUTPUT_DIR,\n",
    "        \n",
    "        # Hyperparameters\n",
    "        num_train_epochs=config.NUM_EPOCHS,\n",
    "        learning_rate=config.LEARNING_RATE,\n",
    "        per_device_train_batch_size=config.BATCH_SIZE,\n",
    "        per_device_eval_batch_size=config.BATCH_SIZE,\n",
    "        gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n",
    "        weight_decay=config.WEIGHT_DECAY,\n",
    "        \n",
    "        # Logging and Saving (per epoch, as requested)\n",
    "        logging_dir=os.path.join(config.MODEL_OUTPUT_DIR, \"logs\"),\n",
    "        logging_strategy=\"epoch\",\n",
    "        eval_strategy=\"epoch\", # Corrected from evaluation_strategy\n",
    "        save_strategy=\"epoch\",\n",
    "        \n",
    "        # We will run a separate script to find the best model\n",
    "        load_best_model_at_end=False,\n",
    "        save_total_limit=config.NUM_EPOCHS, # Save all checkpoints\n",
    "        \n",
    "        # Other settings\n",
    "        predict_with_generate=True,\n",
    "        fp16=config.DEVICE.type == 'cuda',\n",
    "        report_to=\"tensorboard\",\n",
    "        generation_max_length=config.MAX_SUMMARY_LENGTH,\n",
    "        generation_num_beams=config.EVAL_BEAMS,\n",
    "    )\n",
    "    \n",
    "    # 5. Initialize Trainer\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=partial(metrics_utils.compute_metrics, tokenizer=tokenizer),\n",
    "    )\n",
    "\n",
    "    # 6. Start Training\n",
    "    logging.info(f\"--- Training started for {config.NUM_EPOCHS} epochs ---\")\n",
    "    trainer.train()\n",
    "    logging.info(\"--- Training finished successfully ---\")\n",
    "    logging.info(f\"Checkpoints and logs are saved in: {config.MODEL_OUTPUT_DIR}\")\n",
    "    logging.info(\"Run 'evaluate.py' to find the best model and save it.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7e1b65c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 18:21:14,270 [INFO] - --- Starting mBART v15 Training ---\n",
      "2025-11-13 18:21:14,275 [INFO] - Logging to: mbart-large-50-cnn-summarizer-v15\\training_log_v15_2025-11-13_18-21-14.log\n",
      "2025-11-13 18:21:14,276 [INFO] - Loading and processing data from: ../../Dataset/new_large_CNN_dataset.csv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e96f6c3694774737bc9d2848efe3ac9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9223 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 18:21:17,640 [INFO] - Dataset split: 16601 train, 1845 test\n",
      "2025-11-13 18:21:17,642 [INFO] - Loading base model and tokenizer from: facebook/mbart-large-50\n",
      "2025-11-13 18:21:26,440 [INFO] - Tokenizing datasets...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ab0d5a9680f475dafc859a10d0c8301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/16601 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9428dc4e74ff40adba7dd00f576ee927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1845 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-13 18:22:12,913 [INFO] - Configuring training arguments...\n",
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_27548\\1042724579.py:99: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n",
      "2025-11-13 18:22:14,899 [INFO] - --- Training started for 4 epochs ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='949' max='8304' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 949/8304 17:24 < 2:15:09, 0.91 it/s, Epoch 0.46/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 117\u001b[0m\n\u001b[0;32m    114\u001b[0m     logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRun \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mevaluate.py\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to find the best model and save it.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    116\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 117\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[9], line 111\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# 6. Start Training\u001b[39;00m\n\u001b[0;32m    110\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Training started for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 111\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m--- Training finished successfully ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    113\u001b[0m logging\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoints and logs are saved in: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mMODEL_OUTPUT_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\trainer.py:2325\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2323\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2324\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2325\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2326\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\trainer.py:2674\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2667\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2668\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2669\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2670\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2671\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2672\u001b[0m )\n\u001b[0;32m   2673\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2674\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2676\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2677\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2678\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2679\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2680\u001b[0m ):\n\u001b[0;32m   2681\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2682\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\transformers\\trainer.py:4071\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m   4068\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED:\n\u001b[0;32m   4069\u001b[0m         kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscale_wrt_gas\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m-> 4071\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mbackward(loss, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   4073\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\accelerate\\accelerator.py:2730\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[1;34m(self, loss, **kwargs)\u001b[0m\n\u001b[0;32m   2728\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   2729\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 2730\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   2731\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m learning_rate \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_lomo_optimizer:\n\u001b[0;32m   2732\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlomo_backward(loss, learning_rate)\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\_tensor.py:624\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    614\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    615\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    616\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    617\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    622\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    623\u001b[0m     )\n\u001b[1;32m--> 624\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    625\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    626\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\admin\\anaconda3\\envs\\summarizer_env3\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# Import custom modules\n",
    "import config\n",
    "import data_utils\n",
    "import metrics_utils\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Configures logging to file and stream.\"\"\"\n",
    "    os.makedirs(config.MODEL_OUTPUT_DIR, exist_ok=True)\n",
    "    log_filename = os.path.join(\n",
    "        config.MODEL_OUTPUT_DIR, \n",
    "        f\"training_log_v15_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "    )\n",
    "    # Use force=True to re-configure logging in a notebook environment\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_filename),\n",
    "            logging.StreamHandler()\n",
    "        ],\n",
    "        force=True\n",
    "    )\n",
    "    logging.info(f\"--- Starting mBART v15 Training ---\")\n",
    "    logging.info(f\"Logging to: {log_filename}\")\n",
    "\n",
    "def main():\n",
    "    setup_logging()\n",
    "\n",
    "    # 1. Load and Prepare Data\n",
    "    logging.info(f\"Loading and processing data from: {config.DATA_PATH}\")\n",
    "    tokenized_datasets = data_utils.load_and_prep_dataset(config.DATA_PATH)\n",
    "    logging.info(f\"Dataset split: {len(tokenized_datasets['train'])} train, {len(tokenized_datasets['test'])} test\")\n",
    "\n",
    "    # 2. Load Base Model and Tokenizer\n",
    "    logging.info(f\"Loading base model and tokenizer from: {config.BASE_MODEL}\")\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(config.BASE_MODEL)\n",
    "    model = MBartForConditionalGeneration.from_pretrained(config.BASE_MODEL, use_safetensors=True)\n",
    "    \n",
    "    # 3. Tokenize Datasets\n",
    "    logging.info(\"Tokenizing datasets...\")\n",
    "    tokenized_datasets = tokenized_datasets.map(\n",
    "        partial(\n",
    "            data_utils.tokenize_function,\n",
    "            tokenizer=tokenizer,\n",
    "            max_input_len=config.MAX_INPUT_LENGTH,\n",
    "            max_summary_len=config.MAX_SUMMARY_LENGTH\n",
    "        ),\n",
    "        batched=True,\n",
    "        remove_columns=['article', 'summary', 'target_lang']\n",
    "    )\n",
    "    \n",
    "    # 4. Define Training Arguments\n",
    "    logging.info(\"Configuring training arguments...\")\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=config.MODEL_OUTPUT_DIR,\n",
    "        \n",
    "        # Hyperparameters\n",
    "        num_train_epochs=config.NUM_EPOCHS,\n",
    "        learning_rate=config.LEARNING_RATE,\n",
    "        per_device_train_batch_size=config.BATCH_SIZE,\n",
    "        per_device_eval_batch_size=config.BATCH_SIZE,\n",
    "        gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n",
    "        weight_decay=config.WEIGHT_DECAY,\n",
    "        \n",
    "        # --- LOGGING FIX ---\n",
    "        logging_dir=os.path.join(config.MODEL_OUTPUT_DIR, \"logs\"),\n",
    "        logging_strategy=\"steps\",   # Log every N steps\n",
    "        logging_steps=150,          # Log training loss every 100 steps\n",
    "        eval_strategy=\"epoch\",      # Run full evaluation every epoch\n",
    "        save_strategy=\"epoch\",\n",
    "        # ---------------------\n",
    "        \n",
    "        load_best_model_at_end=False,\n",
    "        save_total_limit=config.NUM_EPOCHS, # Save all checkpoints\n",
    "        \n",
    "        # Other settings\n",
    "        predict_with_generate=True,\n",
    "        fp16=config.DEVICE.type == 'cuda',\n",
    "        report_to=\"tensorboard\",\n",
    "        generation_max_length=config.MAX_SUMMARY_LENGTH,\n",
    "        generation_num_beams=config.EVAL_BEAMS,\n",
    "    )\n",
    "    \n",
    "    # 5. Initialize Trainer\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=partial(metrics_utils.compute_metrics, tokenizer=tokenizer),\n",
    "    )\n",
    "\n",
    "    # 6. Start Training\n",
    "    logging.info(f\"--- Training started for {config.NUM_EPOCHS} epochs ---\")\n",
    "    trainer.train()\n",
    "    logging.info(\"--- Training finished successfully ---\")\n",
    "    logging.info(f\"Checkpoints and logs are saved in: {config.MODEL_OUTPUT_DIR}\")\n",
    "    logging.info(\"Run 'evaluate.py' to find the best model and save it.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63292ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from functools import partial\n",
    "from datetime import datetime\n",
    "from transformers import (\n",
    "    MBartForConditionalGeneration,\n",
    "    MBart50TokenizerFast,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer\n",
    ")\n",
    "\n",
    "# Import custom modules\n",
    "import config\n",
    "import data_utils\n",
    "import metrics_utils\n",
    "\n",
    "def setup_logging():\n",
    "    \"\"\"Configures logging to file and stream.\"\"\"\n",
    "    os.makedirs(config.MODEL_OUTPUT_DIR, exist_ok=True)\n",
    "    log_filename = os.path.join(\n",
    "        config.MODEL_OUTPUT_DIR, \n",
    "        f\"training_log_v15_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.log\"\n",
    "    )\n",
    "    # Use force=True to re-configure logging in a notebook environment\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s [%(levelname)s] - %(message)s\",\n",
    "        handlers=[\n",
    "            logging.FileHandler(log_filename)\n",
    "            # REMOVED the StreamHandler(). This will stop the conflict\n",
    "            # and allow the Trainer to log to the console by default.\n",
    "        ],\n",
    "        force=True\n",
    "    )\n",
    "    logging.info(f\"--- Starting mBART v15 Training ---\")\n",
    "    logging.info(f\"Logging to: {log_filename}\")\n",
    "\n",
    "def main():\n",
    "    setup_logging()\n",
    "\n",
    "    # 1. Load and Prepare Data\n",
    "    logging.info(f\"Loading and processing data from: {config.DATA_PATH}\")\n",
    "    final_datasets = data_utils.load_and_prep_dataset(config.DATA_PATH)\n",
    "    logging.info(f\"Dataset split: {len(final_datasets['train'])} train, {len(final_datasets['test'])} test\")\n",
    "\n",
    "    # 2. Load Base Model and Tokenizer\n",
    "    logging.info(f\"Loading base model and tokenizer from: {config.BASE_MODEL}\")\n",
    "    tokenizer = MBart50TokenizerFast.from_pretrained(config.BASE_MODEL)\n",
    "    # Use safetensors=True to avoid the torch.load vulnerability error\n",
    "    model = MBartForConditionalGeneration.from_pretrained(config.BASE_MODEL, use_safetensors=True)\n",
    "    \n",
    "    # 3. Tokenize Datasets\n",
    "    logging.info(\"Tokenizing datasets...\")\n",
    "    # Use functools.partial to pass static arguments to the map function\n",
    "    _tokenize_fn = partial(\n",
    "        data_utils.tokenize_function,\n",
    "        tokenizer=tokenizer,\n",
    "        max_input_len=config.MAX_INPUT_LENGTH,\n",
    "        max_summary_len=config.MAX_SUMMARY_LENGTH\n",
    "    )\n",
    "    tokenized_datasets = final_datasets.map(\n",
    "        _tokenize_fn,\n",
    "        batched=True,\n",
    "        remove_columns=['article', 'summary', 'target_lang']\n",
    "    )\n",
    "    \n",
    "    # 4. Define Training Arguments\n",
    "    logging.info(\"Configuring training arguments...\")\n",
    "    training_args = Seq2SeqTrainingArguments(\n",
    "        output_dir=config.MODEL_OUTPUT_DIR,\n",
    "        \n",
    "        # Hyperparameters\n",
    "        num_train_epochs=config.NUM_EPOCHS,\n",
    "        learning_rate=config.LEARNING_RATE,\n",
    "        per_device_train_batch_size=config.BATCH_SIZE,\n",
    "        per_device_eval_batch_size=config.BATCH_SIZE,\n",
    "        gradient_accumulation_steps=config.GRADIENT_ACCUMULATION_STEPS,\n",
    "        weight_decay=config.WEIGHT_DECAY,\n",
    "        \n",
    "        # --- LOGGING FIX ---\n",
    "        logging_dir=os.path.join(config.MODEL_OUTPUT_DIR, \"logs\"),\n",
    "        logging_strategy=\"steps\",   # Log every N steps\n",
    "        logging_steps=50,           # Log training loss every 50 steps\n",
    "        log_level=\"info\",           # This will now correctly print to the notebook\n",
    "        eval_strategy=\"epoch\",      # Run full evaluation every epoch\n",
    "        save_strategy=\"epoch\",\n",
    "        # ---------------------\n",
    "        \n",
    "        load_best_model_at_end=False, # We run a separate script to evaluate\n",
    "        save_total_limit=config.NUM_EPOCHS, # Save all checkpoints\n",
    "        \n",
    "        # Other settings\n",
    "        predict_with_generate=True,\n",
    "        fp16=config.DEVICE.type == 'cuda',\n",
    "        report_to=\"tensorboard\",\n",
    "        generation_max_length=config.MAX_SUMMARY_LENGTH,\n",
    "        generation_num_beams=config.EVAL_BEAMS,\n",
    "    )\n",
    "    \n",
    "    # 5. Initialize Trainer\n",
    "    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "    # Use partial to pass the tokenizer to the metrics function\n",
    "    _compute_metrics_fn = partial(metrics_utils.compute_metrics, tokenizer=tokenizer)\n",
    "    \n",
    "    trainer = Seq2SeqTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_datasets[\"train\"],\n",
    "        eval_dataset=tokenized_datasets[\"test\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=_compute_metrics_fn,\n",
    "    )\n",
    "\n",
    "    # 6. Start Training\n",
    "    logging.info(f\"--- Training started for {config.NUM_EPOCHS} epochs ---\")\n",
    "    trainer.train()\n",
    "    logging.info(\"--- Training finished successfully ---\")\n",
    "    logging.info(f\"Checkpoints and logs are saved in: {config.MODEL_OUTPUT_DIR}\")\n",
    "    logging.info(\"Run 'evaluate.py' to find the best model and save it.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9ab61d",
   "metadata": {},
   "source": [
    "Evaluation and Saving the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4451d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "import logging\n",
    "import config \n",
    "import logging_utils\n",
    "\n",
    "def find_best_checkpoint():\n",
    "    \"\"\"\n",
    "    Parses trainer_state.json files to find the best checkpoint\n",
    "    and copies it to the final_model directory.\n",
    "    \"\"\"\n",
    "    log_path = logging_utils.setup_logging(config.MODEL_OUTPUT_DIR, \"evaluation_log_v15\")\n",
    "    print(f\"--- Starting Evaluation & Save Script ---\")\n",
    "    print(f\"Logging all output to: {log_path}\\n\")\n",
    "    logging.info(\"--- Starting Post-Training Evaluation & Save Script ---\")\n",
    "    \n",
    "    logging.info(f\"Using '{config.METRIC_FOR_BEST_MODEL}' as the key metric.\")\n",
    "    print(f\"Using '{config.METRIC_FOR_BEST_MODEL}' as the key metric.\")\n",
    "    \n",
    "    best_metric_value = -1.0 \n",
    "    best_checkpoint_path = None\n",
    "    all_results = []\n",
    "\n",
    "    checkpoint_dirs = [\n",
    "        d for d in os.listdir(config.MODEL_OUTPUT_DIR) \n",
    "        if d.startswith(\"checkpoint-\") and os.path.isdir(os.path.join(config.MODEL_OUTPUT_DIR, d))\n",
    "    ]\n",
    "    \n",
    "    if not checkpoint_dirs:\n",
    "        logging.error(f\"No checkpoint directories found in {config.MODEL_OUTPUT_DIR}\")\n",
    "        print(f\"Error: No checkpoint directories found in {config.MODEL_OUTPUT_DIR}\")\n",
    "        return\n",
    "\n",
    "    logging.info(f\"Found {len(checkpoint_dirs)} checkpoints to evaluate.\")\n",
    "    print(f\"Found {len(checkpoint_dirs)} checkpoints to evaluate.\")\n",
    "\n",
    "    for chkpt_dir in checkpoint_dirs:\n",
    "        state_path = os.path.join(config.MODEL_OUTPUT_DIR, chkpt_dir, \"trainer_state.json\")\n",
    "        if not os.path.exists(state_path):\n",
    "            logging.warning(f\"No trainer_state.json found in {chkpt_dir}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        with open(state_path, \"r\") as f:\n",
    "            state = json.load(f)\n",
    "        \n",
    "        eval_log = None\n",
    "        for log in reversed(state[\"log_history\"]):\n",
    "            if config.METRIC_FOR_BEST_MODEL in log:\n",
    "                eval_log = log\n",
    "                break\n",
    "        \n",
    "        if eval_log:\n",
    "            all_results.append(eval_log)\n",
    "            metric_value = eval_log[config.METRIC_FOR_BEST_MODEL]\n",
    "            \n",
    "            if metric_value > best_metric_value:\n",
    "                best_metric_value = metric_value\n",
    "                best_checkpoint_path = os.path.join(config.MODEL_OUTPUT_DIR, chkpt_dir)\n",
    "                logging.info(f\"*** New best checkpoint: {chkpt_dir} ({config.METRIC_FOR_BEST_MODEL}: {metric_value}) ***\")\n",
    "                print(f\"*** New best checkpoint: {chkpt_dir} ({config.METRIC_FOR_BEST_MODEL}: {metric_value}) ***\")\n",
    "        else:\n",
    "            logging.warning(f\"No evaluation metrics found in {state_path}.\")\n",
    "\n",
    "    # Log summary table\n",
    "    summary_header = \"\\n\" + \"=\"*80 + \"\\n\" + \"--- FINAL EVALUATION SUMMARY ---\".center(80) + \"\\n\"\n",
    "    table_header = f\"{'Checkpoint':<20} | {'Step':<10} | {'Loss':<10} | {config.METRIC_FOR_BEST_MODEL:<12} | {'RougeL':<10}\"\n",
    "    summary_divider = \"-\" * len(table_header)\n",
    "    \n",
    "    logging.info(summary_header); print(summary_header)\n",
    "    logging.info(table_header); print(table_header)\n",
    "    logging.info(summary_divider); print(summary_divider)\n",
    "    \n",
    "    for log in sorted(all_results, key=lambda x: x['step']):\n",
    "        name = f\"checkpoint-{log['step']}\"\n",
    "        loss = log.get('eval_loss', 0.0)\n",
    "        bleurt = log.get(config.METRIC_FOR_BEST_MODEL, 0.0)\n",
    "        rougeL = log.get('eval_rougeL', 0.0)\n",
    "        row = f\"{name:<20} | {log['step']:<10} | {loss:<10.4f} | {bleurt:<12.4f} | {rougeL:<10.4f}\"\n",
    "        logging.info(row); print(row)\n",
    "    \n",
    "    logging.info(\"=\"*80 + \"\\n\"); print(\"=\"*80 + \"\\n\")\n",
    "\n",
    "    # Save the best model\n",
    "    if best_checkpoint_path:\n",
    "        logging.info(f\"Best model identified: {best_checkpoint_path}\")\n",
    "        print(f\"Best model identified: {best_checkpoint_path}\")\n",
    "        \n",
    "        if os.path.exists(config.FINAL_SAVE_PATH):\n",
    "            logging.warning(f\"Removing existing directory: {config.FINAL_SAVE_PATH}\")\n",
    "            print(f\"Removing existing directory: {config.FINAL_SAVE_PATH}\")\n",
    "            shutil.rmtree(config.FINAL_SAVE_PATH)\n",
    "            \n",
    "        logging.info(f\"Copying {best_checkpoint_path} to {config.FINAL_SAVE_PATH}...\")\n",
    "        print(f\"Copying {best_checkpoint_path} to {config.FINAL_SAVE_PATH}...\")\n",
    "        shutil.copytree(best_checkpoint_path, config.FINAL_SAVE_PATH)\n",
    "        \n",
    "        logging.info(\"--- Best model saved successfully! ---\")\n",
    "        print(\"\\n--- Best model saved successfully! ---\")\n",
    "        logging.info(f\"You can now test it by running: python test.py\")\n",
    "        print(f\"You can now test it by running: python test.py\")\n",
    "    else:\n",
    "        logging.error(\"Could not determine the best model.\")\n",
    "        print(\"Error: Could not determine the best model.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        find_best_checkpoint()\n",
    "    except (KeyboardInterrupt):\n",
    "        print(\"\\n--- Evaluation Interrupted ---\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- Evaluation FAILED --- \\nError: {e}\\nSee log file for details.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa3af74",
   "metadata": {},
   "source": [
    "Testing and Inferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "128902db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file mbart-large-50-cnn-summarizer-v15\\final_model\\config.json\n",
      "Model config MBartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"MBartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"early_stopping\": null,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": null,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": null,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"MBart50Tokenizer\",\n",
      "  \"transformers_version\": \"4.57.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250054\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Test Script ---\n",
      "Logging all output to: mbart-large-50-cnn-summarizer-v15\\testing_log_v15_2025-11-14_07-35-31.log\n",
      "\n",
      "Loading final model from: mbart-large-50-cnn-summarizer-v15\\final_model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading weights file mbart-large-50-cnn-summarizer-v15\\final_model\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "loading configuration file mbart-large-50-cnn-summarizer-v15\\final_model\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": [\n",
      "    2\n",
      "  ],\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Could not locate the custom_generate/generate.py inside mbart-large-50-cnn-summarizer-v15\\final_model.\n",
      "loading file sentencepiece.bpe.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model v15 loaded successfully on cuda ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "         A major tech firm today unveiled its latest flagship smartphone,\n",
      "featuring a revolutionary new camera system with 'periscope zoom'\n",
      "technology. The device, which also boasts a foldable OLED display          and\n",
      "5G connectivity, aims to redefine the premium mobile market.          Analysts\n",
      "are optimistic, noting that the innovative camera could be          a key\n",
      "differentiator in a crowded field. However, concerns remain          about the\n",
      "device's high price point, which exceeds $1,500,          potentially limiting\n",
      "its mass-market appeal despite the advanced features.\n",
      "\n",
      "================================================================================\n",
      "GENERATED ENGLISH SUMMARY (v15):\n",
      "A major tech firm has unveiled its latest flagship smartphone, featuring a\n",
      "revolutionary new camera system with 'periscope zoom' technology. This device,\n",
      "equipped with a foldable OLED display and 5G connectivity, aims to redefine the\n",
      "premium mobile market. Analysts are optimistic, noting that the innovative\n",
      "camera could be a key differentiator in a crowded field. However, concerns\n",
      "remain regarding its high $1,500 price point, potentially limiting its mass-\n",
      "market appeal despite advanced features.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "            , \n",
      "''          foldable OLED\n",
      "            \n",
      "    ,          \n",
      "       ,    ,  \n",
      "$1,500             \n",
      "   \n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import textwrap\n",
    "import os\n",
    "import config \n",
    "import logging_utils\n",
    "import logging\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Loads the final v15 model and tokenizer.\"\"\"\n",
    "    global model, tokenizer\n",
    "    \n",
    "    if not os.path.exists(config.FINAL_SAVE_PATH):\n",
    "        print(f\"Error: Model path not found: {config.FINAL_SAVE_PATH}\")\n",
    "        logging.error(f\"Model path not found: {config.FINAL_SAVE_PATH}\")\n",
    "        print(\"Please run 'train.py' and 'evaluate.py' first.\")\n",
    "        return False\n",
    "\n",
    "    print(f\"Loading final model from: {config.FINAL_SAVE_PATH}...\")\n",
    "    logging.info(f\"Loading final model from: {config.FINAL_SAVE_PATH}...\")\n",
    "    try:\n",
    "        model = MBartForConditionalGeneration.from_pretrained(config.FINAL_SAVE_PATH).to(config.DEVICE)\n",
    "        tokenizer = MBart50TokenizerFast.from_pretrained(config.FINAL_SAVE_PATH)\n",
    "        model.eval()\n",
    "        print(f\"--- Model v15 loaded successfully on {config.DEVICE} ---\")\n",
    "        logging.info(f\"--- Model v15 loaded successfully on {config.DEVICE} ---\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        logging.error(f\"Error loading model: {e}\", exc_info=True)\n",
    "        return False\n",
    "\n",
    "def summarize_text(article_text: str):\n",
    "    \"\"\"Generates a high-quality summary.\"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        print(\"Model is not loaded. Please load the model first.\")\n",
    "        logging.error(\"summarize_text called before model was loaded.\")\n",
    "        return\n",
    "\n",
    "    gen_kwargs = {\n",
    "        \"num_beams\": 12,\n",
    "        \"length_penalty\": 2.0,\n",
    "        \"repetition_penalty\": 2.5,\n",
    "        \"no_repeat_ngram_size\": 3,\n",
    "        \"do_sample\": False,\n",
    "        \"early_stopping\": True,\n",
    "        \"min_length\": 30,\n",
    "        \"max_length\": 250,\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SOURCE ARTICLE:\")\n",
    "    print(textwrap.fill(article_text, width=80))\n",
    "    logging.info(f\"--- Summarizing Source Article --- \\n{article_text}\\n\")\n",
    "\n",
    "    tokenizer.src_lang = \"en_XX\"\n",
    "    inputs = tokenizer(article_text, return_tensors=\"pt\", max_length=1024, truncation=True).to(config.DEVICE)\n",
    "\n",
    "    # --- Generate English ---\n",
    "    eng_summary_ids = model.generate(\n",
    "        inputs.input_ids,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"en_XX\"],\n",
    "        **gen_kwargs\n",
    "    )\n",
    "    english_summary = tokenizer.decode(eng_summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATED ENGLISH SUMMARY (v15):\")\n",
    "    print(textwrap.fill(english_summary, width=80))\n",
    "    logging.info(f\"--- Generated English Summary --- \\n{english_summary}\\n\")\n",
    "\n",
    "    # --- Generate Hindi ---\n",
    "    hin_summary_ids = model.generate(\n",
    "        inputs.input_ids,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"],\n",
    "        **gen_kwargs\n",
    "    )\n",
    "    hindi_summary = tokenizer.decode(hin_summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATED HINDI SUMMARY (v15):\")\n",
    "    print(textwrap.fill(hindi_summary, width=80)) \n",
    "    logging.info(f\"--- Generated Hindi Summary --- \\n{hindi_summary}\\n\")\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "def main():\n",
    "    log_path = logging_utils.setup_logging(config.MODEL_OUTPUT_DIR, \"testing_log_v15\")\n",
    "    print(f\"--- Starting Test Script ---\")\n",
    "    print(f\"Logging all output to: {log_path}\\n\")\n",
    "    logging.info(\"--- Starting Test Script ---\")\n",
    "    \n",
    "    if load_model():\n",
    "        article_to_test = \"\"\"\n",
    "        A major tech firm today unveiled its latest flagship smartphone, \n",
    "        featuring a revolutionary new camera system with 'periscope zoom' \n",
    "        technology. The device, which also boasts a foldable OLED display \n",
    "        and 5G connectivity, aims to redefine the premium mobile market. \n",
    "        Analysts are optimistic, noting that the innovative camera could be \n",
    "        a key differentiator in a crowded field. However, concerns remain \n",
    "        about the device's high price point, which exceeds $1,500, \n",
    "        potentially limiting its mass-market appeal despite the advanced features.\n",
    "        \"\"\"\n",
    "        summarize_text(article_to_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except (KeyboardInterrupt):\n",
    "        print(\"\\n--- Test Interrupted ---\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- Test FAILED --- \\nError: {e}\\nSee log file for details.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c43bc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file mbart-large-50-cnn-summarizer-v15\\final_model\\config.json\n",
      "Model config MBartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"MBartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"early_stopping\": null,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": null,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": null,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"MBart50Tokenizer\",\n",
      "  \"transformers_version\": \"4.57.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250054\n",
      "}\n",
      "\n",
      "loading weights file mbart-large-50-cnn-summarizer-v15\\final_model\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Test Script ---\n",
      "Loading final model from: mbart-large-50-cnn-summarizer-v15\\final_model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file mbart-large-50-cnn-summarizer-v15\\final_model\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": [\n",
      "    2\n",
      "  ],\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Could not locate the custom_generate/generate.py inside mbart-large-50-cnn-summarizer-v15\\final_model.\n",
      "loading file sentencepiece.bpe.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model v15 loaded successfully on cuda ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "         A major tech firm today unveiled its latest flagship smartphone,\n",
      "featuring a revolutionary new camera system with 'periscope zoom'\n",
      "technology. The device, which also boasts a foldable OLED display          and\n",
      "5G connectivity, aims to redefine the premium mobile market.          Analysts\n",
      "are optimistic, noting that the innovative camera could be          a key\n",
      "differentiator in a crowded field. However, concerns remain          about the\n",
      "device's high price point, which exceeds $1,500,          potentially limiting\n",
      "its mass-market appeal despite the advanced features.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "            , \n",
      "''          foldable OLED\n",
      "            \n",
      "    ,          \n",
      "       ,    ,  \n",
      "$1,500             \n",
      "   \n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import textwrap\n",
    "import os\n",
    "import config \n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Loads the final v15 model and tokenizer.\"\"\"\n",
    "    global model, tokenizer\n",
    "    \n",
    "    if not os.path.exists(config.FINAL_SAVE_PATH):\n",
    "        print(f\"Error: Model path not found: {config.FINAL_SAVE_PATH}\")\n",
    "        print(\"Please run 'train.py' and 'evaluate.py' first.\")\n",
    "        return False\n",
    "\n",
    "    print(f\"Loading final model from: {config.FINAL_SAVE_PATH}...\")\n",
    "    try:\n",
    "        model = MBartForConditionalGeneration.from_pretrained(config.FINAL_SAVE_PATH).to(config.DEVICE)\n",
    "        tokenizer = MBart50TokenizerFast.from_pretrained(config.FINAL_SAVE_PATH)\n",
    "        model.eval()\n",
    "        print(f\"--- Model v15 loaded successfully on {config.DEVICE} ---\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return False\n",
    "\n",
    "def summarize_text(article_text: str):\n",
    "    \"\"\"Generates a high-quality Hindi summary.\"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        print(\"Model is not loaded. Please load the model first.\")\n",
    "        return\n",
    "\n",
    "    # Generation parameters are important for quality\n",
    "    gen_kwargs = {\n",
    "        \"num_beams\": 12,\n",
    "        \"length_penalty\": 2.0,\n",
    "        \"repetition_penalty\": 2.5,\n",
    "        \"no_repeat_ngram_size\": 3,\n",
    "        \"do_sample\": False,\n",
    "        \"early_stopping\": True,\n",
    "        \"min_length\": 30,\n",
    "        \"max_length\": 250,\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SOURCE ARTICLE:\")\n",
    "    print(textwrap.fill(article_text, width=80))\n",
    "\n",
    "    tokenizer.src_lang = \"en_XX\"\n",
    "    inputs = tokenizer(article_text, return_tensors=\"pt\", max_length=1024, truncation=True).to(config.DEVICE)\n",
    "\n",
    "    # --- Generate Hindi Summary ---\n",
    "    hin_summary_ids = model.generate(\n",
    "        inputs.input_ids,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"],\n",
    "        **gen_kwargs\n",
    "    )\n",
    "    hindi_summary = tokenizer.decode(hin_summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATED HINDI SUMMARY (v15):\")\n",
    "    print(textwrap.fill(hindi_summary, width=80)) \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "def main():\n",
    "    print(f\"--- Starting Test Script ---\")\n",
    "    \n",
    "    if load_model():\n",
    "        article_to_test = \"\"\"\n",
    "        A major tech firm today unveiled its latest flagship smartphone, \n",
    "        featuring a revolutionary new camera system with 'periscope zoom' \n",
    "        technology. The device, which also boasts a foldable OLED display \n",
    "        and 5G connectivity, aims to redefine the premium mobile market. \n",
    "        Analysts are optimistic, noting that the innovative camera could be \n",
    "        a key differentiator in a crowded field. However, concerns remain \n",
    "        about the device's high price point, which exceeds $1,500, \n",
    "        potentially limiting its mass-market appeal despite the advanced features.\n",
    "        \"\"\"\n",
    "        summarize_text(article_to_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n--- Test Interrupted by user ---\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- Test FAILED with an unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "78c13929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file mbart-large-50-cnn-summarizer-v15\\final_model\\config.json\n",
      "Model config MBartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"MBartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"early_stopping\": null,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": null,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": null,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"MBart50Tokenizer\",\n",
      "  \"transformers_version\": \"4.57.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250054\n",
      "}\n",
      "\n",
      "loading weights file mbart-large-50-cnn-summarizer-v15\\final_model\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading final model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file mbart-large-50-cnn-summarizer-v15\\final_model\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": [\n",
      "    2\n",
      "  ],\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Could not locate the custom_generate/generate.py inside mbart-large-50-cnn-summarizer-v15\\final_model.\n",
      "loading file sentencepiece.bpe.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model v15 Loaded ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "         A major tech firm today unveiled its latest flagship smartphone,\n",
      "featuring a revolutionary new camera system with 'periscope zoom'\n",
      "technology. The device, which also boasts a foldable OLED display          and\n",
      "5G connectivity, aims to redefine the premium mobile market.          Analysts\n",
      "are optimistic, noting that the innovative camera could be          a key\n",
      "differentiator in a crowded field. However, concerns remain          about the\n",
      "device's high price point, which exceeds $1,500,          potentially limiting\n",
      "its mass-market appeal despite the advanced features.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "            , \n",
      "''          foldable OLED\n",
      "            \n",
      "    ,          \n",
      "       ,    ,  \n",
      "$1,500             \n",
      "   \n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import textwrap\n",
    "import os\n",
    "import config \n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Loads the final v15 model and tokenizer.\"\"\"\n",
    "    global model, tokenizer\n",
    "    \n",
    "    if not os.path.exists(config.FINAL_SAVE_PATH):\n",
    "        print(f\"Error: Model path not found: {config.FINAL_SAVE_PATH}\")\n",
    "        print(\"Please run 'train.py' and 'evaluate.py' first.\")\n",
    "        return False\n",
    "\n",
    "    # Minimal loading message\n",
    "    print(\"Loading final model...\")\n",
    "    try:\n",
    "        model = MBartForConditionalGeneration.from_pretrained(config.FINAL_SAVE_PATH).to(config.DEVICE)\n",
    "        tokenizer = MBart50TokenizerFast.from_pretrained(config.FINAL_SAVE_PATH)\n",
    "        model.eval()\n",
    "        print(\"--- Model v15 Loaded ---\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return False\n",
    "\n",
    "def summarize_text(article_text: str):\n",
    "    \"\"\"Generates a high-quality Hindi summary.\"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        print(\"Model is not loaded. Please load the model first.\")\n",
    "        return\n",
    "\n",
    "    # Generation parameters are important for quality\n",
    "    gen_kwargs = {\n",
    "        \"num_beams\": 12,\n",
    "        \"length_penalty\": 2.0,\n",
    "        \"repetition_penalty\": 2.5,\n",
    "        \"no_repeat_ngram_size\": 3,\n",
    "        \"do_sample\": False,\n",
    "        \"early_stopping\": True,\n",
    "        \"min_length\": 30,\n",
    "        \"max_length\": 250,\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SOURCE ARTICLE:\")\n",
    "    print(textwrap.fill(article_text, width=80))\n",
    "\n",
    "    tokenizer.src_lang = \"en_XX\"\n",
    "    inputs = tokenizer(article_text, return_tensors=\"pt\", max_length=1024, truncation=True).to(config.DEVICE)\n",
    "\n",
    "    # --- Generate Hindi Summary ---\n",
    "    hin_summary_ids = model.generate(\n",
    "        inputs.input_ids,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"],\n",
    "        **gen_kwargs\n",
    "    )\n",
    "    hindi_summary = tokenizer.decode(hin_summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATED HINDI SUMMARY (v15):\")\n",
    "    print(textwrap.fill(hindi_summary, width=80)) \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "def main():\n",
    "    if load_model():\n",
    "        article_to_test = \"\"\"\n",
    "        A major tech firm today unveiled its latest flagship smartphone, \n",
    "        featuring a revolutionary new camera system with 'periscope zoom' \n",
    "        technology. The device, which also boasts a foldable OLED display \n",
    "        and 5G connectivity, aims to redefine the premium mobile market. \n",
    "        Analysts are optimistic, noting that the innovative camera could be \n",
    "        a key differentiator in a crowded field. However, concerns remain \n",
    "        about the device's high price point, which exceeds $1,500, \n",
    "        potentially limiting its mass-market appeal despite the advanced features.\n",
    "        \"\"\"\n",
    "        summarize_text(article_to_test)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n--- Test Interrupted by user ---\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\n--- Test FAILED with an unexpected error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e71156ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file mbart-large-50-cnn-summarizer-v15\\final_model\\config.json\n",
      "Model config MBartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"MBartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"early_stopping\": null,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": null,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": null,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"MBart50Tokenizer\",\n",
      "  \"transformers_version\": \"4.57.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250054\n",
      "}\n",
      "\n",
      "loading weights file mbart-large-50-cnn-summarizer-v15\\final_model\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading final model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file mbart-large-50-cnn-summarizer-v15\\final_model\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": [\n",
      "    2\n",
      "  ],\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Could not locate the custom_generate/generate.py inside mbart-large-50-cnn-summarizer-v15\\final_model.\n",
      "loading file sentencepiece.bpe.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Model v15 Loaded on cuda ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import textwrap\n",
    "import os\n",
    "import config\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "gen_kwargs = {}\n",
    "\n",
    "def load_model_and_settings():\n",
    "    \"\"\"Loads the v15 model, tokenizer, and high-quality generation parameters.\"\"\"\n",
    "    global model, tokenizer, gen_kwargs\n",
    "    \n",
    "    if model is not None:\n",
    "        print(\"Model is already loaded.\")\n",
    "        return True\n",
    "\n",
    "    if not os.path.exists(config.FINAL_SAVE_PATH):\n",
    "        print(f\"Error: Model path not found: {config.FINAL_SAVE_PATH}\")\n",
    "        print(\"Please run 'train.py' and 'evaluate.py' first.\")\n",
    "        return False\n",
    "\n",
    "    print(\"Loading final model...\")\n",
    "    try:\n",
    "        model = MBartForConditionalGeneration.from_pretrained(config.FINAL_SAVE_PATH).to(config.DEVICE)\n",
    "        tokenizer = MBart50TokenizerFast.from_pretrained(config.FINAL_SAVE_PATH)\n",
    "        model.eval()\n",
    "        \n",
    "        gen_kwargs = {\n",
    "            \"num_beams\": 12,\n",
    "            \"length_penalty\": 2.0,\n",
    "            \"repetition_penalty\": 2.5,\n",
    "            \"no_repeat_ngram_size\": 3,\n",
    "            \"do_sample\": False,\n",
    "            \"early_stopping\": True,\n",
    "            \"min_length\": 30,\n",
    "            \"max_length\": 250,\n",
    "        }\n",
    "        \n",
    "        print(f\"--- Model v15 Loaded on {config.DEVICE} ---\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        return False\n",
    "\n",
    "def summarize_hindi(article_text: str):\n",
    "    \"\"\"Generates a high-quality Hindi summary for the given text.\"\"\"\n",
    "    if model is None or tokenizer is None:\n",
    "        print(\"Model is not loaded. Please run the setup cell (Cell 1) first.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SOURCE ARTICLE:\")\n",
    "    print(textwrap.fill(article_text, width=80))\n",
    "\n",
    "    tokenizer.src_lang = \"en_XX\"\n",
    "    inputs = tokenizer(article_text, return_tensors=\"pt\", max_length=1024, truncation=True).to(config.DEVICE)\n",
    "\n",
    "    # Generate Hindi Summary\n",
    "    hin_summary_ids = model.generate(\n",
    "        inputs.input_ids,\n",
    "        forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"],\n",
    "        **gen_kwargs  \n",
    "    )\n",
    "    hindi_summary = tokenizer.decode(hin_summary_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GENERATED HINDI SUMMARY (v15):\")\n",
    "    print(textwrap.fill(hindi_summary, width=80)) \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "load_model_and_settings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "705c9aee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 1/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "India secured a decisive victory over Australia in the final match of the T20\n",
      "series, winning by a margin of 35 runs in Bengaluru. Batting first, India posted\n",
      "a competitive total of 198 for 4, thanks to a powerful half-century from captain\n",
      "Suryakumar Yadav.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "    20        35   \n",
      "         ,   4   \n",
      "   198           \n",
      "     \n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 2/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "NASA's Artemis program achieved a major milestone this week as the Orion\n",
      "spacecraft successfully completed its uncrewed flyby of the Moon and is now on\n",
      "its return trajectory to Earth. The mission, Artemis I, is a critical test of\n",
      "the agency's deep space exploration systems.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "           ,  \n",
      "               \n",
      "               \n",
      "      I      \n",
      "   \n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 3/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "A new international report has found that the rate of deforestation in the\n",
      "Amazon rainforest accelerated by nearly 20% in the last year, reaching its\n",
      "highest level in over a decade. The report attributes the surge to increased\n",
      "illegal logging and agricultural expansion.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "              \n",
      "  20%    ,           \n",
      "               \n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 4/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "The highly anticipated sequel to a blockbuster science fiction film has\n",
      "officially begun production, with the studio releasing the first on-set photo.\n",
      "The image features the return of the original cast members alongside several new\n",
      "additions.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "         \n",
      "              ,  \n",
      "         \n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 5/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "A legacy automaker has announced a massive $50 billion investment into its\n",
      "electric vehicle (EV) division, signaling a dramatic acceleration of its\n",
      "transition away from internal combustion engines. The company plans to launch 15\n",
      "new all-electric models over the next three years.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "        ()    $50\n",
      "      ,        \n",
      "              15 \n",
      "-        \n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 6/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "Diplomats from several nations met in Geneva to resume peace talks aimed at\n",
      "resolving a long-standing regional conflict. The negotiations, which had been\n",
      "stalled for months, were restarted following a recent de-escalation of\n",
      "hostilities.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "             ,   \n",
      "               \n",
      "          \n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 7/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "Researchers have published a landmark study detailing a new gene-editing\n",
      "technique that shows promise in correcting genetic defects responsible for\n",
      "certain inherited diseases. The method uses a modified version of the CRISPR-\n",
      "Cas9 system.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "       ,    -\n",
      "       herited         \n",
      "    CRISPR-Cas9         \n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 8/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "A major tech firm today unveiled its latest flagship smartphone, featuring a\n",
      "revolutionary new camera system with 'periscope zoom' technology. The device,\n",
      "which also boasts a foldable OLED display and 5G connectivity, aims to redefine\n",
      "the premium mobile market.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "           , \n",
      "'' (periscope zoom)         \n",
      "   foldable OLED display  5G (LTE)    , \n",
      "         \n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 9/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "A new trade agreement between two major economic blocs was signed this week,\n",
      "aiming to reduce tariffs and streamline regulations across dozens of industries.\n",
      "The pact is the culmination of nearly five years of intense negotiations.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "              \n",
      " ,             \n",
      "              \n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 10/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "Global stock markets experienced a volatile week as central banks around the\n",
      "world signaled a more aggressive stance on combating inflation. The US Federal\n",
      "Reserve hinted at larger-than-expected interest rate hikes, causing a sell-off\n",
      "in technology stocks.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "     -   ,     \n",
      "            \n",
      "             \n",
      "  -  \n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "articles_to_test = [\n",
    "    # 1. Sports\n",
    "    \"\"\"India secured a decisive victory over Australia in the final match of the T20 series, winning by a margin of 35 runs in Bengaluru. Batting first, India posted a competitive total of 198 for 4, thanks to a powerful half-century from captain Suryakumar Yadav.\"\"\",\n",
    "    \n",
    "    # 2. Science\n",
    "    \"\"\"NASA's Artemis program achieved a major milestone this week as the Orion spacecraft successfully completed its uncrewed flyby of the Moon and is now on its return trajectory to Earth. The mission, Artemis I, is a critical test of the agency's deep space exploration systems.\"\"\",\n",
    "    \n",
    "    # 3. Environment\n",
    "    \"\"\"A new international report has found that the rate of deforestation in the Amazon rainforest accelerated by nearly 20% in the last year, reaching its highest level in over a decade. The report attributes the surge to increased illegal logging and agricultural expansion.\"\"\",\n",
    "    \n",
    "    # 4. Entertainment\n",
    "    \"\"\"The highly anticipated sequel to a blockbuster science fiction film has officially begun production, with the studio releasing the first on-set photo. The image features the return of the original cast members alongside several new additions.\"\"\",\n",
    "    \n",
    "    # 5. Automotive/Tech\n",
    "    \"\"\"A legacy automaker has announced a massive $50 billion investment into its electric vehicle (EV) division, signaling a dramatic acceleration of its transition away from internal combustion engines. The company plans to launch 15 new all-electric models over the next three years.\"\"\",\n",
    "    \n",
    "    # 6. World News\n",
    "    \"\"\"Diplomats from several nations met in Geneva to resume peace talks aimed at resolving a long-standing regional conflict. The negotiations, which had been stalled for months, were restarted following a recent de-escalation of hostilities.\"\"\",\n",
    "    \n",
    "    # 7. Health\n",
    "    \"\"\"Researchers have published a landmark study detailing a new gene-editing technique that shows promise in correcting genetic defects responsible for certain inherited diseases. The method uses a modified version of the CRISPR-Cas9 system.\"\"\",\n",
    "    \n",
    "    # 8. Technology\n",
    "    \"\"\"A major tech firm today unveiled its latest flagship smartphone, featuring a revolutionary new camera system with 'periscope zoom' technology. The device, which also boasts a foldable OLED display and 5G connectivity, aims to redefine the premium mobile market.\"\"\",\n",
    "    \n",
    "    # 9. International Relations\n",
    "    \"\"\"A new trade agreement between two major economic blocs was signed this week, aiming to reduce tariffs and streamline regulations across dozens of industries. The pact is the culmination of nearly five years of intense negotiations.\"\"\",\n",
    "    \n",
    "    # 10. Finance (from previous test)\n",
    "    \"\"\"Global stock markets experienced a volatile week as central banks around the world signaled a more aggressive stance on combating inflation. The US Federal Reserve hinted at larger-than-expected interest rate hikes, causing a sell-off in technology stocks.\"\"\"\n",
    "]\n",
    "\n",
    "for i, article in enumerate(articles_to_test, 1):\n",
    "    print(f\"\\n\\n--- SUMMARIZING ARTICLE {i}/{len(articles_to_test)} ---\")\n",
    "    summarize_hindi(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f7de129d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 1/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "     A new study published in the Lancet suggests that moderate, regular\n",
      "exercise,     such as brisk walking or cycling for 30 minutes a day,     can\n",
      "significantly reduce the risk of cardiovascular disease.     The research\n",
      "tracked over 100,000 individuals across 15 countries,     finding that even non-\n",
      "strenuous activity was highly beneficial     for long-term heart health,\n",
      "independent of other risk factors.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "            30    \n",
      "               \n",
      "  15   100,000          \n",
      "     -       \n",
      " ,        \n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 2/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "     Quantum computing researchers at a leading university have announced     a\n",
      "breakthrough in qubit stability. By developing a new silicon-based\n",
      "architecture, they managed to maintain a quantum state for several\n",
      "milliseconds, a significant leap from the previous microsecond records.     This\n",
      "advancement addresses a major hurdle in building scalable,     fault-tolerant\n",
      "quantum computers.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "          \n",
      ",    -        \n",
      ",           ,  \n",
      "             \n",
      "  , -         \n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 3/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "     Oceanographers are expressing growing concern over the rapid expansion\n",
      "of \"oxygen minimum zones\" (OMZs) in the world's oceans.     These vast areas,\n",
      "where oxygen levels are too low to support most marine life,     are being\n",
      "exacerbated by climate change and nutrient runoff from agriculture.     The\n",
      "expanding dead zones threaten vital fishing industries and marine biodiversity.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "   (OMZs)         , \n",
      "             \n",
      "             \n",
      "  ,           \n",
      "       OMZs    -    \n",
      "   \n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 4/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "     The James Webb Space Telescope has captured a stunning new image     of a\n",
      "distant stellar nursery, revealing intricate details of star formation     that\n",
      "were previously hidden by dust clouds. The infrared image shows     dozens of\n",
      "protostars in their earliest stages, offering astronomers     an unprecedented\n",
      "glimpse into the processes that formed our own solar system.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "           \n",
      "    ,         infra  \n",
      " protostars      ,    \n",
      "            \n",
      "        ,     \n",
      "  \n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 5/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "     Semiconductor manufacturing giant TSMC has confirmed plans to build     a\n",
      "new $40 billion fabrication plant in Arizona. The move is part     of a broader\n",
      "global trend to diversify the microchip supply chain,     which has faced\n",
      "significant disruptions over the past few years.     The new plant is expected\n",
      "to create thousands of high-tech jobs     but also faces challenges related to\n",
      "water supply and skilled labor.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      " (TSMC)   (Arizona)    $40    \n",
      "            (microchip)\n",
      "          ,   \n",
      "          - \n",
      "             - \n",
      "       \n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 6/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "     AI safety researchers are calling for new regulatory frameworks to govern\n",
      "the development of \"frontier\" AI models. A recently published paper     argues\n",
      "that as models become more powerful, their potential for misuse     in\n",
      "cybersecurity and disinformation campaigns grows exponentially.     The authors\n",
      "propose a system of independent audits and licensing for     models that exceed\n",
      "a certain capability threshold.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "AI    \"\" AI       \n",
      "               - \n",
      "    ,       \n",
      "    ,       \n",
      "              \n",
      " \n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 7/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "     A critical summit on global food security concluded in Rome, with\n",
      "leaders pledging to increase funding for sustainable agriculture     and\n",
      "emergency food aid. The talks were overshadowed by ongoing supply     chain\n",
      "disruptions and a sharp rise in fertilizer costs, which have     compounded food\n",
      "shortages in several developing nations.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "            ,\n",
      "             \n",
      "  ,       \n",
      "       ,      \n",
      "  \n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 8/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "     A 1962 Ferrari 250 GTO has become the most expensive car from the\n",
      "automaker ever sold at public auction, fetching a staggering $51.7 million.\n",
      "The legendary vehicle, one of only 36 ever built, was the centerpiece     of a\n",
      "high-profile art and luxury auction in New York.     The sale underscores the\n",
      "soaring value of rare, classic automobiles     as tangible assets for high-net-\n",
      "worth collectors.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      " 250 GTO,     ,    -  \n",
      "   $51.7      ,   36   \n",
      " ,        --    ,\n",
      "         \n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 9/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "     Biotech firm Moderna has announced promising results from its Phase 3\n",
      "trial for a combination vaccine targeting both COVID-19 and influenza.     The\n",
      "single-shot mRNA vaccine reportedly produced a stronger immune     response\n",
      "against both viruses than separate, co-administered flu     and COVID shots. If\n",
      "approved, the combination vaccine could simplify     annual immunization\n",
      "campaigns significantly.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "      3        , \n",
      "COVID-19   (COVID)   -      \n",
      "           , \n",
      "-, -     COVID        \n",
      " ,              \n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 10/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "     A global cybersecurity firm has issued an alert regarding a\n",
      "sophisticated new strain of ransomware targeting hospital networks.     The\n",
      "malware, dubbed \"Medusa,\" not only encrypts critical patient     data but also\n",
      "exfiltrates it, putting added pressure on healthcare     providers to pay the\n",
      "ransom. Experts are urging all medical N-institutions to update their security\n",
      "protocols immediately.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "      \"\"    Ransomware  \n",
      "  ,           \n",
      "              ,\n",
      "        ransom     \n",
      "   N-        \n",
      "   \n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "articles_to_test = [\n",
    "    # 1. Health\n",
    "    \"\"\"\n",
    "    A new study published in the Lancet suggests that moderate, regular exercise,\n",
    "    such as brisk walking or cycling for 30 minutes a day,\n",
    "    can significantly reduce the risk of cardiovascular disease.\n",
    "    The research tracked over 100,000 individuals across 15 countries,\n",
    "    finding that even non-strenuous activity was highly beneficial\n",
    "    for long-term heart health, independent of other risk factors.\n",
    "    \"\"\",\n",
    "    \n",
    "    # 2. Technology\n",
    "    \"\"\"\n",
    "    Quantum computing researchers at a leading university have announced\n",
    "    a breakthrough in qubit stability. By developing a new silicon-based\n",
    "    architecture, they managed to maintain a quantum state for several\n",
    "    milliseconds, a significant leap from the previous microsecond records.\n",
    "    This advancement addresses a major hurdle in building scalable,\n",
    "    fault-tolerant quantum computers.\n",
    "    \"\"\",\n",
    "    \n",
    "    # 3. Environment\n",
    "    \"\"\"\n",
    "    Oceanographers are expressing growing concern over the rapid expansion\n",
    "    of \"oxygen minimum zones\" (OMZs) in the world's oceans.\n",
    "    These vast areas, where oxygen levels are too low to support most marine life,\n",
    "    are being exacerbated by climate change and nutrient runoff from agriculture.\n",
    "    The expanding dead zones threaten vital fishing industries and marine biodiversity.\n",
    "    \"\"\",\n",
    "    \n",
    "    # 4. Space\n",
    "    \"\"\"\n",
    "    The James Webb Space Telescope has captured a stunning new image\n",
    "    of a distant stellar nursery, revealing intricate details of star formation\n",
    "    that were previously hidden by dust clouds. The infrared image shows\n",
    "    dozens of protostars in their earliest stages, offering astronomers\n",
    "    an unprecedented glimpse into the processes that formed our own solar system.\n",
    "    \"\"\",\n",
    "    \n",
    "    # 5. Business\n",
    "    \"\"\"\n",
    "    Semiconductor manufacturing giant TSMC has confirmed plans to build\n",
    "    a new $40 billion fabrication plant in Arizona. The move is part\n",
    "    of a broader global trend to diversify the microchip supply chain,\n",
    "    which has faced significant disruptions over the past few years.\n",
    "    The new plant is expected to create thousands of high-tech jobs\n",
    "    but also faces challenges related to water supply and skilled labor.\n",
    "    \"\"\",\n",
    "    \n",
    "    # 6. Artificial Intelligence\n",
    "    \"\"\"\n",
    "    AI safety researchers are calling for new regulatory frameworks to govern\n",
    "    the development of \"frontier\" AI models. A recently published paper\n",
    "    argues that as models become more powerful, their potential for misuse\n",
    "    in cybersecurity and disinformation campaigns grows exponentially.\n",
    "    The authors propose a system of independent audits and licensing for\n",
    "    models that exceed a certain capability threshold.\n",
    "    \"\"\",\n",
    "    \n",
    "    # 7. World News\n",
    "    \"\"\"\n",
    "    A critical summit on global food security concluded in Rome, with\n",
    "    leaders pledging to increase funding for sustainable agriculture\n",
    "    and emergency food aid. The talks were overshadowed by ongoing supply\n",
    "    chain disruptions and a sharp rise in fertilizer costs, which have\n",
    "    compounded food shortages in several developing nations.\n",
    "    \"\"\",\n",
    "    \n",
    "    # 8. Entertainment\n",
    "    \"\"\"\n",
    "    A 1962 Ferrari 250 GTO has become the most expensive car from the\n",
    "    automaker ever sold at public auction, fetching a staggering $51.7 million.\n",
    "    The legendary vehicle, one of only 36 ever built, was the centerpiece\n",
    "    of a high-profile art and luxury auction in New York.\n",
    "    The sale underscores the soaring value of rare, classic automobiles\n",
    "    as tangible assets for high-net-worth collectors.\n",
    "    \"\"\",\n",
    "    \n",
    "    # 9. Health & Medicine\n",
    "    \"\"\"\n",
    "    Biotech firm Moderna has announced promising results from its Phase 3\n",
    "    trial for a combination vaccine targeting both COVID-19 and influenza.\n",
    "    The single-shot mRNA vaccine reportedly produced a stronger immune\n",
    "    response against both viruses than separate, co-administered flu\n",
    "    and COVID shots. If approved, the combination vaccine could simplify\n",
    "    annual immunization campaigns significantly.\n",
    "    \"\"\",\n",
    "    \n",
    "    # 10. Cybersecurity\n",
    "    \"\"\"\n",
    "    A global cybersecurity firm has issued an alert regarding a\n",
    "    sophisticated new strain of ransomware targeting hospital networks.\n",
    "    The malware, dubbed \"Medusa,\" not only encrypts critical patient\n",
    "    data but also exfiltrates it, putting added pressure on healthcare\n",
    "    providers to pay the ransom. Experts are urging all medical\n",
    "N-institutions to update their security protocols immediately.\n",
    "    \"\"\"\n",
    "]\n",
    "\n",
    "for i, article in enumerate(articles_to_test, 1):\n",
    "    print(f\"\\n\\n--- SUMMARIZING ARTICLE {i}/{len(articles_to_test)} ---\")\n",
    "    summarize_hindi(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f35d760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 1/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "     India secured a decisive victory over Australia in the final match of the\n",
      "T20      series, winning by a margin of 35 runs in Bengaluru. Batting first,\n",
      "India posted      a competitive total of 198 for 4, thanks to a powerful half-\n",
      "century from captain      Suryakumar Yadav, who scored 78 off just 45 balls. In\n",
      "response, Australia's      chase faltered early as they lost key wickets to\n",
      "India's fast bowlers.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "    20         35  \n",
      "          ,    \n",
      "198          ,   45\n",
      "  78     ,      \n",
      "             \n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 2/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "     A major tech firm today unveiled its latest flagship smartphone, featuring\n",
      "a      revolutionary new camera system with 'periscope zoom' technology. The\n",
      "device,      which also boasts a foldable OLED display and 5G connectivity, aims\n",
      "to redefine      the premium mobile market. Analysts are optimistic, noting that\n",
      "the innovative      camera could be a key differentiator in a crowded field.\n",
      "However, concerns remain      about the device's high price point, which exceeds\n",
      "$1,500, potentially      limiting its mass-market appeal despite the advanced\n",
      "features.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "            , \n",
      "''          foldable OLED\n",
      "            \n",
      "    ,          \n",
      "       ,    ,  \n",
      "$1,500             \n",
      "   \n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 3/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "     NASA's Artemis program achieved a major milestone this week as the Orion\n",
      "spacecraft successfully completed its uncrewed flyby of the Moon and is now on\n",
      "its return trajectory to Earth. The mission, Artemis I, is a critical test of\n",
      "the agency's deep space exploration systems, including the powerful Space Launch\n",
      "System (SLS) rocket and the Orion crew capsule. During its journey, Orion\n",
      "traveled farther from Earth than any human-rated spacecraft has ever gone\n",
      "before, capturing stunning high-resolution images of the lunar surface and\n",
      "Earth from a distance. The spacecraft's heat shield will face its most extreme\n",
      "test during re-entry, when it will endure temperatures of nearly 5,000\n",
      "degrees Fahrenheit while traveling at over 24,000 miles per hour. A successful\n",
      "splashdown in the Pacific Ocean will pave the way for Artemis II, the program's\n",
      "first crewed mission, which will send astronauts on a similar lunar flyby,\n",
      "further cementing humanity's path back to the Moon and, eventually, to Mars.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "           , \n",
      "              \n",
      "            \n",
      " ,     ()      \n",
      "        ,      \n",
      "     ,       -\n",
      "             -\n",
      "   5,000      24,000    \n",
      "               \n",
      " , Artemis II        ,     \n",
      "     \n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 4/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "     Global stock markets experienced a volatile week as central banks around\n",
      "the      world signaled a more aggressive stance on combating inflation. The US\n",
      "Federal      Reserve hinted at larger-than-expected interest rate hikes, causing\n",
      "a sell-off      in technology stocks and growth-oriented sectors. Meanwhile, the\n",
      "European      Central Bank is facing pressure to act as energy prices continue\n",
      "to soar      across the continent, impacting both consumer spending and\n",
      "industrial      production. Investors are now closely watching upcoming\n",
      "inflation data and      corporate earnings reports for signs of a potential\n",
      "economic slowdown. Experts      suggest a period of uncertainty is likely to\n",
      "continue as markets digest the      new reality of tighter monetary policy.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "     -   ,     \n",
      "            \n",
      "         ,    \n",
      "-      ,   () \n",
      "       ,      \n",
      "               \n",
      " inflation           \n",
      "            , \n",
      "              \n",
      "  \n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 5/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "     Diplomats from several nations met in Geneva to resume peace talks aimed at\n",
      "resolving a long-standing regional conflict. The negotiations, which had\n",
      "been stalled for months, were restarted following a recent de-escalation of\n",
      "hostilities. Observers are cautiously optimistic, but acknowledge that\n",
      "significant political hurdles remain. The primary goal of the current round\n",
      "of talks is to establish a lasting ceasefire and facilitate the delivery of\n",
      "humanitarian aid to affected civilian populations.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "             ,   \n",
      "               \n",
      "       ,     \n",
      "            \n",
      "            \n",
      "           \n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 6/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "     Researchers have published a landmark study in the journal 'Nature'\n",
      "detailing      a new gene-editing technique that shows promise in correcting\n",
      "genetic defects      responsible for certain inherited diseases. The method,\n",
      "which uses a modified      version of the CRISPR-Cas9 system, demonstrated a\n",
      "significantly higher precision      and lower rate of off-target mutations in\n",
      "lab experiments compared to existing      technologies. The study focused on a\n",
      "specific mutation linked to cystic      fibrosis, and the results in human cell\n",
      "cultures were highly encouraging.      While the research is still in its early\n",
      "stages and human trials are years      away, the scientific community is hailing\n",
      "it as a potential breakthrough.      The technique's improved safety profile\n",
      "could overcome some of the major hurdles      that have slowed the clinical\n",
      "application of gene therapy. However, the      scientists involved urge caution,\n",
      "emphasizing that extensive further research      is required to validate the\n",
      "findings and ensure the long-term safety and      efficacy of this new approach\n",
      "before it can be considered for patient treatment.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "  ''         , \n",
      "  -        ,  CRISPR-Cas9 \n",
      "       ,        \n",
      "               \n",
      "     -        , \n",
      "   (C cystic fibrosis)      \n",
      " ,           \n",
      "        ,     \"\n",
      "breakthrough\"  ,          \n",
      "             \n",
      "       ,       \n",
      "  \n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 7/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "     A new international report has found that the rate of deforestation in the\n",
      "Amazon rainforest accelerated by nearly 20% in the last year, reaching its\n",
      "highest level in over a decade. The report, which uses satellite data,\n",
      "attributes the surge to increased illegal logging, agricultural expansion,\n",
      "and mining activities. Environmental groups are calling for urgent government\n",
      "intervention and stronger enforcement of existing protection laws. The Amazon\n",
      "is a critical global ecosystem, playing a vital role in regulating the\n",
      "planet's climate by absorbing vast amounts of carbon dioxide. Scientists\n",
      "warn that continued deforestation could push the rainforest towards a\n",
      "tipping point, where it would transition into a drier, savanna-like state,\n",
      "with devastating consequences for global biodiversity and climate patterns.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "              \n",
      "  20%    ,           \n",
      "        ,   ,  \n",
      "             \n",
      "             \n",
      "            \n",
      "           \n",
      "               \n",
      ",    , -     ,  \n",
      "biodiversity        \n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 8/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "     The highly anticipated sequel to a blockbuster science fiction film has\n",
      "officially begun production, with the studio releasing the first on-set photo.\n",
      "The image features the return of the original cast members alongside several new\n",
      "additions. The director has promised that the sequel will expand the universe\n",
      "in exciting ways while honoring the spirit of the first film. The movie is\n",
      "currently slated for a summer release next year.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "           \n",
      "    ,             \n",
      "                \n",
      "               \n",
      "          \n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 9/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "     A legacy automaker has announced a massive $50 billion investment into its\n",
      "electric vehicle (EV) division, signaling a dramatic acceleration of its\n",
      "transition away from internal combustion engines. The company plans to\n",
      "launch 15 new all-electric models over the next three years, including sedans,\n",
      "SUVs, and a pickup truck. A key part of the strategy involves building\n",
      "several new \"gigafactories\" for battery production in North America and\n",
      "Europe to secure its supply chain. This move is seen by industry experts      as\n",
      "a direct response to the growing dominance of EV-native companies and\n",
      "increasing regulatory pressure to reduce emissions.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "        ()    $50 \n",
      "   ,      15  - \n",
      "  ,  ,          \n",
      "          \"\" \n",
      "            \n",
      " -            \n",
      "       \n",
      "\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--- SUMMARIZING ARTICLE 10/10 ---\n",
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      "     A new trade agreement between two major economic blocs was signed this\n",
      "week,      aiming to reduce tariffs and streamline regulations across dozens of\n",
      "industries.      The pact, which covers everything from agricultural goods to\n",
      "digital services,      is the culmination of nearly five years of intense\n",
      "negotiations. Proponents      argue that the deal will boost economic growth,\n",
      "lower consumer prices, and      create hundreds of thousands of new jobs by\n",
      "fostering closer integration and      simplifying cross-border commerce.\n",
      "However, the agreement has also faced      criticism from labor unions and\n",
      "environmental groups, who argue that it      lacks sufficient protections for\n",
      "workers' rights and fails to implement      strong environmental standards. They\n",
      "warn that the deal could lead to a \"race      to the bottom\" as companies\n",
      "relocate to regions with lower wages and weaker      regulations. The signatory\n",
      "governments have defended the pact, stating that      it includes robust\n",
      "chapters on labor and the environment and will be subject      to regular\n",
      "reviews to ensure compliance and address any emerging issues.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "         ,   \n",
      "                \n",
      "       ,       \n",
      "           ,   \n",
      "      ,        \n",
      ",            \n",
      "  ,            \n",
      "          ,  \"  \"\n",
      "              \n",
      "              \n",
      "    \n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "articles_to_test = [\n",
    "    # --- Short Article (Sports) ---\n",
    "    \"\"\"\n",
    "    India secured a decisive victory over Australia in the final match of the T20 \n",
    "    series, winning by a margin of 35 runs in Bengaluru. Batting first, India posted \n",
    "    a competitive total of 198 for 4, thanks to a powerful half-century from captain \n",
    "    Suryakumar Yadav, who scored 78 off just 45 balls. In response, Australia's \n",
    "    chase faltered early as they lost key wickets to India's fast bowlers.\n",
    "    \"\"\",\n",
    "    \n",
    "    # --- Medium Article (Technology) ---\n",
    "    \"\"\"\n",
    "    A major tech firm today unveiled its latest flagship smartphone, featuring a \n",
    "    revolutionary new camera system with 'periscope zoom' technology. The device, \n",
    "    which also boasts a foldable OLED display and 5G connectivity, aims to redefine \n",
    "    the premium mobile market. Analysts are optimistic, noting that the innovative \n",
    "    camera could be a key differentiator in a crowded field. However, concerns remain \n",
    "    about the device's high price point, which exceeds $1,500, potentially \n",
    "    limiting its mass-market appeal despite the advanced features.\n",
    "    \"\"\",\n",
    "    \n",
    "    # --- Long Article (Science/Space) ---\n",
    "    \"\"\"\n",
    "    NASA's Artemis program achieved a major milestone this week as the Orion \n",
    "    spacecraft successfully completed its uncrewed flyby of the Moon and is now on \n",
    "    its return trajectory to Earth. The mission, Artemis I, is a critical test of \n",
    "    the agency's deep space exploration systems, including the powerful Space Launch \n",
    "    System (SLS) rocket and the Orion crew capsule. During its journey, Orion \n",
    "    traveled farther from Earth than any human-rated spacecraft has ever gone \n",
    "    before, capturing stunning high-resolution images of the lunar surface and \n",
    "    Earth from a distance. The spacecraft's heat shield will face its most extreme \n",
    "    test during re-entry, when it will endure temperatures of nearly 5,000 \n",
    "    degrees Fahrenheit while traveling at over 24,000 miles per hour. A successful \n",
    "    splashdown in the Pacific Ocean will pave the way for Artemis II, the program's \n",
    "    first crewed mission, which will send astronauts on a similar lunar flyby, \n",
    "    further cementing humanity's path back to the Moon and, eventually, to Mars.\n",
    "    \"\"\",\n",
    "    \n",
    "    # --- Medium Article (Business/Finance) ---\n",
    "    \"\"\"\n",
    "    Global stock markets experienced a volatile week as central banks around the \n",
    "    world signaled a more aggressive stance on combating inflation. The US Federal \n",
    "    Reserve hinted at larger-than-expected interest rate hikes, causing a sell-off \n",
    "    in technology stocks and growth-oriented sectors. Meanwhile, the European \n",
    "    Central Bank is facing pressure to act as energy prices continue to soar \n",
    "    across the continent, impacting both consumer spending and industrial \n",
    "    production. Investors are now closely watching upcoming inflation data and \n",
    "    corporate earnings reports for signs of a potential economic slowdown. Experts \n",
    "    suggest a period of uncertainty is likely to continue as markets digest the \n",
    "    new reality of tighter monetary policy.\n",
    "    \"\"\",\n",
    "\n",
    "    # --- Short Article (World News) ---\n",
    "    \"\"\"\n",
    "    Diplomats from several nations met in Geneva to resume peace talks aimed at \n",
    "    resolving a long-standing regional conflict. The negotiations, which had \n",
    "    been stalled for months, were restarted following a recent de-escalation of \n",
    "    hostilities. Observers are cautiously optimistic, but acknowledge that \n",
    "    significant political hurdles remain. The primary goal of the current round \n",
    "    of talks is to establish a lasting ceasefire and facilitate the delivery of \n",
    "    humanitarian aid to affected civilian populations.\n",
    "    \"\"\",\n",
    "    \n",
    "    # --- Long Article (Health/Science) ---\n",
    "    \"\"\"\n",
    "    Researchers have published a landmark study in the journal 'Nature' detailing \n",
    "    a new gene-editing technique that shows promise in correcting genetic defects \n",
    "    responsible for certain inherited diseases. The method, which uses a modified \n",
    "    version of the CRISPR-Cas9 system, demonstrated a significantly higher precision \n",
    "    and lower rate of off-target mutations in lab experiments compared to existing \n",
    "    technologies. The study focused on a specific mutation linked to cystic \n",
    "    fibrosis, and the results in human cell cultures were highly encouraging. \n",
    "    While the research is still in its early stages and human trials are years \n",
    "    away, the scientific community is hailing it as a potential breakthrough. \n",
    "    The technique's improved safety profile could overcome some of the major hurdles \n",
    "    that have slowed the clinical application of gene therapy. However, the \n",
    "    scientists involved urge caution, emphasizing that extensive further research \n",
    "    is required to validate the findings and ensure the long-term safety and \n",
    "    efficacy of this new approach before it can be considered for patient treatment.\n",
    "    \"\"\",\n",
    "\n",
    "    # --- Medium Article (Environment) ---\n",
    "    \"\"\"\n",
    "    A new international report has found that the rate of deforestation in the \n",
    "    Amazon rainforest accelerated by nearly 20% in the last year, reaching its \n",
    "    highest level in over a decade. The report, which uses satellite data, \n",
    "    attributes the surge to increased illegal logging, agricultural expansion, \n",
    "    and mining activities. Environmental groups are calling for urgent government \n",
    "    intervention and stronger enforcement of existing protection laws. The Amazon \n",
    "    is a critical global ecosystem, playing a vital role in regulating the \n",
    "    planet's climate by absorbing vast amounts of carbon dioxide. Scientists \n",
    "    warn that continued deforestation could push the rainforest towards a \n",
    "    tipping point, where it would transition into a drier, savanna-like state, \n",
    "    with devastating consequences for global biodiversity and climate patterns.\n",
    "    \"\"\",\n",
    "\n",
    "    # --- Short Article (Entertainment) ---\n",
    "    \"\"\"\n",
    "    The highly anticipated sequel to a blockbuster science fiction film has \n",
    "    officially begun production, with the studio releasing the first on-set photo. \n",
    "    The image features the return of the original cast members alongside several new \n",
    "    additions. The director has promised that the sequel will expand the universe \n",
    "    in exciting ways while honoring the spirit of the first film. The movie is \n",
    "    currently slated for a summer release next year.\n",
    "    \"\"\",\n",
    "\n",
    "    # --- Medium Article (Automotive/Tech) ---\n",
    "    \"\"\"\n",
    "    A legacy automaker has announced a massive $50 billion investment into its \n",
    "    electric vehicle (EV) division, signaling a dramatic acceleration of its \n",
    "    transition away from internal combustion engines. The company plans to \n",
    "    launch 15 new all-electric models over the next three years, including sedans, \n",
    "    SUVs, and a pickup truck. A key part of the strategy involves building \n",
    "    several new \"gigafactories\" for battery production in North America and \n",
    "    Europe to secure its supply chain. This move is seen by industry experts \n",
    "    as a direct response to the growing dominance of EV-native companies and \n",
    "    increasing regulatory pressure to reduce emissions.\n",
    "    \"\"\",\n",
    "\n",
    "    # --- Long Article (International Relations) ---\n",
    "    \"\"\"\n",
    "    A new trade agreement between two major economic blocs was signed this week, \n",
    "    aiming to reduce tariffs and streamline regulations across dozens of industries. \n",
    "    The pact, which covers everything from agricultural goods to digital services, \n",
    "    is the culmination of nearly five years of intense negotiations. Proponents \n",
    "    argue that the deal will boost economic growth, lower consumer prices, and \n",
    "    create hundreds of thousands of new jobs by fostering closer integration and \n",
    "    simplifying cross-border commerce. However, the agreement has also faced \n",
    "    criticism from labor unions and environmental groups, who argue that it \n",
    "    lacks sufficient protections for workers' rights and fails to implement \n",
    "    strong environmental standards. They warn that the deal could lead to a \"race \n",
    "    to the bottom\" as companies relocate to regions with lower wages and weaker \n",
    "    regulations. The signatory governments have defended the pact, stating that \n",
    "    it includes robust chapters on labor and the environment and will be subject \n",
    "    to regular reviews to ensure compliance and address any emerging issues.\n",
    "    \"\"\",\n",
    "]\n",
    "\n",
    "for i, article in enumerate(articles_to_test, 1):\n",
    "    print(f\"\\n\\n--- SUMMARIZING ARTICLE {i}/{len(articles_to_test)} ---\")\n",
    "    \n",
    "    summarize_hindi(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2625c7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "SOURCE ARTICLE:\n",
      " Global stock markets experienced a volatile week as central banks  around the\n",
      "world signaled a more aggressive stance on combating inflation.  The US Federal\n",
      "Reserve hinted at larger-than-expected interest rate hikes,  causing a sell-off\n",
      "in technology stocks and growth-oriented sectors.  Meanwhile, the European\n",
      "Central Bank is facing pressure to act as energy  prices continue to soar across\n",
      "the continent, impacting both consumer  spending and industrial production.\n",
      "\n",
      "================================================================================\n",
      "GENERATED HINDI SUMMARY (v15):\n",
      "     -   ,      \n",
      "            \n",
      "          ,   \n",
      " -      ,   (ECB) \n",
      "           ,   \n",
      "        \n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "article_to_test = \"\"\"\n",
    "Global stock markets experienced a volatile week as central banks \n",
    "around the world signaled a more aggressive stance on combating inflation. \n",
    "The US Federal Reserve hinted at larger-than-expected interest rate hikes, \n",
    "causing a sell-off in technology stocks and growth-oriented sectors. \n",
    "Meanwhile, the European Central Bank is facing pressure to act as energy \n",
    "prices continue to soar across the continent, impacting both consumer \n",
    "spending and industrial production.\n",
    "\"\"\"\n",
    "\n",
    "summarize_hindi(article_to_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f171ecb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd9b187c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45b84937",
   "metadata": {},
   "source": [
    "Bulk Testing on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a5d999f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file mbart-large-50-cnn-summarizer-v15\\final_model\\config.json\n",
      "Model config MBartConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"gelu\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": true,\n",
      "  \"architectures\": [\n",
      "    \"MBartForConditionalGeneration\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 1024,\n",
      "  \"decoder_attention_heads\": 16,\n",
      "  \"decoder_ffn_dim\": 4096,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 12,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"dropout\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"early_stopping\": null,\n",
      "  \"encoder_attention_heads\": 16,\n",
      "  \"encoder_ffn_dim\": 4096,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 12,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": null,\n",
      "  \"max_position_embeddings\": 1024,\n",
      "  \"model_type\": \"mbart\",\n",
      "  \"normalize_before\": true,\n",
      "  \"normalize_embedding\": true,\n",
      "  \"num_beams\": null,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 1,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": false,\n",
      "  \"tokenizer_class\": \"MBart50Tokenizer\",\n",
      "  \"transformers_version\": \"4.57.0\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 250054\n",
      "}\n",
      "\n",
      "loading weights file mbart-large-50-cnn-summarizer-v15\\final_model\\model.safetensors\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Starting Bulk Test Script ---\n",
      "Logging all output to: mbart-large-50-cnn-summarizer-v15\\bulk_test_interactive_log_v15_2025-11-14_08-51-07.log\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file mbart-large-50-cnn-summarizer-v15\\final_model\\generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 0,\n",
      "  \"decoder_start_token_id\": 2,\n",
      "  \"early_stopping\": true,\n",
      "  \"eos_token_id\": [\n",
      "    2\n",
      "  ],\n",
      "  \"forced_eos_token_id\": 2,\n",
      "  \"max_length\": 200,\n",
      "  \"num_beams\": 5,\n",
      "  \"pad_token_id\": 1\n",
      "}\n",
      "\n",
      "Could not locate the custom_generate/generate.py inside mbart-large-50-cnn-summarizer-v15\\final_model.\n",
      "loading file sentencepiece.bpe.model\n",
      "loading file tokenizer.json\n",
      "loading file added_tokens.json\n",
      "loading file special_tokens_map.json\n",
      "loading file tokenizer_config.json\n",
      "loading file chat_template.jinja\n",
      "Summarizing Articles:  10%|         | 1/10 [00:06<00:55,  6.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Article 1/10 ---\n",
      "\n",
      "SOURCE:\n",
      "Charlie Kirk's suspected killer brought into custody after confessing to father\n",
      "2 days ago Share Save Christal Hayes and Regan Morris Utah Share Save Utah\n",
      "governor details how Charlie Kirk murder suspect apprehended A 22-year-old from\n",
      "Utah has been arrested over the killing of right-wing activist Charlie Kirk, who\n",
      "was shot dead while on stage at a university event earlier this week. Tyler\n",
      "Robinson was taken into custody late on Thursday, after a 33-hour manhunt that\n",
      "ended after his father helped persuade him to surrender to police. His arrest\n",
      "was first announced by US President Donald Trump, who called for the suspect to\n",
      "face the death penalty. The killing of Kirk, who was shot while debating with\n",
      "students on Wednesday, has shocked Americans and laid bare the country's sharp\n",
      "partisan divisions. Follow live updates Shooting throws Utah students into heart\n",
      "of US political divide 'I will never let your legacy die' - Charlie Kirk's widow\n",
      "gives tearful address after shooting Investigators said at a news conference on\n",
      "Friday that the suspect had confessed to his father and said he would rather\n",
      "take his own life than surrender. The father then called a youth pastor who is a\n",
      "family friend. Both men tried to calm the suspect down, police said. The pastor,\n",
      "who also serves as a court security officer, later called the US Marshals, who\n",
      "detained the suspect at around 22:00 local time on Thursday. Utah's Governor\n",
      "Spencer Cox said surveillance images showed Mr Robinson arriving on the campus\n",
      "of Utah Valley University around four hours before a shot rang out, killing Kirk\n",
      "and sending students running for cover. Cox told journalists that when Mr\n",
      "Robinson was taken into custody, he was wearing clothing similar to what was\n",
      "seen on CCTV cameras at the scene of the shooting. The governor added that\n",
      "investigators had interviewed a family member, who said the suspect had become\n",
      "more political in recent years. Cox said the family member had spoken of a\n",
      "recent incident when Mr Robinson had mentioned that Kirk was coming to Utah and\n",
      "that he \"was full of hate and spreading hate\". BBC reports from Utah home linked\n",
      "to Charlie Kirk shooting suspect Cox said investigators had also spoken to a\n",
      "roommate of the suspect who had shown them exchanges of messages with an account\n",
      "named \"Tyler\" on the messaging app Discord. The messages referred to a need to\n",
      "retrieve a rifle from \"a drop point\" and the firearm being left in a bush,\n",
      "wrapped in a towel. The FBI said on Thursday that they had found the suspected\n",
      "weapon - an imported Mauser .30-06 bolt action rifle - wrapped in a towel in a\n",
      "wooded area near campus. Cox told reporters that inscriptions had been found\n",
      "engraved on casings recovered with the rifle, which had a scope mounted on top\n",
      "of it. The inscriptions included \"hey fascist! CATCH!\" and \"O Bella ciao, Bella\n",
      "ciao\". \"Bella ciao\" means \"goodbye beautiful\" in Italian. It is also the title\n",
      "of a song dedicated to the Italian resistance that fought against the occupying\n",
      "troops of Nazi Germany. Utah's governor said he was not aware of any potential\n",
      "further arrests in the investigation. State prosecutors said they planned to\n",
      "file formal charges against Mr Robinson on Tuesday. He is accused of aggravated\n",
      "murder, obstruction of justice, and felony discharge of a firearm, according to\n",
      "a Utah County Sheriff inmate booking sheet obtained by the BBC.\n",
      "\n",
      "GENERATED HINDI:\n",
      "  22             \n",
      "   ,          \n",
      "              \n",
      "            ,   33\n",
      "               \n",
      "  ,          \n",
      "              \n",
      "               \n",
      "   ,            \n",
      "       \"   \"    \n",
      "           .30-06  \n",
      "     \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing Articles:  20%|        | 2/10 [00:12<00:49,  6.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Article 2/10 ---\n",
      "\n",
      "SOURCE:\n",
      "Lockerbie bombing accused says he was forced into false confession 2 hours ago\n",
      "Share Save David Cowan Scotland Home Affairs Correspondent Share Save Getty\n",
      "Images Pan Am flight 103 was destroyed over Lockerbie on 21 December 1988 The\n",
      "Libyan accused of building the bomb that brought down an American airliner over\n",
      "Lockerbie 36 years ago has claimed he was forced into making a false confession.\n",
      "Abu Agila Mas'ud Kheir Al-Marimi, 74, has said he was in custody in Libya when\n",
      "three masked men ordered him to memorise information about the destruction of\n",
      "Pan Am 103 and another terror attack. Referred to as Mas'ud by prosecutors, he\n",
      "said he repeated what he had learned to a Libyan official, under duress, after\n",
      "the men threatened his family. Masud's lawyers have asked a federal court in\n",
      "Washington to rule the alleged confession inadmissible in advance of his trial\n",
      "in April next year. Details of the alleged confession were first made public\n",
      "five years ago after the US department of justice announced it was charging\n",
      "Mas'ud over the atrocity which claimed the lives of 270 people on 21 December\n",
      "1988. A criminal complaint summary compiled by the FBI claimed he had admitted\n",
      "playing a key role in the attack when he was in detention in 2012, following the\n",
      "collapse of Colonel Gaddafi's regime the previous year. The bureau said Mas'ud\n",
      "had talked of being involved in the plot along with other members of the Libyan\n",
      "intelligence service. Afterwards, he was said to have been congratulated in\n",
      "person by the Libyan dictator, who had told him he had performed \"a great\n",
      "national duty\" against the Americans. Reuters Abu Agila Mas'ud Kheir Al-Marimi\n",
      "has been in custody since December 2022 Of the 259 passengers and crew who died\n",
      "on board the airliner, 190 were from the United States, where the Mas'ud case\n",
      "will be tried. The only Lockerbie trial so far took place between May 2000 and\n",
      "January 2001, when three Scottish judges convicted Libyan intelligence agent\n",
      "Abdulbasset al-Megrahi of playing a key role in the plot. Megrahi was jailed for\n",
      "life but released on compassionate grounds by the Scottish government in 2009\n",
      "after he was diagnosed with terminal cancer. He died three years later at home\n",
      "in Tripoli. With just seven months to go until the scheduled start of Mas'ud's\n",
      "trial, public defenders acting on his behalf have lodged a motion with the\n",
      "district court asking a judge to rule that his confession should not be allowed\n",
      "as evidence. The Libyan has pled not guilty to the charges against him. But the\n",
      "\"motion to suppress\" reveals for the first time his version of what led to the\n",
      "alleged jailhouse confession. Setting the scene, the motion quotes a US\n",
      "Department of State report which said Gaddafi's regime had controlled Libya\n",
      "through extrajudicial killings and intimidation, torture, arbitrary arrest and\n",
      "detention. The document says that after the revolution there was a climate of\n",
      "anger and retaliation against those associated or thought to be associated with\n",
      "Gaddafi. Contemporary reports by US government officials recount more incidents\n",
      "of \"arbitrary and unlawful killings, kidnappings, torture and other cruel and\n",
      "inhuman or degrading treatment.\" The defence lawyers say: \"Just as... a black\n",
      "man accused of killing a white man in Jim Crow-era Arkansas would fear mob\n",
      "violence... so would a Libyan who allegedly worked for Gaddafi have feared\n",
      "retaliation against himself and his family in post-revolution Libya.\" It was\n",
      "against that background, according to Mas'ud, that he was abducted from his home\n",
      "by armed men, separated from his family and his medication, held incommunicado\n",
      "in an unofficial prison facility and denied procedural rights. He says he saw\n",
      "bodies lying in the streets when he was being driven to the prison and while in\n",
      "custody, and encountered other inmates who had been beaten and abused. Mas'ud\n",
      "has told his legal team that he was alone in a small room when three men in\n",
      "civilian clothes came in. They were unarmed but wearing face coverings and did\n",
      "not identify themselves. He says he was certain the men, who handed him the\n",
      "piece of paper, were anti-Gadaffi revolutionaries. \"The single, handwritten\n",
      "sheet began with an order that Mr Al-Marimi confess to the Lockerbie incident,\n",
      "as well as another terrorist attack,\" his defence lawyers claim. The men told\n",
      "him to read over the details and to repeat what it said when he was questioned\n",
      "by someone else the next day. \"They told him he had to answer the questions with\n",
      "what was on the paper, otherwise bad things would happen to him or his family.\n",
      "\"Mr Al-Marimi felt he had no choice but to comply. He had ample reason to fear\n",
      "for himself; before his seizure, he had personally witnessed beatings in others\n",
      "prisons. \"But he was more afraid for his family. He had six children and felt\n",
      "they still had lives left to live. If he resisted, his children could be\n",
      "assaulted or killed. He personally knew about a friend's daughter who had been\n",
      "shot before his abduction.\" AFP via Getty Images Abdelbaset Ali Mohamed al-\n",
      "Megrahi is the only person to have been convicted in connection with the\n",
      "Lockerbie bombing\n",
      "\n",
      "GENERATED HINDI:\n",
      "     74       - \n",
      "     1988    ,   103,    \n",
      "270                \n",
      "             \n",
      "     ,          \n",
      "           2012     \n",
      "           ,   \n",
      "  \"  \"        ,\n",
      "                \n",
      "               \n",
      " -  2001          \n",
      "   ,  \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing Articles:  30%|       | 3/10 [00:18<00:44,  6.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Article 3/10 ---\n",
      "\n",
      "SOURCE:\n",
      "Netanyahu is only obstacle to bringing hostages home, families say 8 hours ago\n",
      "Share Save Yang Tian Share Save Getty Images Families of Israeli hostages still\n",
      "being held by Hamas have said Prime Minister Benjamin Netanyahu is the \"one\n",
      "obstacle\" preventing their return and reaching a peace deal. The Hostages and\n",
      "Missing Families Forum wrote on social media that Israel's strike on Qatar last\n",
      "week shows \"every time a deal approaches, Netanyahu sabotages it\". On Tuesday\n",
      "Israel attacked senior Hamas leaders meeting in a house in the Qatari capital\n",
      "Doha. The Palestinian armed group said five of its members and a Qatari security\n",
      "official were killed. On Sunday US Secretary of State Marco Rubio arrived in\n",
      "Israel for a visit during which he is due to meet Netanyahu as Israel faces\n",
      "international condemnation for the attack. Netanyahu said on Saturday that\n",
      "eliminating Hamas leaders in Qatar \"would rid the main obstacle\" to releasing\n",
      "the hostages and ending the war. He also accused Hamas of blocking all ceasefire\n",
      "attempts in order to drag out the war in Gaza. Hamas members had been in Doha to\n",
      "discuss the latest US proposal for a ceasefire in Gaza. However, families of the\n",
      "hostages described the Israeli PM's response as \"the latest excuse for failing\n",
      "to bring home\" their loved ones. \"The targeted operation in Qatar proved beyond\n",
      "any doubt that there is one obstacle to returning the 48 hostages and ending the\n",
      "war: Prime Minister Netanyahu,\" they said. \"The time has come to end the excuses\n",
      "designed to buy time so he can cling to power.\" The group added that Netanyahu's\n",
      "\"stalling\" had cost \"the lives of 42 hostages and threatens the lives of\n",
      "additional hostages who are barely surviving\". Reuters Rubio arrived in Israel\n",
      "on Sunday morning Before his departure, Rubio said US President Donald Trump was\n",
      "not happy with the strike on Qatar, but stressed that the US-Israeli\n",
      "relationship was \"very strong\". \"Obviously we're not happy about it, the\n",
      "president was not happy about it. Now we need to move forward and figure out\n",
      "what comes next,\" Rubio said. He added that Trump's priority remains the return\n",
      "of all hostages and an end to the the war in Gaza. When asked whether the strike\n",
      "on Doha complicates Qatar's willingness to work with the US, Rubio said \"they've\n",
      "been good partners on a number of fronts\". Qatar is a key US ally in the region\n",
      "and the location of a major American air base. In the wake of the strike, Qatar\n",
      "condemned Israel's attack as \"cowardly\" and a \"flagrant violation of\n",
      "international law\". Netanyahu said the move was \"fully justified\" because it\n",
      "targeted senior Hamas leaders who organised the 7 October 2023 attacks.\n",
      "Meanwhile, Israeli forces have stepped up their assault on Gaza City with a wave\n",
      "of heavy air strikes, reducing entire apartment blocks and large concrete\n",
      "structures to rubble. Israel has also warned all residents in the region to\n",
      "leave immediately in anticipation of a huge ground offensive. Residents said the\n",
      "Israeli military has been targeting schools and makeshift shelters, often\n",
      "issuing warnings only moments before bombardments. On Saturday, the Israel\n",
      "Defense Forces (IDF) said about 250,000 people had left the city and moved\n",
      "south. Netanyahu's plan to occupy Gaza City has drawn international criticism,\n",
      "with the UN warning a military escalation in an area where famine has been\n",
      "declared will push civilians into an \"even deeper catastrophe\". On Sunday, the\n",
      "Hamas-run health ministry in Gaza said the bodies of 68 people killed by the\n",
      "Israeli military had arrived at its hospitals over the previous day. Since UN-\n",
      "backed global food security experts confirmed a famine in Gaza City on 22\n",
      "August, the ministry has reported that at least 144 people have died from\n",
      "starvation and malnutrition across the territory. Israel has said it is\n",
      "expanding its efforts to facilitate aid deliveries and has disputed the health\n",
      "ministry's figures on malnutrition-related deaths. The Israeli military launched\n",
      "a campaign in Gaza in response to the Hamas-led attack on southern Israel on 7\n",
      "October, in which about 1,200 people were killed and 251 others were taken\n",
      "hostage. At least 64,871 people have been killed in Israeli attacks in Gaza\n",
      "since then, according to the territory's Hamas-run health ministry.\n",
      "\n",
      "GENERATED HINDI:\n",
      "              ,\n",
      "             \n",
      "      48       \n",
      "               \"\n",
      "\"        ,    \n",
      "           -  \n",
      "     ,          \n",
      "      ,    PM  \n",
      " \"          \"    \n",
      ",      \"\"  42        \n",
      "      ,        \n",
      "   \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing Articles:  40%|      | 4/10 [00:24<00:35,  5.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Article 4/10 ---\n",
      "\n",
      "SOURCE:\n",
      "William Golding's novel Lord of the Flies was first published on 17 September\n",
      "1954, and is now recognised as a classic. In History looks at how Golding's\n",
      "story of English schoolboys and their descent into barbarism narrowly escaped\n",
      "being thrown in the bin. \"Write what you know\" is advice often given to aspiring\n",
      "authors, and Lord of the Flies is a spectacular example of how clichs can still\n",
      "contain essential truths. A teacher at a boys' school who had witnessed first-\n",
      "hand the inhumanity of World War Two, William Golding condensed this knowledge\n",
      "and experience into his debut novel, a deceptively simple tale of shipwrecked\n",
      "boys reverting to savagery on a desert island. Its subversion of a familiar plot\n",
      "went on to resonate with generations of readers, and serve as a grim warning\n",
      "that the evils of Nazi Germany could be repeated anywhere. Golding was about to\n",
      "turn 43 when Lord of the Flies was first published. His big idea was a sinister\n",
      "20th-Century reimagining of The Coral Island, RM Ballantyne's 1857 tale of\n",
      "derring-do in which a group of shipwrecked British schoolboys civilise a desert\n",
      "island, making it a playground for fun and games. Much of his original\n",
      "manuscript was handwritten on exercise books during school time. He even worked\n",
      "on the novel during lessons, while his pupils were occupied with their\n",
      "textbooks. A few of them were tasked with counting the number of words he'd\n",
      "written per page. WATCH: 'What matters to me is that there shall be a beginning,\n",
      "a middle and an end'. In 1953, Golding sent his novel to nine publishers, all of\n",
      "whom rejected it. Undaunted, he offered the manuscript to Faber and Faber, one\n",
      "of the most prestigious London firms. It was picked up by Charles Monteith, a\n",
      "junior editor who had only worked at the publishing house for a few months. The\n",
      "signs were not promising.\n",
      "\n",
      "GENERATED HINDI:\n",
      "    \"   \" (Lord of the Flies),  17\n",
      " 1954    ,       43   \n",
      "              \n",
      "      '  ' (The Coral Island)  20\n",
      "  - (derring-do)    ,    \n",
      "       ,        \n",
      "               \n",
      "       ,      \n",
      "         1953     \n",
      "   ,         ,   \n",
      "    (Faber and Faber)      \n",
      "     \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing Articles:  50%|     | 5/10 [00:30<00:29,  5.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Article 5/10 ---\n",
      "\n",
      "SOURCE:\n",
      "Ready for an autumn declutter? Four ways to get started 7 hours ago Share Save\n",
      "Alex Taylor BBC News, London Share Save Getty Images After the summer holidays,\n",
      "for many of us September feels like a fresh start. As the darker evenings loom\n",
      "we're likely to spend more time inside making it a natural time to start looking\n",
      "at our homes afresh and to think about decluttering. Tidying our living space\n",
      "can reduce stress and boost our energy levels, according to psychologists. But\n",
      "choosing what to keep and what to get rid of isn't easy, with many of us\n",
      "struggling to decide. BBC Radio 4's Woman's Hour spoke to experts on the four\n",
      "best strategies to clear space in your home. Listen to the BBC Woman's Hour\n",
      "special: Decluttering here 1. Start small Getty Images The biggest challenge for\n",
      "many is starting the process, so it's important to give yourself permission to\n",
      "declutter. This includes throwing away items, including gifts, that have been\n",
      "kept through a sense of obligation rather than enjoyment. Going slowly is also a\n",
      "must, says writer and Interior Design Masters judge Michelle Ogundehin. \"It's\n",
      "not about thinking 'right, that's it! It's all got to go!' It's about taking it\n",
      "one step at a time,\" she explains. Ogundehin suggests picking an individual\n",
      "drawer or cupboard first, then building up gradually over time. This makes the\n",
      "task feel manageable rather than overwhelming. 2. Plan where to take your items\n",
      "Getty Images It is essential to plan ahead for what will happen to the items you\n",
      "discard. Ideally this should be before decluttering begins, or within the first\n",
      "few days. \"You don't want to have piles and piles of stuff building up in the\n",
      "hallway,\" says professional organiser Ingrid Jansen. As the co-founder of\n",
      "Declutter Hub, a community of over 60,000 members alongside a weekly podcast,\n",
      "she says there are ever increasing options for items to be donated, recycled,\n",
      "given away or disposed of. Consider charity shops, or giveaway apps like Freegle\n",
      "and Freecycle, or the tip for items which can't be reused. There's also the\n",
      "option to sell items second hand through online marketplaces such as Vinted or\n",
      "eBay. But, fellow Declutter Hub co-founder Lesley Spellman warns that the lure\n",
      "of upselling can be a double-edged sword. Despite our best intentions, it can\n",
      "lead to churning - the habit of packing items for disposal, without actually\n",
      "moving them on. \"Selling is a fantastic way to make money out of your clutter,\n",
      "but you have to exercise some realism,\" she says. If you're that person that's\n",
      "got three bin bags of stuff still under your desk, it may be time to get rid. 3.\n",
      "Prioritise quality not quantity Getty Images We've all heard the phrase \"be\n",
      "brutal!\", but effective decluttering doesn't mean discarding things you love for\n",
      "the sake of it. Instead, Ogundehin advises: \"surround yourself with the things\n",
      "that tell the story that you want them to tell\". \"Keep the things that reinforce\n",
      "you, that uplift you when you walk through the door,\" she adds, be that holiday\n",
      "purchases, pictures, or masterpieces by your children, nephews or nieces. Key to\n",
      "managing this is employing what Ogundehin terms \"containment\" - dedicating set\n",
      "spaces for objects, rather than spreading them endlessly throughout your home.\n",
      "4. Distinguish between nostalgia and sentimentality Getty Images\n",
      "\n",
      "GENERATED HINDI:\n",
      "  4  \" \" ,      \n",
      "              \n",
      "             ,\n",
      "   (   )    -  \n",
      "      ,    / \n",
      "            \n",
      "              \n",
      " , ,          ,\n",
      "   -       , \n",
      "              \n",
      "     ,   \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing Articles:  60%|    | 6/10 [00:35<00:23,  5.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Article 6/10 ---\n",
      "\n",
      "SOURCE:\n",
      "The Cook Islands is proving that sustainable tourism isn't just possible  it's\n",
      "essential. Here's how this South Pacific nation is preserving their paradise for\n",
      "generations for come. Landing on Rarotonga, the largest of the Cook Islands\n",
      "chain felt like stepping back in time. Gazing out of the taxi from Rarotonga\n",
      "airport to our resort, we were immediately struck by the absence of high-rise\n",
      "hotels, fast-food restaurants and corporate chains. There were no traffic\n",
      "lights, only coconut palms lining the road, the scent of salt and frangipani\n",
      "drifting through the air and the jungle meeting the ocean in a seamless\n",
      "panorama. It felt like Hawaii in the 1960s: uncrowded, laid-back and\n",
      "refreshingly authentic. Our taxi driver pointed to a low-slung resort along the\n",
      "shoreline. \"No building can be higher than a coconut tree,\" she said. This isn't\n",
      "just a local tradition but a law set in 1965 by the Cook Islands' first premier,\n",
      "Albert Henry, to prevent overdevelopment. She explained that only Cook Islanders\n",
      "can own land, ensuring that large corporations don't dominate the landscape. We\n",
      "looked out onto hotels blending naturally into their surroundings and white-sand\n",
      "beaches ringed with long green parks, all free from litter and crowds. We soon\n",
      "learned that this preservation of paradise is deeply intentional. Cook Islanders\n",
      "have made a conscious effort to ensure that Rarotonga never follows the path of\n",
      "overdevelopment seen in places like Honolulu. Instead, locals have committed to\n",
      "conservation, low-impact tourism and sustainable practices that benefit both\n",
      "locals and visitors. \"People come here because it is a paradise uncluttered by\n",
      "overdevelopment,\" explained Jeremy Goodwin, regenerative tourism manager for the\n",
      "Cook Islands Tourism Corporation (CITC). \"Our sacred duty as custodians of the\n",
      "land is to look after our paradise.\"\n",
      "\n",
      "GENERATED HINDI:\n",
      "  (Cook Islands),    (South Pacific)   \n",
      "  ,  (Rarotonga)       \n",
      "     , , -     \n",
      "    1960          1965\n",
      "            \n",
      "          ,     \n",
      "             \n",
      "     (Jeremy Goodwin)       \n",
      "   , -      , \n",
      "          , \n",
      "        , \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing Articles:  70%|   | 7/10 [00:39<00:15,  5.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Article 7/10 ---\n",
      "\n",
      "SOURCE:\n",
      "Late-night host Jimmy Kimmel has been pulled off air \"indefinitely\" by ABC over\n",
      "comments he made about the shooting of right-wing influencer Charlie Kirk. In\n",
      "his Monday night monologue, Kimmel said the \"MAGA gang\" was trying to score\n",
      "political points off Kirk's killing, and mocked the president's reaction to the\n",
      "shooting. The BBC's Regan Morris spoke to fans outside Jimmy Kimmel Live's LA\n",
      "studio, who called the cancellation \"bizarre\" and \"the ultimate in cancel\n",
      "culture.\" Read more on this story.\n",
      "\n",
      "GENERATED HINDI:\n",
      "-         monologue     \n",
      "       \"  \"    \n",
      "      \" \"        \n",
      "       ,        \n",
      "             \"\" \n",
      "  \"  \"          LA\n",
      "     \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing Articles:  80%|  | 8/10 [00:44<00:10,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Article 8/10 ---\n",
      "\n",
      "SOURCE:\n",
      "The UK entered WW2 on 3 September, 1939. Eighty years after the war's end, few\n",
      "people know that a British island's unique language was used as a clandestine\n",
      "code during Nazi occupation \"Bonsouair! Seyiz les beinv'nus!\" announced Jo\n",
      "Thorpe, as she stepped up to the microphone and spread her arms in welcome. \"Or\n",
      "for anyone who's not from Jersey, good evening and welcome! We're thrilled to\n",
      "see you tonight, and we hope you enjoy the show.\" The lights dimmed, and her\n",
      "band, Sonneux, started playing. Accompanied by violin, double bass, guitar and\n",
      "recorder, Thorpe began to sing a familiar folk melody  only there was something\n",
      "surprising about her tune. Though it sounded like she was singing in French, in\n",
      "fact, she was singing in another language entirely: Jrriais, Jersey's native\n",
      "language. \"In Jersey, we call this a veil'ye,\" she explained during a mid-show\n",
      "break, handing me a bowl of bean crock, a traditional, hearty island stew. \"It's\n",
      "an evening of singing, music and storytelling, where people read, perform and\n",
      "share memories. Before radio and TV, a veil'ye was how people socialised. Every\n",
      "village had them. They were part of life. But the tradition died out, so we\n",
      "decided to bring them back. And of course, we couldn't do that without\n",
      "Jrriais.\" Jersey is the largest of the Channel Islands, an archipelago of six\n",
      "inhabited islands (and many more uninhabited ones) scattered over the English\n",
      "Channel, just over 14 miles from the French coast. Despite its proximity to\n",
      "France, it's actually a British Crown Dependency, and as such, the island has\n",
      "two official languages: English and French. But it also has Jrriais.\n",
      "\n",
      "GENERATED HINDI:\n",
      "3 , 1939       (WW2)      \n",
      ",      \"!   !\"   \n",
      "      ,    , \n",
      ",  ,      ,      \n",
      ", , ,     ''      \n",
      "               \n",
      "    ,   -  ,   \n",
      "               \n",
      "        ,      \n",
      "       ,     14  \n",
      "   \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing Articles:  90%| | 9/10 [00:47<00:04,  4.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Article 9/10 ---\n",
      "\n",
      "SOURCE:\n",
      "On January 23rd, just three days after John. F. Kennedy delivered his inaugural\n",
      "speech as the 35th President of the United States, one little known event could\n",
      "have changed American history in the most catastrophic way imaginable. A\n",
      "refuelling accident caused a B-52 bomber to break apart above a farm in\n",
      "Goldsboro, North Carolina, causing two 3.8 megaton nuclear bombs to plummet to\n",
      "the ground. Bar one small safety switch and a huge amount of luck, millions\n",
      "could have been killed. Accidents like this are known as 'Broken Arrows' and\n",
      "they have happened more than many people realise.\n",
      "\n",
      "GENERATED HINDI:\n",
      "23  ,  .   35        \n",
      " ,            \n",
      "     3.8          ,\n",
      "          ' '   , \n",
      "                \n",
      "                \n",
      "       \n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Summarizing Articles: 100%|| 10/10 [00:53<00:00,  5.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "--- Article 10/10 ---\n",
      "\n",
      "SOURCE:\n",
      "Malawi - where the petrol queue might overshadow the queue to vote 19 hours ago\n",
      "Share Save Nomsa Maseko BBC Africa, Lilongwe and Mangochi Share Save BBC The\n",
      "queue to get fuel rather than the queue to vote is what is on the mind of many\n",
      "Malawians as Tuesday's general election approaches. Prolonged petrol shortages,\n",
      "along with regular power cuts, the rising cost of living, hunger, poverty,\n",
      "inequality and youth unemployment, add to the tangible frustration here. The\n",
      "presidential, parliamentary and local council candidates are competing for votes\n",
      "against a background of cynicism about what might actually change. In a sign\n",
      "that money is tight, electioneering has been somewhat muted compared to the\n",
      "past. This is despite the presidential race being seen as a rematch between the\n",
      "incumbent, Lazarus Chakwera, and the man he beat in 2020, then-President Peter\n",
      "Mutharika. There are 15 other candidates. But the usual colourful campaign\n",
      "carnival is missing. The free T-shirts usually doled out with abandon to whip up\n",
      "enthusiasm are more limited. There are fewer giant election billboards on the\n",
      "nation's main roads. Back in the snaking petrol lines, patience runs thin, which\n",
      "has at times led to fist fights. BBC / AFP via Getty Images Peter Mutharika (L)\n",
      "and Lazarus Chakwera (R) have been political rivals for the past decade Sensing\n",
      "the fuel shortage was becoming an election issue, Chakwera has tried to tackle\n",
      "it head on. In a televised address, eight days before polls open, he\n",
      "acknowledged the frustration and apologised. The president then turned his fire\n",
      "on allegedly corrupt officials who he accused of deliberately sabotaging the oil\n",
      "market. Like fuel, new job opportunities are also hard to come by. To put food\n",
      "on the table, young men have been selling petrol and diesel using small plastic\n",
      "containers at five times the official price. In the southern town of Mangochi,\n",
      "they refused to be interviewed except to say, as they walked away, that preying\n",
      "on desperate motorists was the only way to survive. With food costs rising at\n",
      "more than 30% in the past year, and wages not keeping pace, things are becoming\n",
      "harder to afford. The high inflation rate has been partly put down to a shortage\n",
      "of foreign currency, which has forced some importers to buy US dollars on the\n",
      "more expensive black market. The costs have then been passed on to the consumer.\n",
      "The effect of the economic troubles on young people could be particularly\n",
      "significant in this election  as around half of registered voters are under the\n",
      "age of 35. And yet the two leading presidential candidates are considerably\n",
      "older. Chakwera is 70 and Mutharika is 85. \"When young people cast their ballots\n",
      "next week, they should think about the poverty crisis. The coming president\n",
      "should fix the employment rate because many of the young people are unemployed,\"\n",
      "said 33-year-old Monica Chinoko, who works in the capital, Lilongwe. Many\n",
      "younger voters have told the BBC that these continuous problems have dampened\n",
      "enthusiasm for the elections. \"Looking at the candidates - it's really a tough\n",
      "choice to make because hope has been lost. We've been voting and voting but\n",
      "things haven't gotten better,\" said Ashley Phiri, 35. \"But I'm hoping that this\n",
      "time around, the next leader will radically transform Malawi.\" Supporters of\n",
      "opposition candidate Peter Mutharika argue that things were better when he was\n",
      "in State House Mutharika's election convoy has made several stops in the\n",
      "villages along the Bakili Muluzi highway. In one place, a supporter held up a\n",
      "sign saying \"back to state house\" and said life was better when the former\n",
      "president was in office. At a Mutharika rally in Machinga, an elderly woman\n",
      "wearing a colourful headscarf and sarong held up a huge bucket and shouted\n",
      "\"fertiliser\". What is at stake in Malawi's elections? 'I tell my children not to\n",
      "play so we save money on soap' She was highlighting the crucial issue for the\n",
      "80% of the population who live in rural areas. Many of these people survive on\n",
      "what they grow on their smallholdings and make money from what is left over.\n",
      "Chakwera had promised to reduce the cost of the vital farming input  but the\n",
      "price has gone in the opposite direction. It is now six times more than it was\n",
      "in 2019. The president has \"accused some opposition parties of working with\n",
      "private traders to distort fertiliser prices\", his office said. He has pledged\n",
      "to smallholder farmers that the price will come back down under a targeted\n",
      "programme due to start next month. Supporters of Lazarus Chakwera are confident\n",
      "he will be re-elected Chakwera has had a tough five years at the helm but\n",
      "remains optimistic. He says he is investing in the future of the country and as\n",
      "a headline policy he has pledged that the state will deposit 500,000 Malawi\n",
      "kwacha ($290; 210) in individual accounts for every child born after the\n",
      "general election. They will be able to access it once they reach 18. Another\n",
      "former president, Joyce Banda  the country's only female head of state  is\n",
      "also running again. She has pledged to fight corruption, transform the economy\n",
      "and improve rural infrastructure. The other presidential candidates, including\n",
      "Atupele Muluzi, Dalitso Kabambe and current Vice-President Michael Usi, have all\n",
      "promised radical change in one of the world's poorest countries. There is no\n",
      "shortage of choice on the ballot paper, but Malawians will be hoping that\n",
      "whoever emerges as the winner  after Tuesday's vote or a possible second round\n",
      " will be able to put more food on the table and more fuel in the tank. More BBC\n",
      "stories about Malawi: Getty Images/BBC\n",
      "\n",
      "GENERATED HINDI:\n",
      "Malawi          ,    \n",
      "  ,   , ,       \n",
      "         \n",
      "         ,  15 \n",
      "              \n",
      "  ,           , \n",
      "           ,   \n",
      " 30%     ,      ,     \n",
      "              ,\n",
      "   70%     35         \n",
      "    ,  Ashley Phiri  Monica Chinoko  ,\n",
      "             \n",
      "================================================================================\n",
      "\n",
      "Successfully saved full results to: mbart-large-50-cnn-summarizer-v15\\bulk_test_results_interactive_2025-11-14_08-52-03.csv\n",
      "--- Bulk test finished successfully ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "import pandas as pd\n",
    "import textwrap\n",
    "import os\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import sys \n",
    "\n",
    "import config\n",
    "import data_utils\n",
    "import logging_utils\n",
    "\n",
    "# --- Configuration ---\n",
    "DATA_PATH = r\"H:\\News_Summarization\\Dataset\\filtered_articles_CNN.csv\" \n",
    "NUM_SAMPLES = 10\n",
    "# ----------------------------------------------\n",
    "\n",
    "model = None\n",
    "tokenizer = None\n",
    "\n",
    "def load_model():\n",
    "    \"\"\"Loads the final v15 model and tokenizer.\"\"\"\n",
    "    global model, tokenizer\n",
    "    \n",
    "    if not os.path.exists(config.FINAL_SAVE_PATH):\n",
    "        logging.error(f\"Model path not found: {config.FINAL_SAVE_PATH}\")\n",
    "        print(f\"Error: Model path not found: {config.FINAL_SAVE_PATH}\")\n",
    "        return False\n",
    "\n",
    "    logging.info(f\"Loading final model from: {config.FINAL_SAVE_PATH}...\")\n",
    "    try:\n",
    "        model = MBartForConditionalGeneration.from_pretrained(config.FINAL_SAVE_PATH).to(config.DEVICE)\n",
    "        tokenizer = MBart50TokenizerFast.from_pretrained(config.FINAL_SAVE_PATH)\n",
    "        model.eval()\n",
    "        logging.info(f\"Model v15 loaded successfully on {config.DEVICE}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading model: {e}\", exc_info=True)\n",
    "        return False\n",
    "\n",
    "def load_test_data(num_samples=None):\n",
    "    \"\"\"Loads, cleans, and subsets the test data from the CSV.\"\"\"\n",
    "    logging.info(f\"Loading test data from: {DATA_PATH}\")\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_csv(DATA_PATH, engine='python', on_bad_lines='skip')\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Data file not found at: {DATA_PATH}\")\n",
    "        print(f\"Error: Data file not found at: {DATA_PATH}\")\n",
    "        return None\n",
    "        \n",
    "    df.dropna(subset=['raw_news_article', 'hindi_summary'], inplace=True)\n",
    "    \n",
    "    # Clean the text\n",
    "    df['raw_news_article'] = df['raw_news_article'].apply(data_utils.sanitize_text).apply(data_utils.normalize_text)\n",
    "    df['hindi_summary'] = df['hindi_summary'].apply(data_utils.sanitize_text).apply(data_utils.normalize_text)\n",
    "    \n",
    "    if num_samples and num_samples > 0:\n",
    "        if num_samples > len(df):\n",
    "             logging.warning(f\"Requested {num_samples} samples, but only {len(df)} are available. Using all articles.\")\n",
    "             num_samples = len(df)\n",
    "        \n",
    "        logging.warning(f\"Selecting a random subset of {num_samples} articles for testing.\")\n",
    "        df = df.sample(n=num_samples, random_state=42) \n",
    "        \n",
    "    else:\n",
    "        logging.info(f\"Processing all {len(df)} articles found in the file.\")\n",
    "        \n",
    "    return df\n",
    "\n",
    "def run_inference_and_log(df):\n",
    "    \"\"\"\n",
    "    Generates summaries for the dataframe, printing results in real-time.\n",
    "    Returns a list of generated summaries.\n",
    "    \"\"\"\n",
    "    generated_summaries = []\n",
    "    \n",
    "    batch_size = 1 \n",
    "    \n",
    "    gen_kwargs = {\n",
    "        \"num_beams\": 12,\n",
    "        \"length_penalty\": 2.0,\n",
    "        \"repetition_penalty\": 2.5,\n",
    "        \"no_repeat_ngram_size\": 3,\n",
    "        \"do_sample\": False,\n",
    "        \"early_stopping\": True,\n",
    "        \"min_length\": 30,\n",
    "        \"max_length\": 250,\n",
    "    }\n",
    "\n",
    "    logging.info(f\"Starting inference for {len(df)} articles...\")\n",
    "    \n",
    "    df = df.reset_index(drop=True) \n",
    "    \n",
    "    for index, row in tqdm(df.iterrows(), total=len(df), desc=\"Summarizing Articles\"):\n",
    "        article = row['raw_news_article']\n",
    "        \n",
    "        tokenizer.src_lang = \"en_XX\"\n",
    "        inputs = tokenizer(\n",
    "            article, \n",
    "            return_tensors=\"pt\", \n",
    "            max_length=1024, \n",
    "            truncation=True, \n",
    "            padding=True\n",
    "        ).to(config.DEVICE)\n",
    "\n",
    "        summary_ids = model.generate(\n",
    "            inputs.input_ids,\n",
    "            forced_bos_token_id=tokenizer.lang_code_to_id[\"hi_IN\"],\n",
    "            **gen_kwargs\n",
    "        )\n",
    "        \n",
    "        summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        generated_summaries.append(summary)\n",
    "        \n",
    "        log_output_console = (\n",
    "            f\"\\n\\n--- Article {index+1}/{len(df)} ---\\n\"\n",
    "            f\"\\nSOURCE:\\n{textwrap.fill(article, width=80)}\\n\"\n",
    "            f\"\\nGENERATED HINDI:\\n{textwrap.fill(summary, width=80)}\\n\"\n",
    "            f\"{'='*80}\"\n",
    "        )\n",
    "        \n",
    "        print(\"\\n\\n\" + log_output_console)\n",
    "        \n",
    "        log_output_file = (\n",
    "            f\"\\n\\n--- Article {index+1}/{len(df)} (Original Index: {row.name}) ---\\n\"\n",
    "            f\"\\nSOURCE:\\n{textwrap.fill(article, width=80)}\\n\"\n",
    "            f\"\\nGENERATED HINDI:\\n{textwrap.fill(summary, width=80)}\\n\"\n",
    "            f\"\\nREFERENCE HINDI:\\n{textwrap.fill(row['hindi_summary'], width=80)}\\n\"\n",
    "            f\"{'='*80}\"\n",
    "        )\n",
    "        logging.info(log_output_file)\n",
    "        \n",
    "    return generated_summaries\n",
    "\n",
    "def save_results(df, generated_summaries):\n",
    "    \"\"\"Saves the final results to a CSV file.\"\"\"\n",
    "    df['generated_hindi_summary'] = generated_summaries\n",
    "    \n",
    "    output_df = df[['raw_news_article', 'hindi_summary', 'generated_hindi_summary']]\n",
    "    \n",
    "    output_filename = os.path.join(\n",
    "        config.MODEL_OUTPUT_DIR, \n",
    "        f\"bulk_test_results_interactive_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.csv\"\n",
    "    )\n",
    "    \n",
    "    output_df.to_csv(output_filename, index=False, encoding='utf-8-sig')\n",
    "    logging.info(f\"Successfully saved full results to: {output_filename}\")\n",
    "    print(f\"\\nSuccessfully saved full results to: {output_filename}\")\n",
    "\n",
    "def main():\n",
    "    log_path = logging_utils.setup_logging(config.MODEL_OUTPUT_DIR, \"bulk_test_interactive_log_v15\")\n",
    "    print(f\"--- Starting Bulk Test Script ---\")\n",
    "    print(f\"Logging all output to: {log_path}\\n\")\n",
    "    logging.info(\"--- Starting Bulk Test Script ---\")\n",
    "    \n",
    "    if load_model():\n",
    "        test_data = load_test_data(NUM_SAMPLES)\n",
    "        if test_data is not None:\n",
    "            generated_summaries = run_inference_and_log(test_data)\n",
    "            save_results(test_data, generated_summaries)\n",
    "            logging.info(\"--- Bulk test finished successfully ---\")\n",
    "            print(\"--- Bulk test finished successfully ---\")\n",
    "        else:\n",
    "            logging.error(\"Failed to load test data. Aborting.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except KeyboardInterrupt:\n",
    "        logging.warning(\"--- Bulk Test Interrupted by User ---\")\n",
    "        print(\"\\n\\n--- Bulk Test Interrupted by User ---\")\n",
    "        try:\n",
    "            sys.exit(0)\n",
    "        except SystemExit:\n",
    "            os._exit(0)\n",
    "    except Exception as e:\n",
    "        logging.error(\"--- Bulk Test FAILED ---\", exc_info=True)\n",
    "        print(f\"\\n--- Bulk Test FAILED --- \\nError: {e}\\nSee log file for details.\")\n",
    "        try:\n",
    "            sys.exit(1)\n",
    "        except SystemExit:\n",
    "            os._exit(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summarizer_env3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
