2025-11-18 21:37:55,546 [INFO] - root - --- Starting mBART-ViT v16 Training ---
2025-11-18 21:37:55,546 [INFO] - root - Loading tokenizer from: ../finetune_v15/mbart-large-50-cnn-summarizer-v15_fine_tuned/final_model
2025-11-18 21:37:55,559 [INFO] - transformers.tokenization_utils_base - loading file sentencepiece.bpe.model
2025-11-18 21:37:55,559 [INFO] - transformers.tokenization_utils_base - loading file tokenizer.json
2025-11-18 21:37:55,559 [INFO] - transformers.tokenization_utils_base - loading file added_tokens.json
2025-11-18 21:37:55,559 [INFO] - transformers.tokenization_utils_base - loading file special_tokens_map.json
2025-11-18 21:37:55,559 [INFO] - transformers.tokenization_utils_base - loading file tokenizer_config.json
2025-11-18 21:37:55,559 [INFO] - transformers.tokenization_utils_base - loading file chat_template.jinja
2025-11-18 21:37:56,239 [INFO] - root - Loading feature extractor from: google/vit-base-patch16-224-in21k
2025-11-18 21:37:58,082 [INFO] - transformers.image_processing_base - loading configuration file preprocessor_config.json from cache at C:\Users\admin\.cache\huggingface\hub\models--google--vit-base-patch16-224-in21k\snapshots\b4569560a39a0f1af58e3ddaf17facf20ab919b0\preprocessor_config.json
2025-11-18 21:37:58,082 [INFO] - transformers.image_processing_utils - size should be a dictionary on of the following set of keys: ({'width', 'height'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}, {'longest_edge'}, {'max_width', 'max_height'}), got 224. Converted to {'height': 224, 'width': 224}.
2025-11-18 21:37:58,082 [INFO] - transformers.image_processing_base - Image processor ViTFeatureExtractor {
  "do_convert_rgb": null,
  "do_normalize": true,
  "do_rescale": true,
  "do_resize": true,
  "image_mean": [
    0.5,
    0.5,
    0.5
  ],
  "image_processor_type": "ViTFeatureExtractor",
  "image_std": [
    0.5,
    0.5,
    0.5
  ],
  "resample": 2,
  "rescale_factor": 0.00392156862745098,
  "size": {
    "height": 224,
    "width": 224
  }
}

2025-11-18 21:37:58,086 [INFO] - root - Loading training data from: H:\News_Summarization\Dataset\News_Articles_with_Images\balanced_50k_dataset_sets\set_1\multimodal_dataset_set1.parquet
2025-11-18 21:37:58,514 [INFO] - root - Training data loaded: 49951 samples
2025-11-18 21:37:58,514 [INFO] - root - Loading evaluation data from: H:\News_Summarization\Dataset\News_Articles_with_Images\balanced_50k_dataset_sets\set_2\multimodal_dataset_set2.parquet
2025-11-18 21:37:58,958 [INFO] - root - Using a random subset of 800 samples.
2025-11-18 21:37:58,973 [INFO] - root - Evaluation data loaded: 800 samples
2025-11-18 21:37:58,973 [INFO] - root - Loading custom v16 multimodal model architecture...
2025-11-18 21:37:58,980 [INFO] - transformers.configuration_utils - loading configuration file ../finetune_v15/mbart-large-50-cnn-summarizer-v15_fine_tuned/final_model\config.json
2025-11-18 21:37:58,982 [INFO] - transformers.configuration_utils - Model config MBartConfig {
  "_num_labels": 3,
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": true,
  "architectures": [
    "MBartForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "classifier_dropout": 0.0,
  "d_model": 1024,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 12,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "dtype": "float32",
  "early_stopping": null,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 12,
  "eos_token_id": 2,
  "forced_eos_token_id": 2,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_length": null,
  "max_position_embeddings": 1024,
  "model_type": "mbart",
  "normalize_before": true,
  "normalize_embedding": true,
  "num_beams": null,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "scale_embedding": true,
  "static_position_embeddings": false,
  "tokenizer_class": "MBart50Tokenizer",
  "transformers_version": "4.57.0",
  "use_cache": true,
  "vocab_size": 250054
}

2025-11-18 21:37:58,982 [INFO] - transformers.modeling_utils - loading weights file ../finetune_v15/mbart-large-50-cnn-summarizer-v15_fine_tuned/final_model\model.safetensors
2025-11-18 21:37:58,988 [INFO] - transformers.generation.configuration_utils - Generate config GenerationConfig {
  "bos_token_id": 0,
  "decoder_start_token_id": 2,
  "eos_token_id": 2,
  "forced_eos_token_id": 2,
  "pad_token_id": 1
}

2025-11-18 21:37:59,338 [INFO] - transformers.generation.configuration_utils - loading configuration file ../finetune_v15/mbart-large-50-cnn-summarizer-v15_fine_tuned/final_model\generation_config.json
2025-11-18 21:37:59,338 [INFO] - transformers.generation.configuration_utils - Generate config GenerationConfig {
  "bos_token_id": 0,
  "decoder_start_token_id": 2,
  "early_stopping": true,
  "eos_token_id": [
    2
  ],
  "forced_eos_token_id": 2,
  "max_length": 200,
  "num_beams": 5,
  "pad_token_id": 1
}

2025-11-18 21:37:59,341 [INFO] - transformers.dynamic_module_utils - Could not locate the custom_generate/generate.py inside ../finetune_v15/mbart-large-50-cnn-summarizer-v15_fine_tuned/final_model.
2025-11-18 21:37:59,341 [INFO] - transformers.generation.configuration_utils - Generate config GenerationConfig {
  "bos_token_id": 0,
  "decoder_start_token_id": 2,
  "eos_token_id": 2,
  "forced_eos_token_id": 2,
  "pad_token_id": 1
}

2025-11-18 21:38:15,242 [INFO] - transformers.configuration_utils - loading configuration file ../finetune_v15/mbart-large-50-cnn-summarizer-v15_fine_tuned/final_model\config.json
2025-11-18 21:38:15,242 [INFO] - transformers.configuration_utils - Model config MBartConfig {
  "_num_labels": 3,
  "activation_dropout": 0.0,
  "activation_function": "gelu",
  "add_bias_logits": false,
  "add_final_layer_norm": true,
  "architectures": [
    "MBartForConditionalGeneration"
  ],
  "attention_dropout": 0.0,
  "bos_token_id": 0,
  "classif_dropout": 0.0,
  "classifier_dropout": 0.0,
  "d_model": 1024,
  "decoder_attention_heads": 16,
  "decoder_ffn_dim": 4096,
  "decoder_layerdrop": 0.0,
  "decoder_layers": 12,
  "decoder_start_token_id": 2,
  "dropout": 0.1,
  "dtype": "float32",
  "early_stopping": null,
  "encoder_attention_heads": 16,
  "encoder_ffn_dim": 4096,
  "encoder_layerdrop": 0.0,
  "encoder_layers": 12,
  "eos_token_id": 2,
  "forced_eos_token_id": 2,
  "gradient_checkpointing": false,
  "id2label": {
    "0": "LABEL_0",
    "1": "LABEL_1",
    "2": "LABEL_2"
  },
  "init_std": 0.02,
  "is_encoder_decoder": true,
  "label2id": {
    "LABEL_0": 0,
    "LABEL_1": 1,
    "LABEL_2": 2
  },
  "max_length": null,
  "max_position_embeddings": 1024,
  "model_type": "mbart",
  "normalize_before": true,
  "normalize_embedding": true,
  "num_beams": null,
  "num_hidden_layers": 12,
  "output_past": true,
  "pad_token_id": 1,
  "scale_embedding": true,
  "static_position_embeddings": false,
  "tokenizer_class": "MBart50Tokenizer",
  "transformers_version": "4.57.0",
  "use_cache": true,
  "vocab_size": 250054
}

2025-11-18 21:38:15,242 [INFO] - transformers.modeling_utils - loading weights file ../finetune_v15/mbart-large-50-cnn-summarizer-v15_fine_tuned/final_model\model.safetensors
2025-11-18 21:38:15,247 [INFO] - transformers.generation.configuration_utils - Generate config GenerationConfig {
  "bos_token_id": 0,
  "decoder_start_token_id": 2,
  "eos_token_id": 2,
  "forced_eos_token_id": 2,
  "pad_token_id": 1
}

2025-11-18 21:38:15,663 [INFO] - transformers.generation.configuration_utils - loading configuration file ../finetune_v15/mbart-large-50-cnn-summarizer-v15_fine_tuned/final_model\generation_config.json
2025-11-18 21:38:15,666 [INFO] - transformers.generation.configuration_utils - Generate config GenerationConfig {
  "bos_token_id": 0,
  "decoder_start_token_id": 2,
  "early_stopping": true,
  "eos_token_id": [
    2
  ],
  "forced_eos_token_id": 2,
  "max_length": 200,
  "num_beams": 5,
  "pad_token_id": 1
}

2025-11-18 21:38:15,668 [INFO] - transformers.dynamic_module_utils - Could not locate the custom_generate/generate.py inside ../finetune_v15/mbart-large-50-cnn-summarizer-v15_fine_tuned/final_model.
2025-11-18 21:38:16,776 [INFO] - transformers.configuration_utils - loading configuration file config.json from cache at C:\Users\admin\.cache\huggingface\hub\models--google--vit-base-patch16-224-in21k\snapshots\b4569560a39a0f1af58e3ddaf17facf20ab919b0\config.json
2025-11-18 21:38:16,778 [INFO] - transformers.configuration_utils - Model config ViTConfig {
  "architectures": [
    "ViTModel"
  ],
  "attention_probs_dropout_prob": 0.0,
  "encoder_stride": 16,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.0,
  "hidden_size": 768,
  "image_size": 224,
  "initializer_range": 0.02,
  "intermediate_size": 3072,
  "layer_norm_eps": 1e-12,
  "model_type": "vit",
  "num_attention_heads": 12,
  "num_channels": 3,
  "num_hidden_layers": 12,
  "patch_size": 16,
  "pooler_act": "tanh",
  "pooler_output_size": 768,
  "qkv_bias": true,
  "transformers_version": "4.57.0"
}

2025-11-18 21:38:16,778 [INFO] - transformers.modeling_utils - loading weights file model.safetensors from cache at C:\Users\admin\.cache\huggingface\hub\models--google--vit-base-patch16-224-in21k\snapshots\b4569560a39a0f1af58e3ddaf17facf20ab919b0\model.safetensors
2025-11-18 21:38:17,128 [INFO] - root - Freezing mBART encoder and ViT parameters...
2025-11-18 21:38:17,128 [INFO] - transformers.generation.configuration_utils - loading configuration file ../finetune_v15/mbart-large-50-cnn-summarizer-v15_fine_tuned/final_model\generation_config.json
2025-11-18 21:38:17,128 [INFO] - transformers.generation.configuration_utils - Generate config GenerationConfig {
  "bos_token_id": 0,
  "decoder_start_token_id": 2,
  "early_stopping": true,
  "eos_token_id": [
    2
  ],
  "forced_eos_token_id": 2,
  "max_length": 200,
  "num_beams": 5,
  "pad_token_id": 1
}

2025-11-18 21:38:17,128 [INFO] - root - Model loaded successfully.
2025-11-18 21:38:17,128 [INFO] - root - Configuring training arguments...
2025-11-18 21:38:17,128 [INFO] - transformers.training_args - PyTorch: setting up devices
2025-11-18 21:38:19,902 [INFO] - transformers.trainer - Using auto half precision backend
2025-11-18 21:38:19,912 [INFO] - root - --- Performing Pre-Training Sanity Check ---
2025-11-18 21:38:20,433 [INFO] - root - Sanity Check Passed: Model forward pass successful.
2025-11-18 21:38:20,433 [INFO] - root - --- v16 Training started for 3 epochs ---
2025-11-18 21:38:20,903 [INFO] - transformers.trainer - ***** Running training *****
2025-11-18 21:38:20,904 [INFO] - transformers.trainer -   Num examples = 49,951
2025-11-18 21:38:20,904 [INFO] - transformers.trainer -   Num Epochs = 3
2025-11-18 21:38:20,904 [INFO] - transformers.trainer -   Instantaneous batch size per device = 4
2025-11-18 21:38:20,904 [INFO] - transformers.trainer -   Total train batch size (w. parallel, distributed & accumulation) = 32
2025-11-18 21:38:20,904 [INFO] - transformers.trainer -   Gradient Accumulation steps = 8
2025-11-18 21:38:20,904 [INFO] - transformers.trainer -   Total optimization steps = 4,683
2025-11-18 21:38:20,905 [INFO] - transformers.trainer -   Number of trainable parameters = 203,402,240
2025-11-18 22:57:57,755 [INFO] - transformers.trainer - 
***** Running Evaluation *****
2025-11-18 22:57:57,755 [INFO] - transformers.trainer -   Num examples = 800
2025-11-18 22:57:57,755 [INFO] - transformers.trainer -   Batch size = 4
2025-11-18 23:23:14,171 [INFO] - absl - Using default tokenizer.
2025-11-18 23:40:03,716 [INFO] - transformers.trainer - Saving model checkpoint to mbart-vit-summarizer-v16\checkpoint-1561
2025-11-18 23:40:03,728 [INFO] - transformers.configuration_utils - Configuration saved in mbart-vit-summarizer-v16\checkpoint-1561\config.json
2025-11-18 23:40:03,733 [INFO] - transformers.generation.configuration_utils - Configuration saved in mbart-vit-summarizer-v16\checkpoint-1561\generation_config.json
2025-11-18 23:40:42,762 [INFO] - transformers.modeling_utils - Model weights saved in mbart-vit-summarizer-v16\checkpoint-1561\model.safetensors
2025-11-18 23:40:42,767 [INFO] - transformers.tokenization_utils_base - tokenizer config file saved in mbart-vit-summarizer-v16\checkpoint-1561\tokenizer_config.json
2025-11-18 23:40:42,775 [INFO] - transformers.tokenization_utils_base - Special tokens file saved in mbart-vit-summarizer-v16\checkpoint-1561\special_tokens_map.json
2025-11-19 00:58:33,671 [INFO] - transformers.trainer - 
***** Running Evaluation *****
2025-11-19 00:58:33,671 [INFO] - transformers.trainer -   Num examples = 800
2025-11-19 00:58:33,672 [INFO] - transformers.trainer -   Batch size = 4
2025-11-19 01:13:04,676 [INFO] - absl - Using default tokenizer.
2025-11-19 01:30:00,692 [INFO] - transformers.trainer - Saving model checkpoint to mbart-vit-summarizer-v16\checkpoint-3122
2025-11-19 01:30:00,719 [INFO] - transformers.configuration_utils - Configuration saved in mbart-vit-summarizer-v16\checkpoint-3122\config.json
2025-11-19 01:30:00,721 [INFO] - transformers.generation.configuration_utils - Configuration saved in mbart-vit-summarizer-v16\checkpoint-3122\generation_config.json
2025-11-19 01:30:35,203 [INFO] - transformers.modeling_utils - Model weights saved in mbart-vit-summarizer-v16\checkpoint-3122\model.safetensors
2025-11-19 01:30:35,211 [INFO] - transformers.tokenization_utils_base - tokenizer config file saved in mbart-vit-summarizer-v16\checkpoint-3122\tokenizer_config.json
2025-11-19 01:30:35,274 [INFO] - transformers.tokenization_utils_base - Special tokens file saved in mbart-vit-summarizer-v16\checkpoint-3122\special_tokens_map.json
2025-11-19 02:49:20,128 [INFO] - transformers.trainer - 
***** Running Evaluation *****
2025-11-19 02:49:20,128 [INFO] - transformers.trainer -   Num examples = 800
2025-11-19 02:49:20,128 [INFO] - transformers.trainer -   Batch size = 4
2025-11-19 03:07:55,556 [INFO] - absl - Using default tokenizer.
2025-11-19 03:24:46,888 [INFO] - transformers.trainer - Saving model checkpoint to mbart-vit-summarizer-v16\checkpoint-4683
2025-11-19 03:24:46,895 [INFO] - transformers.configuration_utils - Configuration saved in mbart-vit-summarizer-v16\checkpoint-4683\config.json
2025-11-19 03:24:46,900 [INFO] - transformers.generation.configuration_utils - Configuration saved in mbart-vit-summarizer-v16\checkpoint-4683\generation_config.json
2025-11-19 03:25:20,928 [INFO] - transformers.modeling_utils - Model weights saved in mbart-vit-summarizer-v16\checkpoint-4683\model.safetensors
2025-11-19 03:25:20,928 [INFO] - transformers.tokenization_utils_base - tokenizer config file saved in mbart-vit-summarizer-v16\checkpoint-4683\tokenizer_config.json
2025-11-19 03:25:20,938 [INFO] - transformers.tokenization_utils_base - Special tokens file saved in mbart-vit-summarizer-v16\checkpoint-4683\special_tokens_map.json
2025-11-19 03:25:43,176 [INFO] - transformers.trainer - 

Training completed. Do not forget to share your model on huggingface.co/models =)


2025-11-19 03:25:43,191 [INFO] - root - --- v16 Training finished successfully ---
2025-11-19 03:25:43,191 [INFO] - root - Checkpoints and logs are saved in: mbart-vit-summarizer-v16
